{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module: Text Representation and Vectorization\n",
    "\n",
    "## Module Overview\n",
    "\n",
    "This module introduces students to the fundamental techniques for converting text data into numerical representations suitable for machine learning and NLP applications. Students will explore the evolution from traditional sparse representations to modern dense embeddings, gaining both theoretical understanding and practical implementation skills through hands-on exercises with real-world examples.\n",
    "\n",
    "### Module Objectives\n",
    "\n",
    "By the end of this module, students will be able to:\n",
    "\n",
    "1. **Understand Text Vectorization Fundamentals**: Grasp core concepts of transforming text into numerical representations and their importance in NLP\n",
    "2. **Master Basic Vectorization Techniques**: Implement one-hot encoding, bag-of-words, and TF-IDF vectorization methods\n",
    "3. **Apply Advanced Text Representation**: Use n-gram models and understand their impact on capturing text context\n",
    "4. **Work with Word Embeddings**: Understand and implement Word2Vec (CBOW and Skip-Gram) models for dense text representations\n",
    "5. **Leverage Contextual Embeddings**: Apply BERT for context-aware word representations and compare with static embeddings\n",
    "\n",
    "### Module Components\n",
    "\n",
    "#### Theoretical Foundation\n",
    "- Evolution of text representation methods from sparse to dense vectors\n",
    "- Mathematical foundations of vectorization techniques (TF-IDF, cosine similarity)\n",
    "- Distributional vs. distributed representations in NLP\n",
    "- Understanding the transformer architecture and self-attention mechanisms\n",
    "- Contextual vs. static embeddings and their applications\n",
    "\n",
    "#### Practical Skills\n",
    "- Implementation of one-hot encoding and bag-of-words models\n",
    "- TF-IDF vectorization with parameter tuning and optimization\n",
    "- Training Word2Vec models using Gensim library\n",
    "- Working with pre-trained BERT models using Transformers library\n",
    "- Comparative analysis of different vectorization approaches\n",
    "- Performance evaluation using similarity metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Module Content\n",
    "\n",
    "### Lecture Materials\n",
    "- **[Text Representation: Basic Vectorization Approaches (PDF)]** - Comprehensive overview of traditional text vectorization methods\n",
    "- **[Word Embeddings (PDF)]** - Deep dive into distributed representations and Word2Vec\n",
    "- **[BERT (PDF)]** - Understanding contextual embeddings and transformer-based models\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "For practice notebooks 201-204, please use the \"NLP\" Container.\n",
    "\n",
    "</div>\n",
    "\n",
    "### Practical Sessions (practices/)\n",
    "- **[One-Hot Encoding Basics](practices/201_text_processing_one_hot_encoding.ipynb)**\n",
    "  - Understanding categorical data representation\n",
    "  - Manual implementation vs. scikit-learn CountVectorizer\n",
    "  - Advantages and limitations of sparse representations\n",
    "  - Practical applications and use cases\n",
    "\n",
    "- **[Bag-of-Words and Text Processing](practices/202_text_processing_bag_of_words.ipynb)**\n",
    "  - Document tokenization and vocabulary building\n",
    "  - Implementing BoW with CountVectorizer\n",
    "  - Stopword removal and frequency analysis\n",
    "  - Working with repeating words and document matrices\n",
    "  - Enhancement techniques for improved performance\n",
    "\n",
    "- **[TF-IDF Vectorization](practices/203_text_processing_tfidf.ipynb)**\n",
    "  - Understanding Term Frequency and Inverse Document Frequency\n",
    "  - Mathematical foundations and normalization techniques\n",
    "  - Implementing TF-IDF with and without smoothing\n",
    "  - N-gram integration and parameter optimization\n",
    "  - Comparative analysis with basic BoW approaches\n",
    "\n",
    "- **[Word Embeddings with Word2Vec](practices/204_text_processing_word_embeddings.ipynb)**\n",
    "  - CBOW vs. Skip-Gram model architectures\n",
    "  - Training Word2Vec models on custom datasets\n",
    "  - Semantic similarity analysis using cosine similarity\n",
    "  - Vector arithmetic and analogy tasks\n",
    "  - Evaluation and interpretation of embedding quality\n",
    "\n",
    "- **[BERT Contextual Embeddings](practices/205_text_processing_bert.ipynb)**\n",
    "  - Understanding bidirectional context processing\n",
    "  - Working with pre-trained BERT models\n",
    "  - Tokenization with WordPiece and special tokens\n",
    "  - Contextual vs. static embedding comparisons\n",
    "  - Practical applications in text understanding tasks\n",
    "\n",
    "---\n",
    "\n",
    "## Assignments\n",
    "\n",
    "### Assignment 1: Text Processing with Encoding Techniques\n",
    "**File:** [Assignment_201.ipynb](assignments/Assignment_201.ipynb)  \n",
    "**Points:** 10  \n",
    "**Focus:** Implementing and comparing basic vectorization methods (one-hot encoding, bag-of-words, and TF-IDF)\n",
    "\n",
    "**Overview:** This assignment explores three fundamental encoding techniques for NLP by implementing them on real text data and analyzing their differences. Students will work with manual implementations and scikit-learn tools to understand the progression from sparse to weighted representations.\n",
    "\n",
    "**Learning Outcomes:** Understanding of basic text vectorization, hands-on experience with scikit-learn, and critical analysis of method trade-offs.\n",
    "\n",
    "### Assignment 2: Advanced Text Embeddings Analysis\n",
    "**File:** [Assignment_202.ipynb](assignments/Assignment_202.ipynb)  \n",
    "**Points:** 10  \n",
    "**Focus:** Training Word2Vec models and implementing BERT for contextual embeddings, with semantic relationship analysis\n",
    "\n",
    "**Overview:** Building on basic vectorization techniques, this assignment explores advanced embedding methods by comparing static Word2Vec embeddings with contextual BERT representations. Students will work with domain-specific text to understand how context affects semantic capture.\n",
    "\n",
    "**Learning Outcomes:** Practical experience with advanced embeddings, understanding of contextual vs. static representations, and awareness of ethical considerations in NLP.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Path\n",
    "\n",
    "### Beginner Level\n",
    "1. Start with **Basic Vectorization Approaches (PDF)** to understand foundational concepts\n",
    "2. Practice **One-Hot Encoding** for simple categorical representation\n",
    "3. Work through **Bag-of-Words** for document-level vectorization\n",
    "\n",
    "### Intermediate Level\n",
    "4. Master **TF-IDF Vectorization** for weighted term importance\n",
    "5. Explore **Word Embeddings (PDF)** to understand distributed representations\n",
    "6. Implement **Word2Vec** models for semantic similarity tasks\n",
    "\n",
    "### Advanced Level\n",
    "7. Study **BERT (PDF)** for contextual embedding theory\n",
    "8. Practice **BERT implementation** for context-aware representations\n",
    "9. Compare all methods for comprehensive understanding and appropriate use case selection\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Technical Requirements\n",
    "- Solid understanding of Python programming and data structures\n",
    "- Familiarity with NumPy arrays and basic linear algebra operations\n",
    "- Basic knowledge of machine learning concepts (vectors, similarity measures)\n",
    "- Understanding of basic NLP preprocessing from Module 1\n",
    "\n",
    "### Libraries to Install (Only applicable to your local machines)\n",
    "\n",
    "```python\n",
    "# Core NLP and ML libraries\n",
    "pip install nltk spacy gensim\n",
    "\n",
    "# Transformers and deep learning\n",
    "pip install transformers torch\n",
    "\n",
    "# Traditional ML and vectorization\n",
    "pip install scikit-learn pandas numpy\n",
    "\n",
    "# Data visualization\n",
    "pip install matplotlib seaborn\n",
    "\n",
    "# Download language models\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Recommended Background\n",
    "- Completion of Module 1 (NLP Pipeline and Preprocessing)\n",
    "- Basic understanding of linear algebra and vector operations\n",
    "- Familiarity with machine learning concepts (training, evaluation)\n",
    "- Knowledge of Python libraries like pandas and numpy\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Documentation and References\n",
    "- [Scikit-learn Feature Extraction Documentation](https://scikit-learn.org/stable/modules/feature_extraction.html)\n",
    "- [Gensim Word2Vec Documentation](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
    "- [BERT Paper - Original Research](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "### Recommended Reading\n",
    "- Vajjala, Sowmya, et al. *Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems*. O'Reilly Media, 2020. (Chapters 3-4)\n",
    "- Tunstall, Lewis, et al. *Natural Language Processing with Transformers*. O'Reilly Media, 2022. (Chapters 1-3)\n",
    "- Jurafsky, Daniel, and James H. Martin. *Speech and Language Processing*. 3rd edition. (Chapters 6-7)\n",
    "\n",
    "### Online Resources\n",
    "- [Word2Vec Tutorial](https://www.tensorflow.org/tutorials/text/word2vec)\n",
    "- [Understanding BERT](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n",
    "- [TF-IDF from Scratch](https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089)\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. **Review the theoretical foundations** with the provided PDF slides\n",
    "2. **Set up your environment** with the required libraries listed above\n",
    "3. **Work through the notebooks sequentially** - each builds upon previous concepts\n",
    "4. **Complete practical exercises** in each notebook to reinforce learning\n",
    "5. **Experiment with different parameters** to understand their impact on results\n",
    "6. **Apply techniques to your own text data** to see real-world applications\n",
    "\n",
    "### Support and Questions\n",
    "- Review the comprehensive examples and explanations in each notebook\n",
    "- Refer to the documentation links for detailed API references and advanced usage\n",
    "- Practice with different text datasets to understand method strengths and limitations  \n",
    "- Experiment with hyperparameter tuning to optimize performance for specific tasks\n",
    "- Consider the computational trade-offs between different approaches for your use cases\n",
    "\n",
    "### Key Success Metrics\n",
    "- Ability to select appropriate vectorization method based on task requirements\n",
    "- Understanding of when to use sparse vs. dense representations\n",
    "- Competency in implementing and evaluating different text representation techniques\n",
    "- Knowledge of modern contextual embeddings and their advantages over traditional methods"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
