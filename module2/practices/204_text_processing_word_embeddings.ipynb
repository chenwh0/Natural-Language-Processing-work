{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe228023",
   "metadata": {
    "id": "fe228023"
   },
   "source": [
    "# Word Embedding and Word2Vec\n",
    "\n",
    "## What is Word Embedding?\n",
    "\n",
    "Word embedding is a technique in natural language processing (NLP) that transforms words or phrases into numerical vectors, allowing computers to understand and process human language more effectively. Instead of representing words as isolated symbols, word embeddings map each word to a point in a high-dimensional space, where the position of each word reflects its meaning and relationship to other words. This approach enables models to capture semantic similaritiesâ€”words with similar meanings are located close to each other in this vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16df20a",
   "metadata": {
    "id": "c16df20a"
   },
   "source": [
    "## What is Word2Vec?\n",
    "\n",
    "Word2Vec is a popular technique for learning word embeddings, and it offers two main model architectures: **CBOW (Continuous Bag of Words)** and **Skip-Gram**. Both models are designed to capture the relationships between words based on their context in a sentence, but they do so in opposite ways.\n",
    "\n",
    "## CBOW (Continuous Bag of Words)\n",
    "\n",
    "### How CBOW Works\n",
    "\n",
    "- **Goal:** Predict the target word (the word in the middle) using the surrounding context words.\n",
    "- **Input:** The context words around a missing or target word within a specified window size.\n",
    "- **Output:** The model tries to guess the target word that fits best in the given context.\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose you have the sentence:  \n",
    "*\"The cat sat on the mat.\"*\n",
    "\n",
    "If your window size is 2 and the target word is \"sat\", the context words are [\"the\", \"cat\", \"on\", \"the\"]. The CBOW model takes these context words as input and tries to predict \"sat\" as the output.\n",
    "\n",
    "### Visual Explanation\n",
    "\n",
    "In the first figure, you see several context words as input nodes (e.g., w(t-2), w(t-1), w(t+1), w(t+2)). These are combined (projected and summed) to predict the target word w(t) in the output. The model learns to associate groups of context words with the most likely target word that appears in the middle.\n",
    "\n",
    "## Skip-Gram\n",
    "\n",
    "### How Skip-Gram Works\n",
    "\n",
    "- **Goal:** Predict the surrounding context words given a single target word.\n",
    "- **Input:** The current (target) word.\n",
    "- **Output:** The model tries to predict each of the context words that appear around the target word within a specified window.\n",
    "\n",
    "### Example\n",
    "\n",
    "Using the same sentence:  \n",
    "*\"The cat sat on the mat.\"*\n",
    "\n",
    "If the target word is \"sat\" and the window size is 2, the Skip-Gram model takes \"sat\" as input and tries to predict the context words [\"the\", \"cat\", \"on\", \"the\"].\n",
    "\n",
    "### Visual Explanation\n",
    "\n",
    "In the second figure, the input is a single word (w(t)), and the model projects this word to predict multiple output words (w(t-2), w(t-1), w(t+1), w(t+2)). The model learns to use the target word to generate the most likely context words that surround it.\n",
    "\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "| Feature         | CBOW                                      | Skip-Gram                                  |\n",
    "|-----------------|-------------------------------------------|--------------------------------------------|\n",
    "| Input           | Context words                             | Target word                                |\n",
    "| Output          | Target word                               | Context words                              |\n",
    "| Use Case        | Faster, works well with frequent words    | Better for rare words and small datasets   |\n",
    "| Training Speed  | Generally faster                         | Slower, but more accurate for rare words   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56765052",
   "metadata": {
    "id": "56765052"
   },
   "source": [
    "## Why Use Word2Vec?\n",
    "\n",
    "Word2Vec is widely used in NLP for several reasons:\n",
    "\n",
    "- **Semantic Representation:** It captures the meaning and relationships between words, so similar words have similar vectors.\n",
    "- **Distributional Semantics:** Based on the idea that words used in similar contexts have similar meanings, Word2Vec learns from the distribution of words in large text datasets.\n",
    "- **Vector Arithmetic:** The learned vectors can be combined using arithmetic operations to reveal relationships. For example, the vector for \"king\" minus \"man\" plus \"woman\" results in a vector close to \"queen\".\n",
    "- **Efficiency:** Word2Vec is computationally efficient, making it feasible to train on large datasets with extensive vocabularies.\n",
    "- **Transfer Learning:** Pre-trained Word2Vec models can be used as a starting point for various NLP tasks, saving time and resources.\n",
    "- **Wide Applications:** Word2Vec embeddings are used in tasks like text classification, sentiment analysis, information retrieval, machine translation, and question answering.\n",
    "- **Scalability:** The method can handle very large text corpora, which is essential for modern NLP applications.\n",
    "- **Open Source:** Libraries like Gensim provide easy-to-use implementations of Word2Vec, making it accessible for both research and industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f4bd6",
   "metadata": {
    "id": "f18f4bd6"
   },
   "source": [
    "## How Does Word2Vec Work?\n",
    "\n",
    "Word2Vec learns word vectors by training a neural network on a large text corpus. The network is trained to perform one of two tasks, depending on the chosen architecture:\n",
    "\n",
    "- **CBOW:** The model receives several context words as input and tries to predict the target word that appears in the middle of those context words. For example, given the context \"the cat on the,\" the model tries to predict \"mat.\"\n",
    "- **Skip-Gram:** The model receives a single word as input and tries to predict the words that appear around it within a certain window size. For example, given the word \"cat,\" the model tries to predict words like \"the,\" \"on,\" and \"mat.\"\n",
    "\n",
    "During training, the model adjusts the word vectors so that words appearing in similar contexts end up with similar vectors. After training, these vectors can be used to measure the similarity between words, find related words, or serve as input features for other machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2aef3",
   "metadata": {
    "id": "8ec2aef3"
   },
   "source": [
    "## Example: Training Word2Vec in Python\n",
    "\n",
    "Training a Word2Vec model in Python involves several clear steps, from preparing your text data to evaluating the semantic similarity between words. Below is a detailed, practical guide using the full text of \"Alice's Adventures in Wonderland\" from Project Gutenberg, available at [this link](https://www.gutenberg.org/files/11/11-0.txt).\n",
    "\n",
    "### Step 1: Download and Prepare the Text\n",
    "\n",
    "- **Download the Text File (Optional):**  \n",
    "  Use the plain text version of \"Alice's Adventures in Wonderland\" from Project Gutenberg:  \n",
    "  https://www.gutenberg.org/files/11/11-0.txt.\n",
    "\n",
    "- **Text Preprocessing:**  \n",
    "  - Read the text file into Python.\n",
    "  - Replace escape characters (like `\\n`) with spaces to ensure clean sentence boundaries.\n",
    "  - Split the text into sentences, then tokenize each sentence into lowercase words for consistency.\n",
    "  - This step ensures the data is in the right format for training the Word2Vec model.\n",
    "\n",
    "### Step 2: Train the Word2Vec Models\n",
    "\n",
    "- **CBOW Model (Continuous Bag of Words):**  \n",
    "  - Trains the model to predict a target word based on its surrounding context words.\n",
    "  - Useful for larger datasets and more frequent words.\n",
    "\n",
    "- **Skip-Gram Model:**  \n",
    "  - Trains the model to predict the context words given a single target word.\n",
    "  - Especially effective for learning representations of rare words.\n",
    "\n",
    "- **Implementation Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae0f514",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eae0f514",
    "outputId": "10562033-bf0c-4f0c-c864-c62bb462985d"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Step 1: Download the text directly from the URL\n",
    "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Step 2: Preprocess the text\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "data = []\n",
    "for sentence in sent_tokenize(text):\n",
    "    words = [word.lower() for word in word_tokenize(sentence)]\n",
    "    data.append(words)\n",
    "\n",
    "# Step 3: Train the CBOW model\n",
    "cbow_model = Word2Vec(data, min_count=1, vector_size=100, window=5)\n",
    "\n",
    "# Step 4: Train the Skip-Gram model\n",
    "skipgram_model = Word2Vec(data, min_count=1, vector_size=100, window=5, sg=1)\n",
    "\n",
    "# Step 5: Compute cosine similarities\n",
    "print(\"Cosine similarity between 'alice' and 'wonderland' - CBOW:\",\n",
    "      cbow_model.wv.similarity('alice', 'wonderland'))\n",
    "print(\"Cosine similarity between 'alice' and 'machines' - CBOW:\",\n",
    "      cbow_model.wv.similarity('alice', 'machines'))\n",
    "\n",
    "print(\"Cosine similarity between 'alice' and 'wonderland' - Skip-Gram:\",\n",
    "      skipgram_model.wv.similarity('alice', 'wonderland'))\n",
    "print(\"Cosine similarity between 'alice' and 'machines' - Skip-Gram:\",\n",
    "      skipgram_model.wv.similarity('alice', 'machines'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63222548",
   "metadata": {
    "id": "63222548"
   },
   "source": [
    "### Step 3: Evaluate Word Similarity\n",
    "\n",
    "After training your Word2Vec models, you can measure how closely related two words are by calculating the cosine similarity between their vector representations. Cosine similarity values range from -1 (completely dissimilar) to 1 (identical), with values closer to 1 indicating a stronger semantic relationship between the words in the embedding space.\n",
    "\n",
    "**Updated Sample Output:**\n",
    "\n",
    "| Word Pair                | CBOW Similarity | Skip-Gram Similarity |\n",
    "|--------------------------|-----------------|----------------------|\n",
    "| 'alice' & 'wonderland'   | 0.9866          | 0.8666               |\n",
    "| 'alice' & 'machines'     | 0.9544          | 0.8534               |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **'alice' & 'wonderland':**\n",
    "  - **CBOW Similarity (0.9866):** This high value means that, according to the CBOW model, \"alice\" and \"wonderland\" appear in very similar contexts throughout the text, reflecting a strong semantic connection between the two words.\n",
    "  - **Skip-Gram Similarity (0.8666):** This is also a high value, though slightly lower than CBOW, indicating that \"alice\" and \"wonderland\" are still closely related, but the Skip-Gram model may capture slightly different contextual nuances.\n",
    "\n",
    "- **'alice' & 'machines':**\n",
    "  - **CBOW Similarity (0.9544):** While still relatively high, this value is lower than the similarity between \"alice\" and \"wonderland,\" suggesting that \"alice\" and \"machines\" are less frequently found in similar contexts, and thus less semantically related in the story[#].\n",
    "  - **Skip-Gram Similarity (0.8534):** This is the lowest among the four, indicating that \"alice\" and \"machines\" are not strongly related in the text, as expected, since \"machines\" is not a central theme in \"Alice's Adventures in Wonderland\".\n",
    "\n",
    "**Summary:**  \n",
    "Higher cosine similarity values indicate stronger semantic relationships. The results show that \"alice\" and \"wonderland\" are much more semantically related than \"alice\" and \"machines,\" which aligns with the content and themes of the book.\n",
    "\n",
    "### Experimentation\n",
    "\n",
    "- **Change Model Parameters:** You can experiment by adjusting parameters like `vector_size` (the number of dimensions in the embedding), `window` (the context window size), or `min_count` (minimum word frequency) to see how these affect the similarity scores and the quality of the learned relationships.\n",
    "- **Try Different Word Pairs:** Test other word pairs to explore how the model captures various relationships in the text. For example, compare \"queen\" and \"king,\" or \"rabbit\" and \"hole\" to see if the model reflects their narrative connections.\n",
    "- **Observe Effects:** Changing these parameters or word pairs can help you understand how Word2Vec models learn and represent semantic meaning from text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d1554",
   "metadata": {
    "id": "c16d1554"
   },
   "source": [
    "## Applications of Word Embedding\n",
    "\n",
    "Word embeddings like those produced by Word2Vec are used in many NLP tasks:\n",
    "\n",
    "- **Text Classification:** Improve the accuracy of categorizing documents or detecting sentiment by providing rich word representations.\n",
    "- **Named Entity Recognition (NER):** Help identify names, locations, and other entities by leveraging semantic context.\n",
    "- **Information Retrieval:** Enable more accurate search results by matching documents based on semantic similarity rather than just keyword matching.\n",
    "- **Machine Translation:** Facilitate translation by capturing relationships between words in different languages.\n",
    "- **Question Answering:** Enhance the ability of systems to understand and answer questions by providing context-aware word representations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
