{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding Basics\n",
    "\n",
    "One-hot encoding is a foundational technique in natural language processing for converting words into a numerical format that machine learning models can understand. The process begins by building a vocabulary from the text data, where each unique word is assigned a distinct integer index. This index is then used to create a binary vector for each word: the vector has all zeros except for a single one at the position corresponding to the word’s index in the vocabulary.\n",
    "\n",
    "For example, consider the following vocabulary, where each word is mapped to a unique number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example vocabulary\n",
    "vocab = {\"cat\": 1,\n",
    "         \"chases\": 2,\n",
    "         \"mouse\": 3,\n",
    "         \"dog\": 4,\n",
    "         \"runs\": 5,\n",
    "         \"cheese\": 6,\n",
    "         \"eats\": 7,\n",
    "         \"the\": 8,\n",
    "         \"a\": 9\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To one-hot encode a sentence such as `\"cat chases mouse\"`, we can use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_vector(text):\n",
    "    onehot_encoded = []\n",
    "    for word in text.split():\n",
    "        temp = [0] * len(vocab)\n",
    "        if word in vocab:\n",
    "            temp[vocab[word]-1] = 1\n",
    "        onehot_encoded.append(temp)\n",
    "    return onehot_encoded\n",
    "\n",
    "get_onehot_vector(\"cat chases mouse\")\n",
    "# Output: [[1,0,0,0,0,0,0,0,0], [0,1,0,0,0,0,0,0,0], [0,0,1,0,0,0,0,0,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this output, each word in the sentence is represented as a vector of length 9 (the size of the vocabulary), with a single one indicating the position of that word in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform one-hot encoding using scikit-learn’s `CountVectorizer`, which is a convenient tool for transforming text into numerical features. While `CountVectorizer` is typically used to count word occurrences, we can set the `binary=True` parameter to ensure that each word is represented as a 1 if it appears in the text, and 0 otherwise—effectively giving us one-hot encoding at the document level.\n",
    "\n",
    "For example, let’s define our vocabulary and use `CountVectorizer` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define our vocabulary\n",
    "vocab = [\"cat\", \"chases\", \"mouse\", \"dog\", \"runs\", \"cheese\", \"eats\", \"the\", \"a\"]\n",
    "\n",
    "# Initialize CountVectorizer with our vocabulary and binary=True for one-hot encoding\n",
    "vectorizer = CountVectorizer(vocabulary=vocab, binary=True)\n",
    "\n",
    "# Transform the sentence\n",
    "X = vectorizer.transform([\"cat chases mouse\"])\n",
    "\n",
    "# Convert to array\n",
    "print(X.toarray())\n",
    "# Output: [[1 1 1 0 0 0 0 0 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is a single vector representing the entire sentence \"cat chases mouse.\" Each position in the vector corresponds to a word in our vocabulary, with a 1 indicating the presence of that word in the sentence.\n",
    "\n",
    "If we want to obtain a separate one-hot vector for each word—just like our earlier manual approach—we can transform each word individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \"cat chases mouse\".split()\n",
    "X = vectorizer.transform(words)\n",
    "print(X.toarray())\n",
    "# Output: [[1 0 0 0 0 0 0 0 0],\n",
    "#          [0 1 0 0 0 0 0 0 0],\n",
    "#          [0 0 1 0 0 0 0 0 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, each row is a one-hot vector for a single word, matching the output of our manual one-hot encoding function.\n",
    "\n",
    "To see the mapping of words to their indices in the vector, we can access the vocabulary dictionary that `CountVectorizer` uses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_)\n",
    "# Output: {'cat': 0, 'chases': 1, 'mouse': 2, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that scikit-learn assigns indices starting from 0, which is a slight difference from our earlier example where we started from 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
