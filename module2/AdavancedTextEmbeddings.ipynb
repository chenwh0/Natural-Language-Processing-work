{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenwh0/Natural-Language-Processing-work/blob/main/module2/AdavancedTextEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9AI8Hk8WJTP"
      },
      "source": [
        "# **Advanced Text Embeddings**\n",
        "This lab implements advanced embedding methods—Word2Vec (static embeddings) and BERT (contextual embeddings), compare their semantic capture capabilities, and analyze their strengths/weaknesses using real-world text data.\n",
        "# *Sources used*\n",
        "* https://github.com/opengeos/geospatial-data-catalogs\n",
        "* https://www.geeksforgeeks.org/nlp/word2vec-with-gensim/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6A5uobCs4ri"
      },
      "source": [
        "# *Installs & Imports*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QneXU7rBqg9"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kENx4-q_s7g4"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing Libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Tokenization libraries/downloads\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Word2Vec embedding libraries\n",
        "#from gensim.models import Word2Vec\n",
        "\n",
        "# BERT embedding libraries/downloads. Doesn't work on Jupyter notebook bc pip install is installing transformers to my local folder\n",
        "import numpy as np\n",
        "from transformers import BertModel, BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA # for PCA\n",
        "from sklearn.manifold import TSNE # for BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuGwM7tOr5br"
      },
      "source": [
        "# **1. Dataset Preparation**\n",
        "a. Selection - I wanted to do natural language processing on geospatial-related corpus. The data's source also contained concise instructions on how to retrieve the dataset.\n",
        "\n",
        "b. Preprocessing - Remove puncuation, digits, extra whitespaces\n",
        "\n",
        "c. Tokenize - Use NLTK tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOXaEdJTWIjI",
        "outputId": "0f5cedc3-f650-4799-f2bb-a7119eb05140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Indian Remote Sensing satellites (IRS) are a series of Earth Observation satellites, built, launched and maintained by Indian Space Research Organisation. The IRS series provides many remote sensing services to India and international ground stations. With 5 m resolution and products covering areas up to 70 km x 70 km IRS LISS-IV mono data provide a cost effective solution for mapping tasks up to 1:25'000 scale.\n",
            "\n",
            "Preprocessed: indian remote sensing satellites irs are a series of earth observation satellites built launched and maintained by indian space research organisation the irs series provides many remote sensing services to india and international ground stations with m resolution and products covering areas up to km x km irs lissiv mono data provide a cost effective solution for mapping tasks up to scale\n"
          ]
        }
      ],
      "source": [
        "# Select text dataset\n",
        "url = 'https://github.com/opengeos/geospatial-data-catalogs/raw/master/nasa_cmr_catalog.tsv'\n",
        "dataframe = pd.read_csv(url, sep='\\t')\n",
        "title_description_dataframe = dataframe[[\"title\", \"description\"]]\n",
        "title_description_dataframe.head()\n",
        "\n",
        "# Preprocess text by removing punctuation, extra whitespace, and stopwords.\n",
        "def preprocess_text(text: str) -> str:\n",
        "    text = text.lower() # Lowercase all text.\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text) # Remove punctuation\n",
        "    text = re.sub(r\"\\d\", \"\", text) # Remove digits\n",
        "    text = re.sub(r\"\\s+\", \" \", text) # remove extra whitespace\n",
        "    return text\n",
        "\n",
        "preprocessed_dataframe = title_description_dataframe.copy() # Make a copy of original dataframe\n",
        "preprocessed_dataframe[\"description\"] = title_description_dataframe[\"description\"].map(preprocess_text) # Preprocess the copy's data\n",
        "\n",
        "# Display differences\n",
        "print(\"Original:\", title_description_dataframe[\"description\"][0])\n",
        "print(\"\\nPreprocessed:\", preprocessed_dataframe[\"description\"][0])\n",
        "\n",
        "# Trim corpus and combine descriptions\n",
        "descriptions_list = preprocessed_dataframe[\"description\"][:10] # Get 10 descriptions for quick processing\n",
        "descriptions = \" \".join(descriptions_list)\n",
        "\n",
        "# Handle domain specificity. Ex: Replace abbreviations with acutual full name\n",
        "descriptions = descriptions.replace(\"cci\", \"climate change initiative\") # Most common domain-specific abbreviation\n",
        "descriptions = descriptions.replace(\" esa \", \" european space agency \") # 2nd most common domain-specific abbreviation\n",
        "descriptions = descriptions.replace(\" ecv \", \" essential climate variable \")\n",
        "descriptions = descriptions.replace(\" sec \", \" surface elevation changes \")\n",
        "descriptions = descriptions.replace(\" iop \", \" inherent optical properties \")\n",
        "descriptions = descriptions.replace(\" irs \", \" indian remote sensing satellites \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFHIYxZEBqg_"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "data = []\n",
        "for sentence in sent_tokenize(descriptions):\n",
        "        words = list(word_tokenize(sentence))\n",
        "        data.append(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "part of the european space agency greenland ice sheet climate change initiative project the data set provides surface elevation changes surface elevation changes for the greenland ice sheet derived from saralaltika\n",
        "for this new experimental product of surface elevation change is based on data from the altikainstrument onboard the france\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpPalUXFDmex",
        "outputId": "8763a107-32e9-484f-a577-f7bd6c84fb5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "indian remote sensing satellites indian remote sensing satellites are a series of earth observation satellites built launched and maintained by indian space research organisation the indian remote sensing satellites series provides many remote sensing services to india and international ground stations with m resolution and products covering areas up to km x km indian remote sensing satellites lissiv mono data provide a cost effective solution for mapping tasks up to scale the cloud_climate change initiative avhrrpmv dataset covering was generated within the cloud_climate change initiative project which was funded by the european space agency european space agency as part of the european space agency climate change initiative climate change initiative programme contract no inb this dataset is one of the datasets generated in cloud_climate change initiative all of them being based on passiveimager satellite measurementsthis dataset is based on measurements from avhrr onboard the noaa noaa noaa noaa noaa noaa noaa satellites and contains a variety of cloud properties which were derived employing the community cloud retrieval for climate cccl sus et al mcgarragh et al retrieval framework the core cloud properties contained in the cloud_climate change initiative avhrrpmv dataset are cloud maskfraction cloud phase cloud top pressureheighttemperature cloud optical thickness cloud effective radius and cloud liquidice water path spectral cloud albedo is also included as experimental product the cloud properties are available at different processing levels this particular dataset contains levelc monthly averages and histograms data while levelu globally gridded unaveraged data fields is also available as a separate dataset pixelbased uncertainty estimates come along with all properties and have been propagated into the levelc data the data in this dataset are a subset of the avhrrpm lc lu cloud products version dataset produced by the european space agency cloud_climate change initiative project available from httpsdxdoiorgdwdesa_cloud_climate change initiativeavhrrpmv to cite the full dataset please use the following citation stengel martin sus oliver stapelberg stefan finkensieper stephan wã¼rzler benjamin philipp daniel hollmann rainer poulsen caroline european space agency cloud climate change initiative european space agency cloud_climate change initiative data cloud_climate change initiative avhrrpm lclu cld_products v deutscher wetterdienst dwd doidwdesa_cloud_climate change initiativeavhrrpmv the european space agency ocean colour climate change initiative project has produced global level binned multisensor timeseries of satellite oceancolour data with a particular focus for use in climate studiesthis dataset contains their version inherent optical properties inherent optical properties product in mgm on a sinusoidal projection at approximately km spatial resolution and at a number of time resolutions daily day day monthly and yearly composites covering the period note the inherent optical properties data are also included in the all products dataset the inherent optical properties inherent optical properties dataset consists of the total absorption and particle backscattering coefficients and additionally the fraction of detrital dissolved organic matter absorption and phytoplankton absorption the total absorption units m the total backscattering m the absorption by detrital and coloured dissolved organic matter the backscattering by particulate matter and the absorption by phytoplankton share the same spatial resolution of km the values of inherent optical properties are reported for the standard seawifs wavelengths nm this data product is on a sinusoidal equalarea grid projection matching the nasa standard level binned projection the default number of latitude rows is which results in a vertical bin cell size of approximately km the number of longitude columns varies according to the latitude which permits the equal area property unlike the nasa format where the bin cells that do not contain any data are omitted the climate change initiative format retains all cells and simply marks empty cells with a netcdf fill value a separate dataset is also available for data on a geographic projection this data set is part of the european space agency greenland ice sheet climate change initiative project the data set provides surface elevation changes surface elevation changes for the greenland ice sheet derived from saralaltika for this new experimental product of surface elevation change is based on data from the altikainstrument onboard the france cnesindian isro saral satellite the aktika altimeter utilizes kaband radar signals which have less penetration in the upper snow however the surface slope and roughness has an imprint in the derived signal and the new product is only available for the flatter central parts of the greenland ice sheetthe corresponding surface elevation changes grid from cryosat is included for comparison the algorithm used to devive the product is described in the paper âimplications of changing scattering properties on the greenland ice sheet volume change from cryosat altimetryâ by sb simonsen and ls sãrensen remote sensing of the environment pp doijrse the approach used here corresponds to least squares method lsm described in the paper in which the slope within each grid cell is accounted for by subtraction of the gimp dem the data are corrected for both backscatter and leading edge width and the lsm is solved at km grid resolution km search radius and averaged in the postprocessing to km grid resolution and with a correlation length of km the european space agency climate change initiative aerosol project has produced a number of global aerosol essential climate variable essential climate variable products from a set of european satellite instruments with different characteristics this dataset comprises level aerosol products from the atsr instrument on the ers satellite derived using the orac algorithm version it covers the period from for further details about these data products please see the linked documentation the soil moisture climate change initiative combined dataset is one of three datasets created as part of the european space agencys european space agency soil moisture essential climate variable essential climate variable climate change initiative climate change initiative project the product has been created by directly merging level scatterometer and radiometer soil moisture products derived from the amiws ascat smmr ssmi tmi amsre windsat amsr smos and smap satellite instruments passive and active products have also been createdthe v combined product provided as global daily images in netcdf classic file format presents a global coverage of surface soil moisture at a spatial resolution of degrees it is provided in volumetric units m m and covers the period yyyymmdd to for information regarding the theoretical and algorithmic base of the product please see the algorithm theoretical baseline document other additional reference documents and information relating to the dataset can also be found on the climate change initiative soil moisture project websitethe data set should be cited using all three of the following references gruber a scanlon t van der schalie r wagner w and dorigo w evolution of the european space agency climate change initiative soil moisture climate data records and their underlying merging methodology earth syst sci data â httpsdoiorgessd dorigo wa wagner w albergel c albrecht f balsamo g brocca l chung d ertl m forkel m gruber a haas e hamer d p hirschi m ikonen j de jeu r kidd r lahoz w liu yy miralles d lecomte p european space agency climate change initiative soil moisture for improved earth system understanding stateofthe art and future directions in remote sensing of environment issn httpsdoiorgjrse gruber a dorigo w a crow w wagner w triple collocationbased merging of satellite soil moisture retrievals ieee transactions on geoscience and remote sensing pp tgrs the european space agency fire disturbance climate change initiative fire_climate change initiative project has produced maps of global burned area developed from satellite observations the small fire dataset sfd pixel products have been obtained by combining spectral information from sentinel msi data and thermal information from modis modmd collection active fire productsthis dataset is part of v of the small fire dataset also known as fireclimate change initiativesfd which covers subsaharan africa for the year data is available here at pixel resolution degrees corresponding to approximately m at the equator gridded data products are also available in a separate dataset the european space agency ocean colour climate change initiative project has produced global level binned multisensor timeseries of satellite oceancolour data with a particular focus for use in climate studiesthis dataset contains all their version generated ocean colour products on a geographic projection at km spatial resolution and at a number of time resolutions daily day day monthly and yearly composites covering the period data are also available as monthly climatologiesdata products being produced include phytoplankton chlorophylla concentration remotesensing reflectance at six wavelengths total absorption and backscattering coefficients phytoplankton absorption coefficient and absorption coefficients for dissolved and detrital material and the diffuse attenuation coefficient for downwelling irradiance for light of wavelength nm information on uncertainties is also providedthis data product is on a geographic grid projection which is a direct conversion of latitude and longitude coordinates to a rectangular grid typically a fixed multiplier of x the netcdf files follow the cf convention for this projection with a resolution of x a separate dataset is also available for data on a sinusoidal projection we completed a field season in antarctica in with a person field party ten sampling sites along the transantarctic mountains from the convoy range to hatcher bluffs were visited by helicopter or fixedwing aircraft where rock samples were collected all samples were returned to the university of minnesotaduluth where they were prepared for laboratory study laboratory work includes examination of polished thin sections by optical microscope and scanning electron microscope to determine textures mineral assemblages and mineral compositions samples of igneous and metamorphic rock clasts were crushed in order to isolate the mineral zircon zircon from these samples was analyzed by upb o and hf isotopic analysis in order to determine their ages and isotopic character monazite was identified in selected samples for upb age dating in polished thin section a suite of ross orogen granitoids was also prepared for zircon separation and for wholerock geochemical analysis petrographic study is complete for over samples of igneous and metamorphic rock clasts collected from glacial moraines on the backside of the transantarctic mountains mainly between the inlets to the byrd through shackleton glaciers we upb o and hf analyses of zircon and monazite in igneous and metamorphic clasts and in samples of tam granitoids we completed a field season in antarctica in with a person field party ten sampling sites along the transantarctic mountains from the convoy range to hatcher bluffs were visited by helicopter or fixedwing aircraft where rock samples were collected all samples were returned to the university of minnesotaduluth where they were prepared for laboratory study laboratory work includes examination of polished thin sections by optical microscope and scanning electron microscope to determine textures mineral assemblages and mineral compositions samples of igneous and metamorphic rock clasts were crushed in order to isolate the mineral zircon zircon from these samples was analyzed by upb o and hf isotopic analysis in order to determine their ages and isotopic character monazite was identified in selected samples for upb age dating in polished thin section a suite of ross orogen granitoids was also prepared for zircon separation and for wholerock geochemical analysis petrographic study is complete for over samples of igneous and metamorphic rock clasts collected from glacial moraines on the backside of the transantarctic mountains mainly between the inlets to the byrd through shackleton glaciers we upb o and hf analyses of zircon and monazite in igneous and metamorphic clasts and in samples of tam granitoids\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnitc8FYsXN2"
      },
      "source": [
        "#  **2. Embedding Implementation**\n",
        "a. Try **Continuous Bag of Words (CBOW)**: Predicts a target word from its surrounding words. Ex:\n",
        "```python\n",
        "sentence = \"I came, I saw, I conquered.\"\n",
        "inputs = [\"I\", \"came\", \"I\", \"saw\"]\n",
        "output = \"conquered.\"\n",
        "```\n",
        "\n",
        "b. Try **skip-gram** - Predicts surrounding words from a target word. Ex:\n",
        "```python\n",
        "inputs = \"conquered.\"\n",
        "output = [\"I\", \"came\", \"I\", \"saw\"]\n",
        "```\n",
        "c. Compare both using cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWfhFUC7sbZZ",
        "outputId": "07bd2e01-428c-48a4-a5c9-cb3d21e11e13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CBOW cosine similarities:\n",
            "'data' vs 'climate' = 0.24312583\n",
            "'data' vs 'cloud' = -0.07697886\n",
            "\n",
            "Skip-Gram cosine similarities:\n",
            "'data' vs 'climate' = 0.96384984\n",
            "'data' vs 'cloud' = 0.8866163\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec implementation\n",
        "cbow_model = Word2Vec(data, min_count=1, vector_size=100, window=5) # Train CBOW model\n",
        "skipgram_model = Word2Vec(data, min_count=1, vector_size=100, window=5, sg=1) # Train Skip-Gram model\n",
        "\n",
        "# Step 5: Compute cosine similarities\n",
        "print(\"CBOW cosine similarities:\")\n",
        "print(\"'data' vs 'climate' =\", cbow_model.wv.similarity(\"data\", \"climate\"))\n",
        "print(\"'data' vs 'cloud' =\", cbow_model.wv.similarity(\"data\", \"cloud\"))\n",
        "print(\"\\nSkip-Gram cosine similarities:\")\n",
        "print(\"'data' vs 'climate' =\", skipgram_model.wv.similarity(\"data\", \"climate\"))\n",
        "print(\"'data' vs 'cloud' =\", skipgram_model.wv.similarity(\"data\", \"cloud\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "description3 = \"This data is part of the esa greenland ice sheet climate change initive project the data set provides evidence of surface elevation change\"\n",
        "comp_sents = [\"Your clean change of clothes is folded and in the basket.\",           # Clothing context\n",
        "              \"Many young children today worry about climate change.\",               # \"Climate change\" context\n",
        "              \"Corporations claim to be the image for change.\",                      # Progression context\n",
        "              \"It isn't just about donating the change in your pockets.\",            # Monetary context\n",
        "              \"Social change requires organization, not just action.\",               # \"social change\" context\n",
        "              ]"
      ],
      "metadata": {
        "id": "lcxFNkwMFGIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LotnyxtHBqhA",
        "outputId": "634a1fa2-e6eb-45b3-c27d-8babd2bdca21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.719\tMany young children today worry about climate change.\tThis data is part of the esa greenland ice sheet climate change initive project the data set provides evidence of surface elevation change\n",
            "0.489\tCorporations claim to be the image for change.\tThis data is part of the esa greenland ice sheet climate change initive project the data set provides evidence of surface elevation change\n",
            "0.364\tYour clean change of clothes is folded and in the basket.\tThis data is part of the esa greenland ice sheet climate change initive project the data set provides evidence of surface elevation change\n",
            "0.361\tIt isn't just about donating the change in your pockets.\tThis data is part of the esa greenland ice sheet climate change initive project the data set provides evidence of surface elevation change\n",
            "0.199\tSocial change requires organization, not just action.\tThis data is part of the esa greenland ice sheet climate change initive project the data set provides evidence of surface elevation change\n"
          ]
        }
      ],
      "source": [
        "# BERT implementation\n",
        "def cosine_similarity(a, b):\n",
        "    # Compute cosine similarity between two vectors a and b\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def get_bert_for_token(string, term):\n",
        "    # Tokenize the input string using the BERT tokenizer\n",
        "    inputs = tokenizer(string, return_tensors=\"pt\")\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    try:\n",
        "        # Find the index of the target token (term) in the tokenized list\n",
        "        term_idx = tokens.index(term)\n",
        "    except ValueError:\n",
        "        # Raise an error if the token is not found in the sentence\n",
        "        raise ValueError(f\"Token '{term}' not found in: {tokens}\")\n",
        "    # Pass the tokenized input through the BERT model\n",
        "    outputs = model(**inputs)\n",
        "    # Extract and return the embedding for the specified token\n",
        "    return outputs.last_hidden_state[0][term_idx].detach().numpy()\n",
        "\n",
        "description3_rep = get_bert_for_token(description3, \"change\")\n",
        "\n",
        "vals = []\n",
        "# For each comparison sentence, compute the cosine similarity of \"missouri\" embeddings\n",
        "for sent in comp_sents:\n",
        "    try:\n",
        "        # Get the BERT embedding for \"change\" in the current sentence\n",
        "        comp_rep = get_bert_for_token(sent, \"change\")\n",
        "        # Compute cosine similarity between query and comparison embedding\n",
        "        cos_sim = cosine_similarity(description3_rep, comp_rep)\n",
        "        # Store the similarity and sentences for later sorting/printing\n",
        "        vals.append((cos_sim, description3, sent))\n",
        "    except ValueError:\n",
        "        # Skip sentences where \"missouri\" token is not found\n",
        "        continue\n",
        "\n",
        "# Sort results by similarity (highest first), then print them\n",
        "for c, q, s in reversed(sorted(vals)):\n",
        "    print(f\"{c:.3f}\\t{s}\\t{q}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZlR2pb9BqhB"
      },
      "source": [
        "# **3. Analysis and Visualization**\n",
        "\n",
        "Comparison table for the word \"change\":\n",
        "\n",
        "| Metric | Word2Vec (Skip-gram) | BERT |\n",
        "|----------------------|------------------------|---------------|\n",
        "| Same word similarity | 1.0 | 0.719 (most similar case) |\n",
        "| OOV handling | Poor | Subword tokens|\n",
        "| Context sensitivity | Low | High |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrvZHp_dBqhB"
      },
      "outputs": [],
      "source": [
        "# Visualize embeddings using PCA (Word2Vec)\n",
        "word_vectors = skipgram_model.wv[skipgram_model.wv.index_to_key]  # Get the word vectors\n",
        "pca = PCA(n_components=2)  # Initialize PCA\n",
        "result = pca.fit_transform(word_vectors)  # Fit and transform the word vectors\n",
        "\n",
        "# Plot the words in a 2D space\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "\n",
        "# Annotate words in the plot\n",
        "words = list(skipgram_model.wv.index_to_key)\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=12)\n",
        "\n",
        "plt.title(\"Word Embeddings Visualization\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_for_token(string, term):\n",
        "    # Tokenize the input string using the BERT tokenizer\n",
        "    inputs = tokenizer(string, return_tensors=\"pt\")\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    try:\n",
        "        # Find the index of the target token (term) in the tokenized list\n",
        "        term_idx = tokens.index(term)\n",
        "    except ValueError:\n",
        "        # Raise an error if the token is not found in the sentence\n",
        "        raise ValueError(f\"Token '{term}' not found in: {tokens}\")\n",
        "    # Pass the tokenized input through the BERT model\n",
        "    outputs = model(**inputs)\n",
        "    # Extract and return the embedding for the specified token\n",
        "    return outputs.last_hidden_state[0].numpy()"
      ],
      "metadata": {
        "id": "OIwPMxSDb4JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT visualization\n",
        "values_only = np.array([v for v,q,c in vals])\n",
        "\n",
        "values_only = np.array([v for v,q,c in vals])\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=2)\n",
        "embeddings_2d = tsne.fit_transform(values_only)\n",
        "plt.figure(figsize=(10, 7), dpi=1000)\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], marker='o')\n",
        "for i, word in enumerate(words):\n",
        "    plt.text(embeddings_2d[i, 0], embeddings_2d[i, 1],\n",
        "             word, fontsize=10, ha='left', va='bottom')\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.title('Word Embedding Graph (t-SNE with Word2Vec)')\n",
        "plt.grid(True)\n",
        "plt.savefig('embedding.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "9EYrKGkPR3U9",
        "outputId": "640f5241-5c2c-4065-87c3-18c11eea941c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'toarray'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1482100508.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalues_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0membeddings_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchararray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[1;32m    411\u001b[0m                              \"{!r}\".format(__name__, attr))\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'toarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Pyzuw5sw2f"
      },
      "source": [
        "# **Technical Reflection**  \n",
        "## Word2Vec vs BERT\n",
        "Use Word2Vec when you don't have high computing resources. Use BERT when you do.\n",
        "## How BERT handles polysemy\n",
        "*polysemy for example is the same word with multiple meanings e.g. \"The city of **Columbia**\" vs. \"University of **Columbia**\"*\n",
        "\n",
        "BERT handles polysemy because every token pays attention to every other token and contextual weighting occurs at once - \"city\" sends strong signal to earlier “Columbia”. This moves embeddings towards regional entity.  “Univesity of” influences “Columbia” embedding. Shifting it towards an university entity.\n",
        "\n",
        "## Ethical implications of embedding biases\n",
        "Languages are used in different ways by different people. For example, an embedder that is only trained on \"proper\" and \"academic\" English will be less likely to correctly capture the context meant by a non-academic english-speaker, a specific regional english-speaker, or a speaker of AAVE (African American Vernacular English)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}