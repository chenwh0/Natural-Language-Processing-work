{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ETtu9CvVMDR"
      },
      "source": [
        "# Advanced Text Generation Techniques and Tools<"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMRhUhsfdMpZ"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain-community\n",
        "# !pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rerbJgwAigbK"
      },
      "source": [
        "# Loading an LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHIu2hn2dMpa",
        "outputId": "ac663440-1e3d-4a6d-f1ef-bc9cb54b303f"
      },
      "outputs": [],
      "source": [
        "# !wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQcht_ZFijW7",
        "outputId": "d7a66057-ebb9-411f-e1d8-c5c23c286acf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "# Initialize the LlamaCpp language model with specified parameters\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",  # Path to the GGUF model file\n",
        "    n_gpu_layers=-1,       # Use all available GPU layers\n",
        "    max_tokens=500,        # Maximum number of tokens to generate in a response\n",
        "    n_ctx=2048,            # Context window size\n",
        "    seed=42,               # Random seed for reproducibility\n",
        "    verbose=False          # Disable verbose output\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's invoke the LlamaCpp language model with a prompt directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3SNhQF9WthzV",
        "outputId": "e2ef60cd-db82-494e-cfd1-0ddcfaa9e374"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Directly invoke the LlamaCpp language model with a prompt\n",
        "# This sends the prompt string to the model and returns its response\n",
        "llm.invoke(\"Hi! My name is Hatef. What is 2 times 5?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwx2AIuGfCoP"
      },
      "source": [
        "### Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSMBMRxB8gFW"
      },
      "source": [
        "### Building Complex Workflows with Multiple Chains\n",
        "\n",
        "Now that we've mastered basic prompt templating, let's explore how to create more sophisticated workflows by chaining multiple operations together. This approach allows us to break down complex tasks into manageable steps, where each step builds upon the previous one.\n",
        "\n",
        "**Why Chain Multiple Prompts?**\n",
        "- **Modularity**: Each step handles a specific task\n",
        "- **Reusability**: Components can be used in different combinations  \n",
        "- **Quality**: Focused prompts often produce better results than complex single prompts\n",
        "- **Debugging**: Easier to identify and fix issues in individual steps\n",
        "\n",
        "We'll demonstrate this by building a creative story generation pipeline that:\n",
        "1. Creates a compelling title from a summary\n",
        "2. Develops character descriptions based on the title and summary\n",
        "3. Generates the complete story using all previous components\n",
        "\n",
        "**Modern vs. Legacy Approaches**: You'll notice we show both the deprecated `LLMChain` approach and the modern `RunnableSequence` method. The modern approach is preferred for new projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kF--Q5me_-X1",
        "outputId": "3050ac9c-7611-45cb-b573-1bf1902dc679"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Hi Hatef! 2 times 5 is equal to 10.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Define a prompt template for single-turn LLM interaction.\n",
        "# The template uses the variable \"input_prompt\" to insert user input.\n",
        "template = \"\"\"<|user|>\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "# Create a PromptTemplate object with the template and input variable.\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")\n",
        "\n",
        "# Create a basic chain by piping the prompt template into the LlamaCpp language model.\n",
        "# This chain takes an input prompt, formats it using the template, and sends it to the LLM for a response.\n",
        "basic_chain = prompt | llm\n",
        "\n",
        "# Use the chain\n",
        "basic_chain.invoke(\n",
        "    {\"input_prompt\": \"Hi! My name is Hatef. What is 2 times 5?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output Analysis**: Notice how the model generates creative, contextually appropriate titles. The modern pipe operator (`|`) creates a clean, readable chain that's easier to debug and maintain than the deprecated `LLMChain` approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Title Generation Chain\n",
        "Create Title Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrUKuHt_OLpe",
        "outputId": "07975292-7b79-4ce6-87a7-3cb6c354f541"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' The Lone Guardian: A Tale of Man and Tigress.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Create a chain for the title of our story\n",
        "template = \"\"\"<|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "title_chain = title_prompt | llm\n",
        "title_chain.invoke({\"summary\": \"A man and his tiger\"})\n",
        "# Example Output: \"The Unbreakable Bond: A Man and His Tiger\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's create a character description generator that uses both the original summary and the generated title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zTtFEmANOhyE"
      },
      "outputs": [],
      "source": [
        "# Create a chain for the character description using the summary and title\n",
        "template = \"\"\"<|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "character_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "character_chain = character_prompt | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Chain Dependencies**: Notice how the character generator depends on both the original summary and the generated title. This demonstrates how chains can have multiple inputs from different sources - a key pattern in complex workflows.\n",
        "\n",
        "Now let's create the final story generator that combines all our components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Xjf-avW8NAqZ"
      },
      "outputs": [],
      "source": [
        "# Create a chain for the story using the summary, title, and character description\n",
        "template = \"\"\"<|user|>\n",
        "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "story_chain = story_prompt | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Manual Chain Management**: The above approach gives us full control over the data flow between steps. However, LangChain provides more elegant solutions for sequential operations.\n",
        "\n",
        "Let's explore the modern way to create this same pipeline using `RunnableSequence` and helper functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b44ZR0vXRaAo",
        "outputId": "e02f8e23-5e5b-4cfa-e1f4-4fbad0085f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " In the heart of the dense, emerald jungles lived a man named John, whose unyielding love for animals was renowned far and wide. Among his cherished companions was Kali, an extraordinary tiger with piercing golden eyes that mirrored his profound wisdom. Forbidden by society's norms, their bond had flourished into something truly unique; a testament to the resilience of two spirits intertwined in mutual respect and understanding. Embarking on a remarkable journey together, they defied boundaries as John tended to Kali with unwavering compassion, while she offered him an unspoken strength that surpassed all human comprehension. Their story, \"The Bonded Journey: A Man and His Tiger,\" became an epitome of the extraordinary connection possible between mankind and nature when approached with pure love and respect.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Generate the title\n",
        "title = title_chain.invoke({\"summary\": \"A man and his tiger\"})\n",
        "\n",
        "# Step 2: Generate the character description\n",
        "character = character_chain.invoke({\"summary\": \"A man and his tiger\", \"title\": title})\n",
        "\n",
        "# Step 3: Generate the story\n",
        "story = story_chain.invoke({\"summary\": \"A man and his tiger\", \"title\": title, \"character\": character})\n",
        "\n",
        "print(story)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " In \"Unleashed Bonds: The Tale of Man and the Majestic Tiger,\" we follow Samuel, a dedicated wildlife enthusiast whose life takes an unexpected turn when he rescues a breathtakingly beautiful but captive-bound tiger from poachers. Forging a profound connection with this majestic creature, named Shadow by its striking midnight fur and piercing gaze, Samuel finds himself embarking on an extraordinary journey that transcends the boundaries of human understanding. As they traverse through a world marred by misunderstanding towards their kind, their unwavering bond faces challenges from societal norms to legal constraints, but together they champion compassion and conservation, illuminating the strength found in the heartbeats between species. Samuel's courage and Shadow's grace become symbols of hope for a future where man and beast can coexist harmoniously, their tale inspiring generations about the unleashed bonds that define true companionship.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# The llm object is already defined above\n",
        "\n",
        "# 1. Title prompt: Generates a story title given a summary\n",
        "title_prompt = PromptTemplate(\n",
        "    template=\"<|user|>Create a title for a story about {summary}. Only return the title.<|end|><|assistant|>\",\n",
        "    input_variables=[\"summary\"],\n",
        ")\n",
        "title_chain = title_prompt | llm\n",
        "\n",
        "# 2. Character prompt: Describes the main character using summary and title\n",
        "character_prompt = PromptTemplate(\n",
        "    template=\"<|user|>Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|><|assistant|>\",\n",
        "    input_variables=[\"summary\", \"title\"],\n",
        ")\n",
        "character_chain = character_prompt | llm\n",
        "\n",
        "# 3. Story prompt: Generates the story using summary, title, and character description\n",
        "story_prompt = PromptTemplate(\n",
        "    template=\"<|user|>Create a story about {summary} with the title {title}. The main character is: {character}. Only return the story and it cannot be longer than one paragraph<|end|><|assistant|>\",\n",
        "    input_variables=[\"summary\", \"title\", \"character\"],\n",
        ")\n",
        "story_chain = story_prompt | llm\n",
        "\n",
        "# 4. Helper function: Adds the generated title to the input dictionary\n",
        "def with_title(inputs):\n",
        "    title = title_chain.invoke({\"summary\": inputs[\"summary\"]})\n",
        "    return {**inputs, \"title\": title}\n",
        "\n",
        "# Helper function: Adds the generated character description to the input dictionary\n",
        "def with_character(inputs):\n",
        "    character = character_chain.invoke({\"summary\": inputs[\"summary\"], \"title\": inputs[\"title\"]})\n",
        "    return {**inputs, \"character\": character}\n",
        "\n",
        "# Compose the pipeline: sequentially generates title, character, and story\n",
        "llm_chain = (\n",
        "    RunnableLambda(with_title)\n",
        "    | RunnableLambda(with_character)\n",
        "    | RunnableLambda(lambda x: story_chain.invoke({\n",
        "        \"summary\": x[\"summary\"],\n",
        "        \"title\": x[\"title\"],\n",
        "        \"character\": x[\"character\"]\n",
        "      }))\n",
        ")\n",
        "\n",
        "# Test the chain with a sample summary\n",
        "result = llm_chain.invoke({\"summary\": \"A man and his tiger\"})\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UQ-DZ71P-D-"
      },
      "source": [
        "# Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The Core Issue: LLMs Don't Remember**\n",
        "\n",
        "- **LLMs are stateless** — they have no memory of previous conversations\n",
        "- Each prompt is treated as a completely new interaction\n",
        "- Information shared in one prompt is forgotten in the next\n",
        "\n",
        "**Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-15Eoey5EJUO",
        "outputId": "e475493c-ede1-4932-b954-ade7be05c79a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Hi Hatef! 2 times 5 is equal to 10.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's give the LLM our name\n",
        "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Hatef. What is 2 times 5?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "N42wQRl-Lykt",
        "outputId": "d1019050-017d-4dcb-a008-7029e9ebd9fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" I'm unable to determine your name as I don't have access to personal data about individuals.\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Next, we ask the LLM to reproduce the name\n",
        "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The Problem:** Poor user experience, no conversational continuity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Three Memory Types in LangChain:**\n",
        "\n",
        "1. **ConversationBufferMemory** — Store complete conversation history\n",
        "2. **ConversationBufferWindowMemory** — Keep only last k conversations\n",
        "3. **ConversationSummaryMemory** — Summarize conversation history\n",
        "\n",
        "**How Memory Works:**\n",
        "\n",
        "- **Extends prompt template** to include conversation history\n",
        "- **Automatically manages** chat context between interactions\n",
        "- **Enables continuity** in multi-turn conversations\n",
        "\n",
        "**Trade-offs to Consider:**\n",
        "\n",
        "- **Speed vs. Memory** — More history = slower processing\n",
        "- **Accuracy vs. Efficiency** — Complete history vs. compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqATEZjMgET"
      },
      "source": [
        "## ConversationBuffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**How It Works:**\n",
        "\n",
        "- Stores **entire conversation history** in memory\n",
        "- Appends **full chat context** to each new prompt\n",
        "- **No information loss** within context window limits\n",
        "\n",
        "**Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Zoo0PA1fUs70"
      },
      "outputs": [],
      "source": [
        "# Create an updated prompt template to include a chat history\n",
        "template = \"\"\"<|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Feature:** `chat_history` variable holds all previous conversations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgGMS1S9saLi"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import LLMChain\n",
        "\n",
        "# Define the type of Memory we will use\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, Prompt, and Memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mltR_GtkiqDZ",
        "outputId": "15161d8e-2520-4ffc-e104-6147e52bd5f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi! My name is Hatef. What is 2 times 5?',\n",
              " 'chat_history': '',\n",
              " 'text': \" Hello Hatef! 2 times 5 equals 10.\\n\\nHere's a quick breakdown:\\n\\n- You have two numbers, 2 and 5.\\n- The operation is multiplication (times).\\n- Multiply the first number by the second one: 2 * 5 = 10.\\n- Thus, the answer to your question is 10.\"}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate a conversation and ask a basic question\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Hatef. What is 2 times 5?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-je1rmy3dx4",
        "outputId": "5a25e2dd-5fb4-4cd8-8a5d-5da423bd3d33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Hatef. What is 2 times 5?\\nAI:  Hello Hatef! 2 times 5 equals 10.\\n\\nHere's a quick breakdown:\\n\\n- You have two numbers, 2 and 5.\\n- The operation is multiplication (times).\\n- Multiply the first number by the second one: 2 * 5 = 10.\\n- Thus, the answer to your question is 10.\",\n",
              " 'text': ' Your name is the one I have been provided, which in this case is \"AI\" or Artificial Intelligence.'}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Does the LLM remember the name we gave it?\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw3ELCg6Rpsk"
      },
      "source": [
        "## ConversationBufferMemoryWindow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "G0DRT7kjRtiC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/fd/9ctr8dys3tv3jyfrxn6_8y5m0000gp/T/ipykernel_25205/535028085.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# Retain only the last 2 conversations in memory\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, Prompt, and Memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBY69vvcR1Qq",
        "outputId": "76dbe9cc-3161-486c-b5b1-c8ce713ab1ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is 3 + 3?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  Hello Maarten, it's nice to meet you! The answer to 1 + 1 is 2.\\n\\nHowever, if you have any other questions or need further assistance, feel free to ask!\",\n",
              " 'text': \" Hello again! 3 + 3 equals 6. If there's anything else I can help you with, just let me know!\"}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ask two questions and generate two conversations in its memory\n",
        "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Hatef and I am 85 years old. What is 2 times 5?\"})\n",
        "llm_chain.invoke({\"input_prompt\":\"What is 4 + 3?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvSLfKWpR5h5",
        "outputId": "6ce15789-1eae-4817-c676-0282f22b5d40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  Hello Maarten, it's nice to meet you! The answer to 1 + 1 is 2.\\n\\nHowever, if you have any other questions or need further assistance, feel free to ask!\\nHuman: What is 3 + 3?\\nAI:  Hello again! 3 + 3 equals 6. If there's anything else I can help you with, just let me know!\",\n",
              " 'text': ' Your name is Maarten.'}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check whether it knows the name we gave it\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW7qEyctcqeJ",
        "outputId": "54e196e3-b1de-4269-c31d-2ec369efce4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my age?',\n",
              " 'chat_history': '',\n",
              " 'text': \" As a platform, I'm unable to know personal information about you such as your age. It's essential for privacy reasons.\"}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check whether it knows the age we gave it\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What Happened: Age from first interaction was forgotten because only last 2 conversations kept"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSb5OnANMhu2"
      },
      "source": [
        "## ConversationSummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lWHZlJUbwpqE"
      },
      "outputs": [],
      "source": [
        "# Create a summary prompt template\n",
        "summary_prompt_template = \"\"\"<|user|>Summarize the conversations and update with the new lines.\n",
        "\n",
        "Current summary:\n",
        "{summary}\n",
        "\n",
        "new lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"new_lines\", \"summary\"],\n",
        "    template=summary_prompt_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qg1HAgxZMkbO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/fd/9ctr8dys3tv3jyfrxn6_8y5m0000gp/T/ipykernel_25205/4216441337.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "# Define the type of memory we will use\n",
        "memory = ConversationSummaryMemory(\n",
        "    llm=llm,\n",
        "    memory_key=\"chat_history\",\n",
        "    prompt=summary_prompt\n",
        ")\n",
        "\n",
        "# Chain the LLM, prompt, and memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2klIk9CpVSH0",
        "outputId": "1edddb31-7703-4a54-c758-03a6d459e783"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': ' Hatef introduced themselves and asked the AI for the result of 2 times 5, which the AI confirmed as 10. The simple calculation provided further confirms that 2 multiplied by 5 equals 10.',\n",
              " 'text': \" I don't have the capability to recall personal information about individuals unless it has been shared with me in the course of our conversation. Therefore, I cannot know your name.\"}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate a conversation and ask for the name\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Hatef. What is 2 times 5?\"})\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VdOH_I-V-Fy",
        "outputId": "b20dc2a4-1be8-40dd-d683-955b32900875"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_prompt': 'What was the first question I asked?',\n",
              " 'chat_history': ' Hatef initiated the conversation and requested the AI to calculate 2 times 5, which was confirmed by the AI as 10. Subsequently, when asked about their name, the AI clarified its inability to recall personal information shared within the ongoing dialogue.',\n",
              " 'text': ' The first question you asked was: \"calculate 2 times 5.\"'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check whether it has summarized everything thus far\n",
        "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1_LlvrVX9HL",
        "outputId": "f1c989a2-76e2-4348-cbe5-8a4f5a551dc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'chat_history': ' Hatef started the conversation and asked the AI to calculate 2 times 5, resulting in an answer of 10. Later, when inquiring about their name, the AI reminded that it cannot recall personal information shared during the current chat session. When prompted about the initial query, the AI confirmed it was \"calculate 2 times 5.\"'}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check what the summary is thus far\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG5sJa1qvS4N"
      },
      "source": [
        "# Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcBt8bZM56dM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Load OpenAI's LLMs with LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\"\n",
        "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmRZu8DO2p6k"
      },
      "outputs": [],
      "source": [
        "# Create the ReAct template\n",
        "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=react_template,\n",
        "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV-ssNa-4zOK"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools, Tool\n",
        "from langchain.tools import DuckDuckGoSearchResults\n",
        "\n",
        "# You can create the tool to pass to an agent\n",
        "search = DuckDuckGoSearchResults()\n",
        "search_tool = Tool(\n",
        "    name=\"duckduck\",\n",
        "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
        "    func=search.run,\n",
        ")\n",
        "\n",
        "# Prepare tools\n",
        "tools = load_tools([\"llm-math\"], llm=openai_llm)\n",
        "tools.append(search_tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tAr1962vS4T"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "# Construct the ReAct agent\n",
        "agent = create_react_agent(openai_llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSU6ECdYBOOm",
        "outputId": "b6cf304c-c0f2-4939-b682-2f72d7e5a078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to find the current price of a MacBook Pro in USD first before converting it to EUR.\n",
            "Action: duckduck\n",
            "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3m[snippet: View at Best Buy. The best MacBook Pro overall The MacBook Pro 14-inch with the latest M3-series chips offers outstanding, best-in-class performance while getting fantastic battery life and ..., title: The best MacBook Pro in 2024: our picks for the top Pro models, link: https://www.techradar.com/best/best-macbook-pro], [snippet: Starts at $1,299. Upgradable to 24 GB of memory and 2 TB of storage. 67W USB-C charger included. The M2-powered MacBook Pro is available now for a starting price of $1,299 on Apple's website ..., title: MacBook Pro 13-inch (M2, 2022) review | Tom's Guide, link: https://www.tomsguide.com/reviews/macbook-pro-13-inch-m2-2022], [snippet: The late-2023 MacBook Pro update also marks the demise of the 13-inch MacBook Pro, which has been replaced by a 14-inch model with the standard M3 chip, unfortunately at a higher price than the ..., title: Best MacBook Pro Deals: March 2024 | Macworld, link: https://www.macworld.com/article/672811/best-macbook-pro-deals.html], [snippet: For the M3 Pro models, prices start at $2,249.00 for the 512GB/18GB RAM 16-inch MacBook Pro and increase to $2,649.00 for the 512GB/36GB RAM model, both of which are all-time low prices., title: Best Buy Introduces All-Time Low Prices on Apple's M3 MacBook Pro for ..., link: https://www.macrumors.com/2024/04/01/best-buy-m3-macbook-pro/]\u001b[0m\u001b[32;1m\u001b[1;3mI found the current price of a MacBook Pro in USD, now I need to convert it to EUR using the exchange rate.\n",
            "Action: Calculator\n",
            "Action Input: $2,249.00 * 0.85\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 1911.6499999999999\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: The current price of a MacBook Pro in USD is $2,249.00. It would cost approximately 1911.65 EUR with an exchange rate of 0.85 EUR for 1 USD.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?',\n",
              " 'output': 'The current price of a MacBook Pro in USD is $2,249.00. It would cost approximately 1911.65 EUR with an exchange rate of 0.85 EUR for 1 USD.'}"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# What is the Price of a MacBook Pro?\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n",
        "    }\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
