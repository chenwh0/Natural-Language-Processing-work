{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da42a049",
   "metadata": {},
   "source": [
    "# Multi-Agent Rag Lab\n",
    "\n",
    "Throughout this course, we have worked extensively with Python notebooks (JupyterHub/Colab), which are excellent for learning, experimentation, and visualization. However, to implement solutions in real-world scenarios, we need to:\n",
    "\n",
    "1. Organize code into modular Python files (.py)\n",
    "2. Understand proper project structure and file organization\n",
    "3. Learn to run models on our own hardware (not cloud services)\n",
    "4. Manage dependencies and environments independently\n",
    "5. Build systems with multiple independently running components\n",
    "\n",
    "**Important Note**: This lab is designed to be run **locally** on your own computer. It is **not intended** for cloud-based environments such as JupyterHub, Google Colab, Hellbender, Nautilus, or similar platforms. While it may be possible to adapt this lab for cloud environments, it would require additional considerations and modifications that are beyond the scope of this tutorial.\n",
    "\n",
    "This notebook guides you through building a multi-agent RAG system with:\n",
    "- Two specialized agents (Retrieval Agent & Response Agent)\n",
    "- HuggingFace SQuAD dataset\n",
    "- Small LLMs\n",
    "- Simulated MCP servers\n",
    "\n",
    "## Prerequisites\n",
    "- Previous knowledge about LLMs and how to inference them\n",
    "- Basic NLP knowledge\n",
    "- Ideally, this should be run in a high-performance environment\n",
    "- To address the resource constraints we are using smaller extractive models. It still might take some time to run.\n",
    "- Python is installed on your device or environment\n",
    "\n",
    "## LAB STRUCTURE:\n",
    "1. Setup & Installation\n",
    "2. Understanding the Architecture\n",
    "3. Building Retrieval Agent (extractor.py)\n",
    "4. Building Response Agent (generator.py)\n",
    "5. Orchestration System (client.py)\n",
    "6. Exercises & Experiments\n",
    "\n",
    "## STEP 1: Create and Activate Virtual Environment\n",
    "\n",
    "Virtual environments keep dependencies isolated and prevent conflicts.\n",
    "\n",
    "### FOR WINDOWS:\n",
    "\n",
    "Open Command Prompt or PowerShell:\n",
    "\n",
    "```\n",
    "cd path/to/your/project\n",
    "python -m venv rag_env\n",
    "rag_env\\Scripts\\activate\n",
    "```\n",
    "\n",
    "If an Unauthorized access error occurs, use the command:\n",
    "```\n",
    "Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\n",
    "```\n",
    "before running `rag_env\\Scripts\\activate`\n",
    "\n",
    "If you need to run multiple terminals for the same environment, just use:\n",
    "```\n",
    "cd path/to/your/project\n",
    "Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\n",
    "rag_env\\Scripts\\activate\n",
    "```\n",
    "\n",
    "Because the python environment (rag_env) you created will be saved in your project folder\n",
    "\n",
    "### FOR MAC/LINUX:\n",
    "\n",
    "Open Terminal:\n",
    "\n",
    "```bash\n",
    "cd path/to/your/project\n",
    "python3 -m venv rag_env\n",
    "source rag_env/bin/activate\n",
    "```\n",
    "\n",
    "For subsequent terminal sessions, navigate to your project and run:\n",
    "```bash\n",
    "cd path/to/your/project\n",
    "source rag_env/bin/activate\n",
    "```\n",
    "\n",
    "### VERIFY ACTIVATION:\n",
    "\n",
    "**Windows:** You should see (rag_env) at the start of your command line:\n",
    "```\n",
    "(rag_env) C:\\Users\\YourName\\project>\n",
    "```\n",
    "\n",
    "**Mac/Linux:** You should see (rag_env) at the start of your terminal prompt:\n",
    "```\n",
    "(rag_env) username@computername:~/project$\n",
    "```\n",
    "\n",
    "**<span style=\"color: red;\">From this point onwards we assume that you are running commands in this **rag_env** python environment.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61782b4d",
   "metadata": {},
   "source": [
    "## STEP 2: Install Required Libraries in Virtual Environment\n",
    "\n",
    "Make sure your virtual environment is activated (see VERIFY ACTIVATION section above)\n",
    "\n",
    "### INSTALLATION COMMAND:\n",
    "\n",
    "Copy and paste this entire command into your terminal:\n",
    "\n",
    "**For all platforms:**\n",
    "```bash\n",
    "pip install --upgrade pip\n",
    "pip install mcp datasets sentence-transformers torch transformers\n",
    "```\n",
    "\n",
    "**Note for Mac users with Apple Silicon (M1/M2/M3):**\n",
    "If you encounter issues with PyTorch installation, you may need to install the Apple Silicon optimized version:\n",
    "```bash\n",
    "pip install torch torchvision torchaudio\n",
    "```\n",
    "\n",
    "## Project Directory Structure\n",
    "\n",
    "Let's create the proper folder structure for the lab.\n",
    "\n",
    "We plan to save our python files using following structure:\n",
    "```\n",
    "path/to/your/project/\n",
    "├── extractor.py\n",
    "├── generator.py\n",
    "├── client.py\n",
    "```\n",
    "\n",
    "## UNDERSTANDING INDEPENDENT AGENTS\n",
    "\n",
    "**KEY CONCEPT**: In notebooks, code runs sequentially in cells.\n",
    "In this lab, you're building INDEPENDENT agents that:\n",
    "- Live in separate Python files (when deployed in larger scale in servers)\n",
    "- Have their own responsibilities\n",
    "- Communicate through well-defined interfaces (MCP protocol)\n",
    "- Can be tested and deployed independently\n",
    "\n",
    "This is how real-world AI systems are built.\n",
    "\n",
    "### ARCHITECTURE OF INDEPENDENT AGENTS:\n",
    "\n",
    "**RETRIEVAL AGENT (extractor.py):**\n",
    "- **Role**: Information specialist\n",
    "- **Independence**: Can run and be tested without Response Agent\n",
    "- **Responsibilities**:\n",
    "  - Decide HOW to search (strategy selection)\n",
    "  - Execute searches across multiple sources\n",
    "  - Rank and filter results\n",
    "  - Return structured data\n",
    "- **Interface**: receive query → return ranked results\n",
    "\n",
    "**RESPONSE AGENT (generator.py):**\n",
    "- **Role**: Question understanding and answer generation\n",
    "- **Independence**: Can run with ANY retrieval system\n",
    "- **Responsibilities**:\n",
    "  - Analyze user questions\n",
    "  - Determine information needs\n",
    "  - Request data from Retrieval Agent\n",
    "  - Synthesize answers\n",
    "- **Interface**: receive question → return answer\n",
    "\n",
    "**KEY INSIGHT:**\n",
    "The agents don't need to know HOW each other works internally. They only need to know each other's INTERFACE (inputs/outputs).\n",
    "\n",
    "This is a fundamental principle that makes systems:\n",
    "- Easier to understand\n",
    "- Easier to modify\n",
    "- Easier to test\n",
    "- More reliable\n",
    "\n",
    "**COMMUNICATION FLOW:**\n",
    "1. client creates both agents\n",
    "2. user asks a question\n",
    "3. client receives the question\n",
    "4. client sends the question as a query to extractor\n",
    "5. extractor receives the query\n",
    "6. extractor sends the related context to client\n",
    "7. client receives the context to answer the question\n",
    "8. client sends question and context to generator\n",
    "9. generator generates the answer and sends back to client\n",
    "10. client presents the answer to user\n",
    "\n",
    "This is fundamentally different from a notebook where everything is in one scope!\n",
    "\n",
    "***More advanced independent agents can learn by experience!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748de4e0",
   "metadata": {},
   "source": [
    "## STEP 3: Building the Extractor Agent: extractor.py\n",
    "\n",
    "**FILE**: extractor.py\n",
    "\n",
    "Copy this entire code block to a file named 'extractor.py' in your project directory\n",
    "\n",
    "The Retrieval Agent is responsible for:\n",
    "- Loading SQuAD dataset\n",
    "- Save it in a simple vector store\n",
    "- Executing searches using the vector store\n",
    "- Ranking and filtering results\n",
    "- Returning high-quality context for answer generation\n",
    "\n",
    "We will use SQUAD dataset as a simple example: https://huggingface.co/datasets/rajpurkar/squad\n",
    "\n",
    "***In a standard AI agent system, each agent is a self-contained process that can:***\n",
    "1. Communicate with other agents through a standard protocol (MCP/jsonrpc)\n",
    "2. Expose tools (capabilities) that other agents can discover and use (get_tools method)\n",
    "3. Handle structured requests and responses via an agreed-upon message format (call_tool method)\n",
    "4. Operate independently yet participate in a larger orchestration network\n",
    "\n",
    "MCP (Model Context Protocol) is an open standard introduced by Anthropic (and adopted by OpenAI, Hugging Face, etc.) that defines how models, tools, and clients communicate in a consistent, interoperable way. It's designed to make agents and tools portable across environments — e.g., the same agent can be used in Claude, ChatGPT, or a local orchestrator.\n",
    "\n",
    "It defines the structure for:\n",
    "1. Initialization (initialize)\n",
    "2. Tool discovery (tools/list)\n",
    "3. Tool invocation (tools/call)\n",
    "4. Events, resources, and capabilities\n",
    "\n",
    "Note how we structure our agent using these principles in the following agent:\n",
    "\n",
    "```python\n",
    "#extractor.py\n",
    "\"\"\"\n",
    "Extractor MCP Server - loads SQuAD dataset and performs semantic retrieval.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "sys.stdout.reconfigure(line_buffering=True)\n",
    "\n",
    "class ExtractorServer:\n",
    "    def __init__(self):\n",
    "        print(\"Loading SQuAD dataset (first 100 rows)\", file=sys.stderr)\n",
    "        #To keep this lightweight we will only use the first 100 rows of the dataset\n",
    "        dataset = load_dataset(\"squad\", split=\"train[:100]\")\n",
    "        \n",
    "        self.documents = [{\"title\": row[\"title\"], \"context\": row[\"context\"]} for row in dataset]\n",
    "\n",
    "        print(\"Loading embedding model\", file=sys.stderr)\n",
    "        #Using the embedding model \"all-MiniLM-L6-v2\"\n",
    "        self.encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        self.doc_embeddings = self.encoder.encode(\n",
    "            [d[\"context\"] for d in self.documents],\n",
    "            show_progress_bar=False, \n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        print(\"Extractor ready with (100 docs embedded)\", file=sys.stderr)\n",
    "\n",
    "    #Define Server info \n",
    "    def get_server_info(self) -> Dict[str, Any]:\n",
    "        return {\"name\": \"extractor-server\", \"version\": \"2.0.1\"}\n",
    "\n",
    "    #Defining the extraction of relevant data as a tool. These agents use a variety of tools to conduct their operation\n",
    "    #We name this tool \"extract_info\"\n",
    "    #Other agents can call this method and find out what tools this agent has.\n",
    "    def get_tools(self) -> List[Dict[str, Any]]:\n",
    "        return [{\n",
    "            \"name\": \"extract_info\",\n",
    "            \"description\": \"Performs semantic retrieval using embedded SQuAD contexts.\",\n",
    "            \"inputSchema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"question\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"question\"]\n",
    "            }\n",
    "        }]\n",
    "\n",
    "    #Defining the call_tool method\n",
    "    #If other agents want to use this agent, they can call this method, \n",
    "    #and specify that they need to use \"extract_info\" method and give the inputs that it asks.\n",
    "    def call_tool(self, name: str, args: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        if name != \"extract_info\":\n",
    "            raise ValueError(f\"Unknown tool: {name}\")\n",
    "        return self._extract_info(args)\n",
    "\n",
    "    #Defining the _extract_info method\n",
    "    #This is the actual action that the method will execute\n",
    "\n",
    "    def _extract_info(self, args: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        #For a given question it will find the top k related documents.\n",
    "        q = args[\"question\"]\n",
    "        q_emb = self.encoder.encode([q], convert_to_numpy=True)[0]\n",
    "        sims = np.dot(self.doc_embeddings, q_emb)\n",
    "        top = np.argsort(sims)[-3:][::-1]\n",
    "\n",
    "        docs = [self.documents[i] for i in top]\n",
    "        combined = \"\\n\\n\".join([f\"{i+1}. {d['context']}\" for i, d in enumerate(docs)])\n",
    "        #Notice that how we format these responses. This is a Standard Structure in MCPs. \n",
    "        #So that different agents can communicate using these standard data structures.\n",
    "        return {\"content\": [{\"type\": \"text\", \"text\": combined}]} \n",
    "\n",
    "async def main():\n",
    "    #Creating extractor agent as a server\n",
    "    srv = ExtractorServer()\n",
    "\n",
    "    #Now, the following part is about setting up the server.\n",
    "    #Most of the following code is standard practice.\n",
    "    #This specify the structure of input and output structures, so that any other agent can discover this service and call the methods/tools\n",
    "    #MCP protocol use jsonrpc as the underlying communication protocol.\n",
    "    while True:\n",
    "        line = sys.stdin.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        try:\n",
    "            req = json.loads(line.strip())\n",
    "            if req[\"method\"] == \"initialize\":\n",
    "                res = {\"jsonrpc\": \"2.0\",\"id\": req[\"id\"],\n",
    "                       \"result\": {\"protocolVersion\": \"2024-11-05\",\n",
    "                                  \"capabilities\": {},\n",
    "                                  \"serverInfo\": srv.get_server_info()}}\n",
    "            elif req[\"method\"] == \"tools/list\":\n",
    "                res = {\"jsonrpc\": \"2.0\",\"id\": req[\"id\"],\n",
    "                       \"result\": {\"tools\": srv.get_tools()}}\n",
    "            elif req[\"method\"] == \"tools/call\":\n",
    "                name = req[\"params\"][\"name\"]\n",
    "                args = req[\"params\"][\"arguments\"]\n",
    "                result = srv.call_tool(name, args)\n",
    "                res = {\"jsonrpc\": \"2.0\",\"id\": req[\"id\"],\"result\": result}\n",
    "            else:\n",
    "                res = {\"jsonrpc\": \"2.0\",\"id\": req[\"id\"],\n",
    "                       \"error\": {\"code\": -32601,\"message\": \"Unknown method\"}}\n",
    "            print(json.dumps(res), flush=True)\n",
    "        except Exception as e:\n",
    "            print(json.dumps({\"jsonrpc\": \"2.0\",\"id\": None,\n",
    "                              \"error\": {\"code\": -32603,\"message\": str(e)}}), flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Note, that this server can be run alone, independent of other agents or services.\n",
    "    asyncio.run(main())\n",
    "```\n",
    "\n",
    "To run this agent independently in the terminal:\n",
    "\n",
    "**Windows:**\n",
    "```\n",
    "python extractor.py\n",
    "```\n",
    "\n",
    "**Mac/Linux:**\n",
    "```bash\n",
    "python3 extractor.py\n",
    "```\n",
    "\n",
    "If you can see the following output, your agent is working perfectly:\n",
    "```\n",
    "Loading SQuAD dataset (first 100 rows)\n",
    "Loading embedding model\n",
    "Extractor ready with (100 docs embedded)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e2d07",
   "metadata": {},
   "source": [
    "\n",
    "## STEP 4: Building the Response Agent: generator.py\n",
    "\n",
    "**FILE**: generator.py\n",
    "\n",
    "Copy this entire code block to a file named 'generator.py' in your project directory\n",
    "\n",
    "The Response Agent is responsible for:\n",
    "- Loading LLMs\n",
    "- Generating final answers\n",
    "\n",
    "Note that this generator agent is also a standard Agent.\n",
    "\n",
    "***This also follows the same standards:***\n",
    "1. Communicate with other agents through a standard protocol \n",
    "2. Expose tools (capabilities) that other agents can discover and use \n",
    "3. Handle structured requests and responses via an agreed-upon message format\n",
    "4. Operate independently yet participate in a larger orchestration network\n",
    "\n",
    "```python\n",
    "#generator.py\n",
    "\"\"\"\n",
    "Generator MCP Server - uses FLAN-T5-small to generate answers.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "sys.stdout.reconfigure(line_buffering=True)\n",
    "\n",
    "class GeneratorServer:\n",
    "    def __init__(self):\n",
    "        print(\"Loading FLAN-T5-small.\", file=sys.stderr)\n",
    "        self.llm = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=\"google/flan-t5-small\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        print(\"Generator ready!\", file=sys.stderr)\n",
    "\n",
    "    def get_server_info(self) -> Dict[str, Any]:\n",
    "        return {\"name\": \"generator-server\", \"version\": \"2.0.1\"}\n",
    "\n",
    "    #Defining the structure of the tool generate_answer\n",
    "    def get_tools(self) -> List[Dict[str, Any]]:\n",
    "        return [{\n",
    "            \"name\": \"generate_answer\",\n",
    "            \"description\": \"Generate a concise answer using FLAN-T5-small given question + context.\",\n",
    "            \"inputSchema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"question\": {\"type\": \"string\"},\n",
    "                    \"context\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"question\", \"context\"]\n",
    "            }\n",
    "        }]\n",
    "\n",
    "    #Method to call the tool\n",
    "    def call_tool(self, name: str, args: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        if name != \"generate_answer\":\n",
    "            raise ValueError(f\"Unknown tool: {name}\")\n",
    "        return self._generate(args)\n",
    "\n",
    "    #_generate the answer using the given context\n",
    "    def _generate(self, args: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        #Arguments passed by the service caller contains the information needed in this context \"question\" and \"context\"\n",
    "        q, ctx = args[\"question\"], args[\"context\"]\n",
    "        prompt = f\"Answer the question based on context.\\n\\nContext:\\n{ctx}\\n\\nQuestion: {q}\\nAnswer:\"\n",
    "        out = self.llm(prompt, max_length=200, num_return_sequences=1)[0][\"generated_text\"]\n",
    "        return {\"content\": [{\"type\": \"text\", \"text\": out}]}\n",
    "\n",
    "async def main():\n",
    "    #Creation of the generator agent as separate server\n",
    "    srv = GeneratorServer()\n",
    "    while True:\n",
    "        line = sys.stdin.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        try:\n",
    "            req = json.loads(line.strip())\n",
    "            if req[\"method\"] == \"initialize\":\n",
    "                res = {\"jsonrpc\":\"2.0\",\"id\":req[\"id\"],\n",
    "                       \"result\":{\"protocolVersion\":\"2024-11-05\",\n",
    "                                 \"capabilities\":{},\n",
    "                                 \"serverInfo\":srv.get_server_info()}}\n",
    "            elif req[\"method\"] == \"tools/list\":\n",
    "                res = {\"jsonrpc\":\"2.0\",\"id\":req[\"id\"],\n",
    "                       \"result\":{\"tools\":srv.get_tools()}}\n",
    "            elif req[\"method\"] == \"tools/call\":\n",
    "                name=req[\"params\"][\"name\"]\n",
    "                args=req[\"params\"][\"arguments\"]\n",
    "                result=srv.call_tool(name,args)\n",
    "                res={\"jsonrpc\":\"2.0\",\"id\":req[\"id\"],\"result\":result}\n",
    "            else:\n",
    "                res={\"jsonrpc\":\"2.0\",\"id\":req[\"id\"],\n",
    "                     \"error\":{\"code\":-32601,\"message\":\"Unknown method\"}}\n",
    "            print(json.dumps(res),flush=True)\n",
    "        except Exception as e:\n",
    "            print(json.dumps({\"jsonrpc\":\"2.0\",\"id\":None,\n",
    "                              \"error\":{\"code\":-32603,\"message\":str(e)}}),flush=True)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    asyncio.run(main())\n",
    "```\n",
    "\n",
    "Now you can run this independently:\n",
    "\n",
    "**Windows:**\n",
    "```\n",
    "python generator.py\n",
    "```\n",
    "\n",
    "**Mac/Linux:**\n",
    "```bash\n",
    "python3 generator.py\n",
    "```\n",
    "\n",
    "If you can see this response, you can notice that now the small LLM is loaded and ready to use:\n",
    "```\n",
    "Loading FLAN-T5-small.\n",
    "Generator ready!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1191391",
   "metadata": {},
   "source": [
    "## STEP 5: Bringing it all together\n",
    "\n",
    "**FILE**: client.py\n",
    "\n",
    "Copy this entire code block to a file named 'client.py' in your project folder\n",
    "\n",
    "This file orchestrates the entire system:\n",
    "- Starts both agents\n",
    "- Initializes MCP connections\n",
    "- Provides the main query interface\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Q&A MCP Orchestrator Client - coordinates extractor + generator.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import Dict, Any\n",
    "\n",
    "class QAClient:\n",
    "    def __init__(self):\n",
    "        self.req_id = 0\n",
    "        self.servers: Dict[str, subprocess.Popen] = {}\n",
    "\n",
    "    def _next_id(self):\n",
    "        self.req_id += 1\n",
    "        return self.req_id\n",
    "        \n",
    "   #Standard methods to communicate with the other agents/services\n",
    "    async def _send(self, proc, req):\n",
    "        proc.stdin.write(json.dumps(req) + \"\\n\")\n",
    "        proc.stdin.flush()\n",
    "        line = await asyncio.get_event_loop().run_in_executor(None, proc.stdout.readline)\n",
    "        if not line:\n",
    "            raise RuntimeError(\"No response from subprocess; check stderr for errors.\")\n",
    "        return json.loads(line.strip())\n",
    "\n",
    "    #Standard methods connect with other services\n",
    "    async def connect(self, name: str, script: str):\n",
    "        #Since we are using this orchestrator to start other services, we set it up as two subprocesses\n",
    "        proc = subprocess.Popen([sys.executable, script],\n",
    "                                stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n",
    "                                stderr=subprocess.PIPE, text=True)\n",
    "        self.servers[name] = proc\n",
    "        print(f\"Starting {name}...\", file=sys.stderr)\n",
    "        await asyncio.sleep(3)  # allow time to load the models, if loading of your models takes time, and if you get a timeout error, \n",
    "                                # try increasing this sleep time\n",
    "\n",
    "        await self._send(proc, {\"jsonrpc\": \"2.0\", \"id\": self._next_id(),\n",
    "                                \"method\": \"initialize\",\n",
    "                                \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}}})\n",
    "        tools = await self._send(proc, {\"jsonrpc\": \"2.0\", \"id\": self._next_id(),\n",
    "                                        \"method\": \"tools/list\", \"params\": {}})\n",
    "        print(f\"Connected to {name} ({[t['name'] for t in tools['result']['tools']]})\") \n",
    "\n",
    "    #Method to call the tools\n",
    "    async def call_tool(self, server: str, tool: str, args: Dict[str, Any]):\n",
    "        proc = self.servers[server]\n",
    "        req = {\"jsonrpc\": \"2.0\", \"id\": self._next_id(),\n",
    "               \"method\": \"tools/call\",\n",
    "               \"params\": {\"name\": tool, \"arguments\": args}}\n",
    "        res = await self._send(proc, req)\n",
    "        return res[\"result\"][\"content\"][0][\"text\"]\n",
    "\n",
    "    async def run(self):\n",
    "        #Now notice in this run method, we are calling the two agents that we created.\n",
    "        await self.connect(\"extractor\", \"extractor.py\")\n",
    "        await self.connect(\"generator\", \"generator.py\")\n",
    "\n",
    "        #Now we create a terminal input option to input a question.\n",
    "        print(\"\\nAsk a question (type 'quit' to exit)\")\n",
    "        while True:\n",
    "            q = input(\"\\nQ> \").strip()\n",
    "            if q.lower() in {\"quit\", \"exit\"}:\n",
    "                break\n",
    "\n",
    "            print(\"Extracting relevant context...\")\n",
    "\n",
    "            #This is how we call a tool, we are calling the extractor agent, extract_info method and passing down the question\n",
    "            context = await self.call_tool(\"extractor\", \"extract_info\", {\"question\": q})\n",
    "\n",
    "            \n",
    "            print(\"Retrieved context snippet:\")\n",
    "            print(context[:300] + \"...\" if len(context) > 300 else context)\n",
    "\n",
    "            print(\"\\nGenerating answer...\")\n",
    "\n",
    "            #Now that we have the context, let's call the generator to generate the answer.\n",
    "            #Notice that now we call, generator agent, and generate_answer tool and passing down \n",
    "            #question and context both.\n",
    "\n",
    "            answer = await self.call_tool(\"generator\", \"generate_answer\",\n",
    "                                          {\"question\": q, \"context\": context})\n",
    "            print(\"\\nFinal Answer:\\n\" + answer)\n",
    "\n",
    "        for p in self.servers.values():\n",
    "            p.terminate()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(QAClient().run())\n",
    "```\n",
    "\n",
    "To run the complete system:\n",
    "\n",
    "**Windows:**\n",
    "```\n",
    "python client.py\n",
    "```\n",
    "\n",
    "**Mac/Linux:**\n",
    "```bash\n",
    "python3 client.py\n",
    "```\n",
    "\n",
    "If you can see the following output, your system is working correctly:\n",
    "```\n",
    "Starting extractor...\n",
    "Connected to extractor (['extract_info'])\n",
    "Starting generator...\n",
    "Connected to generator (['generate_answer'])\n",
    "\n",
    "Ask a question (type 'quit' to exit)\n",
    "\n",
    "Q> \n",
    "```\n",
    "\n",
    "\n",
    "Note that based on your system performance this might take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e2fe7",
   "metadata": {},
   "source": [
    "## STEP 6: Running experiments\n",
    "\n",
    "Try asking \"What is in front of the Notre Dame Main Building?\"\n",
    "\n",
    "Remember that we have only used first 100 rows of the dataset. So use questions from that context.\n",
    "\n",
    "Now you know the basic standards and the way to setup an agentic AI framework. You can customize these agents to add more tools and use them as you need!\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **Permission errors on Windows**: Make sure to run `Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass` before activating your environment.\n",
    "\n",
    "2. **Python command not found on Mac/Linux**: Try using `python3` instead of `python` for all commands.\n",
    "\n",
    "3. **Model loading takes too long**: If you get timeout errors, increase the `await asyncio.sleep(3)` time in client.py to `await asyncio.sleep(5)` or higher.\n",
    "\n",
    "4. **Memory issues**: The models are kept small to accommodate various systems, but if you still face memory issues, consider closing other applications.\n",
    "\n",
    "5. **Import errors**: Make sure your virtual environment is activated and all packages are installed correctly.\n",
    "\n",
    "## Cloud Platform Considerations\n",
    "\n",
    "If you want to adapt this lab for cloud platforms (JupyterHub, Google Colab, etc.), consider these additional requirements:\n",
    "- Modified subprocess management for containerized environments\n",
    "- Adjusted file paths and permissions\n",
    "- Potential networking restrictions for inter-process communication\n",
    "- Resource limitations that may affect model loading\n",
    "- Different Python executable paths and environment management\n",
    "\n",
    "These adaptations are beyond the scope of this tutorial and would require platform-specific modifications."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "bash",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
