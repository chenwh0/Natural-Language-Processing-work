{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "668400cf86d14b45b6e41261c5288fed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efa04bbae7d34c9396e9933070fe2fac",
              "IPY_MODEL_6333db661f6340e2bcfd5f5410ff7994",
              "IPY_MODEL_e9b7dd9d037740c79dfb39b2e6b57609"
            ],
            "layout": "IPY_MODEL_77e249c5e48f449b9eb9adf20769e075"
          }
        },
        "efa04bbae7d34c9396e9933070fe2fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47d7f0b27934449db06d5cbf04389921",
            "placeholder": "​",
            "style": "IPY_MODEL_b747b5ae55904b92ac3d6143082094f8",
            "value": "tokenizer_config.json: "
          }
        },
        "6333db661f6340e2bcfd5f5410ff7994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f280b9f01b9c4d6992aee37b7e50c09c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54d29471364741bbac13876779ca662b",
            "value": 1
          }
        },
        "e9b7dd9d037740c79dfb39b2e6b57609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2032edc58180445ea8854aa8b7cd1d80",
            "placeholder": "​",
            "style": "IPY_MODEL_37b58aed4ffd46e7b86a12cbb81480b8",
            "value": " 1.29k/? [00:00&lt;00:00, 87.6kB/s]"
          }
        },
        "77e249c5e48f449b9eb9adf20769e075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47d7f0b27934449db06d5cbf04389921": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b747b5ae55904b92ac3d6143082094f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f280b9f01b9c4d6992aee37b7e50c09c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "54d29471364741bbac13876779ca662b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2032edc58180445ea8854aa8b7cd1d80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37b58aed4ffd46e7b86a12cbb81480b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dc09e533d9a48e1a39a3a66108109b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a5d384c28c44d7fada4bb138a140566",
              "IPY_MODEL_579b3d71bb46448c8103736676ff9501",
              "IPY_MODEL_7e36ea2c0af94a309ec4654da557e525"
            ],
            "layout": "IPY_MODEL_f0de0fc2401e41c8b2eff35ee2585edc"
          }
        },
        "5a5d384c28c44d7fada4bb138a140566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a1f2b4f0cc9411ab44570e3bb03a0ab",
            "placeholder": "​",
            "style": "IPY_MODEL_57333c356a6740549a216e9263599ef5",
            "value": "tokenizer.model: 100%"
          }
        },
        "579b3d71bb46448c8103736676ff9501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3ee9b48d72b48d38a8ef769de989ca6",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b6defa061b44d99b340ae09d27a4bf3",
            "value": 499723
          }
        },
        "7e36ea2c0af94a309ec4654da557e525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca8199a1df254707929a9f82f7b14406",
            "placeholder": "​",
            "style": "IPY_MODEL_c82676caa44e42fcb81954bf267320b2",
            "value": " 500k/500k [00:00&lt;00:00, 1.37MB/s]"
          }
        },
        "f0de0fc2401e41c8b2eff35ee2585edc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a1f2b4f0cc9411ab44570e3bb03a0ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57333c356a6740549a216e9263599ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3ee9b48d72b48d38a8ef769de989ca6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6defa061b44d99b340ae09d27a4bf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca8199a1df254707929a9f82f7b14406": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c82676caa44e42fcb81954bf267320b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e8677ceae414adebc7ba176e4a8efaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86559e59b27f4149a6324df7c960de1c",
              "IPY_MODEL_db2798cc70b049c485dc027edf665826",
              "IPY_MODEL_61ee3cfe26c64440ad95ab5300293775"
            ],
            "layout": "IPY_MODEL_2e6bb82724944a9fa20b27a5401710af"
          }
        },
        "86559e59b27f4149a6324df7c960de1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c76a3aadb1284f1ca8518757275b4219",
            "placeholder": "​",
            "style": "IPY_MODEL_37cc52a13d1047e3b34fd48da6020c72",
            "value": "tokenizer.json: "
          }
        },
        "db2798cc70b049c485dc027edf665826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_175c85730a884f769a88611824b9c1b3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c83f0ac15e54ff69ae37e2a5bedfe37",
            "value": 1
          }
        },
        "61ee3cfe26c64440ad95ab5300293775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e37ce5a22d3408c9caac04cc40a1de3",
            "placeholder": "​",
            "style": "IPY_MODEL_653215c47580484d9ac8431180fac17d",
            "value": " 1.84M/? [00:00&lt;00:00, 41.4MB/s]"
          }
        },
        "2e6bb82724944a9fa20b27a5401710af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c76a3aadb1284f1ca8518757275b4219": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37cc52a13d1047e3b34fd48da6020c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "175c85730a884f769a88611824b9c1b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6c83f0ac15e54ff69ae37e2a5bedfe37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e37ce5a22d3408c9caac04cc40a1de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "653215c47580484d9ac8431180fac17d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4462702d70074517a22b01206604540f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad33ace25163437ab4e1d69cbf57aae9",
              "IPY_MODEL_666ed606434744a6be8ba73edd85c744",
              "IPY_MODEL_d67160948de344df9e6e0a7ec3999261"
            ],
            "layout": "IPY_MODEL_e5201f4a820b46dd8f009b0f4e957e82"
          }
        },
        "ad33ace25163437ab4e1d69cbf57aae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e4c24fdc604f88a28c34a600eaf83f",
            "placeholder": "​",
            "style": "IPY_MODEL_5083ce495c2548c7981e5164424a3ffd",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "666ed606434744a6be8ba73edd85c744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc4764c6ed0c49c7979cb92f3a4a1838",
            "max": 551,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88cc4e3166eb4d8c9799026620d7315b",
            "value": 551
          }
        },
        "d67160948de344df9e6e0a7ec3999261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6d795bf5fb64ffda84942d9503b76b7",
            "placeholder": "​",
            "style": "IPY_MODEL_71a175b82395433cbfff8245c2354da7",
            "value": " 551/551 [00:00&lt;00:00, 55.3kB/s]"
          }
        },
        "e5201f4a820b46dd8f009b0f4e957e82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7e4c24fdc604f88a28c34a600eaf83f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5083ce495c2548c7981e5164424a3ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc4764c6ed0c49c7979cb92f3a4a1838": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88cc4e3166eb4d8c9799026620d7315b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6d795bf5fb64ffda84942d9503b76b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a175b82395433cbfff8245c2354da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "192e60dfc2134a9fbf9631dfbaecb082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e34dabe272f94faba3f6c16ce1733717",
              "IPY_MODEL_f3d297b88f63430591b4f7a475013639",
              "IPY_MODEL_2067809f40504d88beee6c0a47d01ec8"
            ],
            "layout": "IPY_MODEL_2668b91e39464f50aeed738fca6b05d8"
          }
        },
        "e34dabe272f94faba3f6c16ce1733717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e095395593ea4cda998df88a2eec0c20",
            "placeholder": "​",
            "style": "IPY_MODEL_bcd17b37b96645c29e9541dfe9accabe",
            "value": "config.json: 100%"
          }
        },
        "f3d297b88f63430591b4f7a475013639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a144edf326044058b15c0add51178f9a",
            "max": 608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b5d2a756007473cb4ec432aee0bc319",
            "value": 608
          }
        },
        "2067809f40504d88beee6c0a47d01ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8dcd5a586744e588af8a9627c3886ca",
            "placeholder": "​",
            "style": "IPY_MODEL_3f37eb452d9b4cb6b029d35553f28122",
            "value": " 608/608 [00:00&lt;00:00, 47.5kB/s]"
          }
        },
        "2668b91e39464f50aeed738fca6b05d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e095395593ea4cda998df88a2eec0c20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcd17b37b96645c29e9541dfe9accabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a144edf326044058b15c0add51178f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b5d2a756007473cb4ec432aee0bc319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8dcd5a586744e588af8a9627c3886ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f37eb452d9b4cb6b029d35553f28122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f34f88fb5dff41e9a6122a963080f666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c41dd92d7a2e420f82688fb91946c8f0",
              "IPY_MODEL_30b4380495bc4472b325ec76ad43b74f",
              "IPY_MODEL_e2d553af676c42cfbd5e37e2951ce225"
            ],
            "layout": "IPY_MODEL_4db71d2d96f04cbc96ca56d8d587ec74"
          }
        },
        "c41dd92d7a2e420f82688fb91946c8f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c133df2a5714fd0843b0fdcacc3ef6a",
            "placeholder": "​",
            "style": "IPY_MODEL_1760b4a615b34c3bb0e74074ee1cdfce",
            "value": "model.safetensors: 100%"
          }
        },
        "30b4380495bc4472b325ec76ad43b74f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_816d174b60ea4a4487e9867c510c65e9",
            "max": 2200119864,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a2fa068a9414797b9502aa9e207f187",
            "value": 2200119864
          }
        },
        "e2d553af676c42cfbd5e37e2951ce225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d7a72e64c6240958e04303514ee663d",
            "placeholder": "​",
            "style": "IPY_MODEL_e4711474247b45b18e15b9f48393648e",
            "value": " 2.20G/2.20G [00:37&lt;00:00, 84.2MB/s]"
          }
        },
        "4db71d2d96f04cbc96ca56d8d587ec74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c133df2a5714fd0843b0fdcacc3ef6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1760b4a615b34c3bb0e74074ee1cdfce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "816d174b60ea4a4487e9867c510c65e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a2fa068a9414797b9502aa9e207f187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d7a72e64c6240958e04303514ee663d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4711474247b45b18e15b9f48393648e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5eb4f22539f7463289e2ebb4498536a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22b64f41c1b6464f93b085bb39237280",
              "IPY_MODEL_284cf15344c1447388c064d2cd0d7aba",
              "IPY_MODEL_ff253a6e28c94043978f8666720cb729"
            ],
            "layout": "IPY_MODEL_623e8f230a5f4a81a1255a36364d6820"
          }
        },
        "22b64f41c1b6464f93b085bb39237280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_697d5c7d5e0c48f8ae1c05a05b094cce",
            "placeholder": "​",
            "style": "IPY_MODEL_a809f2a66f8b4e7b84e2cdd4d1ad9113",
            "value": "generation_config.json: 100%"
          }
        },
        "284cf15344c1447388c064d2cd0d7aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1b7af2290624d27b7afa4b4dd94f28f",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b41629cd14742128c2caf81cb9d8733",
            "value": 124
          }
        },
        "ff253a6e28c94043978f8666720cb729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4db400450d024030b0d793e64923dc80",
            "placeholder": "​",
            "style": "IPY_MODEL_4e81b95201684040808cc4595d86fb6a",
            "value": " 124/124 [00:00&lt;00:00, 5.93kB/s]"
          }
        },
        "623e8f230a5f4a81a1255a36364d6820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "697d5c7d5e0c48f8ae1c05a05b094cce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a809f2a66f8b4e7b84e2cdd4d1ad9113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1b7af2290624d27b7afa4b4dd94f28f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b41629cd14742128c2caf81cb9d8733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4db400450d024030b0d793e64923dc80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e81b95201684040808cc4595d86fb6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zhmNPPjolbx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Advanced Prompt Engineering and Systematic Optimization\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/chenwh0/Natural-Language-Processing-work/blob/main/module7/AdvancedPromptEngineeringandSystematicOptimization.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "**Background:**  This assignment explores **advanced prompt engineering** as both an art and a science. Moving beyond basic instruction-following, you will investigate systematic approaches to prompt optimization, analyze the cognitive and linguistic mechanisms underlying effective prompts, and develop frameworks for evaluating prompt performance across diverse tasks and domains. The focus is on **understanding the theoretical foundations** of prompt engineering while developing practical expertise in optimization techniques.\n",
        "\n",
        "## Instructions and Point Breakdown\n",
        "\n",
        "### 1. **Prompt Engineering Methodology Framework (2 points)**\n",
        "\n",
        "- **Develop a Systematic Approach:**\n",
        "  - Create a structured methodology for prompt design that includes: template design, iterative refinement, and performance evaluation\n",
        "  - Implement **three different prompting paradigms**:\n",
        "    - Zero-shot with precise instructions\n",
        "    - Few-shot with strategic example selection  \n",
        "    - Chain-of-thought with explicit reasoning steps\n",
        "\n",
        "- **Theoretical Foundation:**\n",
        "  - Analyze the cognitive science principles underlying effective prompts\n",
        "  - Investigate how prompt structure affects model reasoning and output quality\n",
        "\n",
        "- **Critical Questions:**\n",
        "  - How does the sequence and placement of information within prompts affect model performance?\n",
        "  - What are the trade-offs between prompt complexity and output reliability?\n",
        "  - How do different prompting paradigms align with various types of reasoning tasks?\n",
        "\n",
        "### 2. **Multi-Domain Prompt Optimization (3 points)**\n",
        "\n",
        "- **Cross-Domain Implementation:**\n",
        "  - Select **three distinct domains** (e.g., creative writing, technical documentation, logical reasoning, data analysis)\n",
        "  - For each domain, develop and iteratively optimize prompts using multiple techniques:\n",
        "    - **Meta-prompting**: Use LLMs to suggest prompt improvements\n",
        "    - **Recursive self-improvement**: Multi-iteration refinement cycles\n",
        "    - **Template-based optimization**: Structured formats with placeholders\n",
        "    - **Context-aware decomposition**: Breaking complex tasks into sub-prompts\n",
        "\n",
        "- **Comparative Analysis:**\n",
        "  - Measure performance across domains using appropriate metrics (accuracy, creativity, coherence, etc.)\n",
        "  - Document the optimization process and effectiveness of each technique\n",
        "  - Analyze which techniques work best for different types of tasks\n",
        "\n",
        "- **Critical Questions:**\n",
        "  - How do optimal prompting strategies vary across different cognitive domains?\n",
        "  - What patterns emerge in successful prompt structures across diverse applications?\n",
        "  - How can you systematically identify when a prompt has reached optimal performance?\n",
        "\n",
        "### 3. **Prompt Interpretability and Failure Analysis (2 points)**\n",
        "\n",
        "- **Deep Analysis of Prompt Mechanisms:**\n",
        "  - Investigate attention patterns and model behavior with different prompt structures\n",
        "  - Analyze failure modes: when and why do optimized prompts fail?\n",
        "  - Study the relationship between prompt complexity and model interpretability\n",
        "\n",
        "- **Systematic Error Analysis:**\n",
        "  - Identify classes of problems where prompt engineering breaks down\n",
        "  - Analyze the role of prompt length, specificity, and linguistic complexity\n",
        "  - Investigate bias introduction through prompt design choices\n",
        "\n",
        "- **Critical Questions:**\n",
        "  - How do different prompt engineering techniques affect model reasoning transparency?\n",
        "  - What are the limits of prompt-based optimization versus model fine-tuning?\n",
        "  - How can we detect when prompts are exploiting spurious correlations rather than genuine understanding?\n",
        "\n",
        "### 4. **Automated Prompt Evaluation and Scaling (2 points)**\n",
        "\n",
        "- **Evaluation Framework Development:**\n",
        "  - Design metrics for prompt quality that go beyond task-specific performance\n",
        "  - Implement automated evaluation systems for prompt comparison\n",
        "  - Develop methods for detecting prompt robustness across edge cases\n",
        "\n",
        "- **Scalability Investigation:**\n",
        "  - Analyze computational costs of different optimization approaches\n",
        "  - Investigate techniques for prompt transfer across related tasks\n",
        "  - Explore automated prompt generation and refinement systems\n",
        "\n",
        "- **Critical Questions:**\n",
        "  - How can we design evaluation metrics that capture both effectiveness and robustness?\n",
        "  - What are the trade-offs between human evaluation and automated assessment of prompts?\n",
        "  - How do we balance prompt optimization costs with performance gains in production systems?\n",
        "\n",
        "### 5. **Future Directions and Theoretical Implications (1 point)**\n",
        "\n",
        "- **Research Synthesis:**\n",
        "  - Synthesize insights from your experiments into broader principles of prompt engineering\n",
        "  - Identify limitations of current prompting approaches and propose solutions\n",
        "  - Discuss implications for human-AI interaction design\n",
        "\n",
        "- **Critical Analysis Questions:**\n",
        "  - How might prompt engineering evolve as model capabilities advance?\n",
        "  - What are the ethical implications of highly optimized prompts that may manipulate model outputs?\n",
        "  - How does effective prompt engineering relate to theories of human cognition and communication?\n",
        "  - What role should prompt engineering play in AI safety and alignment?\n",
        "\n",
        "## Submission Requirements\n",
        "\n",
        "- **Comprehensive Jupyter Notebook** containing:\n",
        "  - Implementation of all optimization techniques across multiple domains\n",
        "  - Detailed experimental results with statistical analysis\n",
        "  - Visualizations of optimization trajectories and performance comparisons  \n",
        "  - Code for automated evaluation systems\n",
        "  - Thorough written analysis addressing all critical questions (**Maximum** 3-4 paragraphs each)\n",
        "\n",
        "- **Technical Rigor:** Include proper experimental controls, statistical significance testing, and reproducibility measures\n",
        "\n",
        "- **Libraries:** Use `transformers`, `openai`, `langchain`, `pandas`, `matplotlib`, `seaborn`, and statistical analysis packages\n",
        "\n",
        "## Advanced Extensions (Optional)\n",
        "\n",
        "- **Multi-model Optimization:** Compare prompt effectiveness across different LLM architectures\n",
        "- **Dynamic Prompt Adaptation:** Implement systems that modify prompts based on real-time performance feedback\n",
        "- **Prompt Compression:** Investigate methods for maintaining effectiveness while reducing prompt length\n",
        "- **Cross-lingual Prompt Engineering:** Analyze prompt optimization across different languages\n",
        "\n",
        "**Grading Rubric:**\n",
        "\n",
        "| Section                                        | Points |\n",
        "|:-----------------------------------------------|:------:|\n",
        "| Prompt engineering methodology framework      | 2      |\n",
        "| Multi-domain prompt optimization              | 3      |\n",
        "| Prompt interpretability & failure analysis    | 2      |\n",
        "| Automated prompt evaluation & scaling         | 2      |\n",
        "| Future directions & theoretical implications  | 1      |\n",
        "| **Total**                                     | **10** |\n",
        "\n",
        "**Evaluation Criteria:**\n",
        "- **Methodological Rigor (30%):** Systematic approach to prompt design and optimization\n",
        "- **Theoretical Understanding (35%):** Depth of analysis of prompt engineering principles and mechanisms  \n",
        "- **Innovation and Insights (35%):** Novel approaches, unexpected findings, and meaningful contributions to the field\n",
        "\n",
        "**Learning Objectives:**\n",
        "Students will develop expertise in advanced prompt engineering as a systematic discipline, understanding both the practical techniques and theoretical foundations necessary for designing effective human-AI interaction systems. This includes mastery of optimization methods, evaluation frameworks, and the ability to analyze and predict prompt effectiveness across diverse applications."
      ],
      "metadata": {
        "id": "7kw9kP0T91Vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Installs"
      ],
      "metadata": {
        "id": "PVmrd0Mn-gFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm textattack -q"
      ],
      "metadata": {
        "id": "__PK9g9KSNzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH4CMvNK9y1_"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing libraries\n",
        "import pandas\n",
        "\n",
        "# LLM libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import trange\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Libraries for cosine similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import util\n",
        "miniLM_model = SentenceTransformer('all-MiniLM-L6-v2') # Load pre-trained model (you can choose different models based on your needs)\n",
        "\n",
        "# Text augumentation library\n",
        "from textattack.augmentation import EasyDataAugmenter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Prompt Engineering Methodology Framework**\n",
        "\n",
        "## Information placement order within prompt matters\n",
        "Information placed at the beginning and end of the prompt will get more attention because they either set the context for how all subsequent tokens are encoded OR they are given more weight and attention in the transformer because they are near the end, thus considered the most recent.\n",
        "Important information that is placed in middle of prompt affect less subsequent tokens and they get less attention.\n",
        "\n",
        "## Prompt complexity vs Output reliability tradeoff\n",
        "The more complex the prompt is, i.e. prompts that require multiple steps to solve, the higher chance the model may have to misrepresent and or misinterpret the prompt, thus making its output less reliable.\n",
        "\n",
        "## Different prompting paradigmns for different reasoning tasks\n",
        "**Zero-shot prompting**: Instruct LLM to do certain task without any samples. LLM may misunderstand question and give erroneous outputs.\n",
        "\n",
        "**Few-shot prompting**: Instruct LLM to do certain task with sample input-output pairs. LLM is able to know general input's format and expected output's format.\n",
        "\n",
        "**Chain of Thought (CoT) prompting**: Instruct LLM to do step-by-step reasoning. Takes up more tokens in window but more effective to guide LLM to solving problem by breaking it down into small sequential tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "2S5AhHV8-kbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelLLM:\n",
        "    def __init__(self, name):\n",
        "        self.name = name # Replace with actual model ID from Hugging Face\n",
        "        self.tokenizer = tinyLlama_tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(name)\n",
        "\n",
        "    def generate(self, prompt, cache=False, temperature=1): # Generate output from 1 string (promnpt)\n",
        "        prompt += \"<|assistant|>\" # Add role assignment tag for TinyLlama\n",
        "        tokenized_inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        token_ids = self.model.generate(**tokenized_inputs, max_new_tokens=300, use_cache=cache, do_sample=True, temperature=temperature)\n",
        "        decoded_tokens = self.tokenizer.decode(token_ids[0], skip_special_tokens=True)\n",
        "        return decoded_tokens\n",
        "\n",
        "    def multi_generate(self, prompts, cache=False, temperature=1): # Generate outputs from list of strings (prompts)\n",
        "        outputs = []\n",
        "        for i in trange(len(prompts)):\n",
        "            decoded_tokens = self.generate(prompts[i], cache=cache, temperature=temperature)\n",
        "            outputs.append(decoded_tokens)\n",
        "        return outputs\n",
        "\n",
        "    def substep_generate(self, prompts, cache=False, temperature=1): # Generate outputs from list of strings (subdivided task prompts)\n",
        "        outputs = []\n",
        "        for i in trange(len(prompts)):\n",
        "            if i == len(prompts) - 1: # If processing last output\n",
        "                outputs[-1] = outputs[-1].replace(prompts[-1]+\"<|assistant|>\", \"\")\n",
        "                outputs[-2] = outputs[-2].replace(prompts[-2]+\"<|assistant|>\", \"\")\n",
        "                prompts[i] += f\"output 1: {outputs[-2]}. output 2: {outputs[-1]}.\" # Add previous 2 outputs as inputs to last prompt for combining.\n",
        "\n",
        "            decoded_tokens = self.generate(prompts[i], cache=cache, temperature=temperature)\n",
        "            outputs.append(decoded_tokens)\n",
        "        return outputs\n",
        "\n",
        "    def tokenize(self, prompt): # Tokenize 1 string (prompt)\n",
        "        token_ids = self.tokenizer.encode(prompt)\n",
        "        decoded_tokens = self.tokenizer.convert_ids_to_tokens(token_ids)\n",
        "        return {\"input\": prompt, \"token_ids\": token_ids, \"decoded_tokens\": decoded_tokens}\n",
        "\n",
        "    def multi_tokenize(self, prompts): # Tokenize list of strings (prompts)\n",
        "        rows = []\n",
        "        for i in range(len(prompts)):\n",
        "            row = self.tokenize(prompts[i])\n",
        "            print(\"Input text:\", prompts[i])\n",
        "            print(\"Token ids:\", row[\"token_ids\"])\n",
        "            print(\"Decoded tokens:\", row[\"decoded_tokens\"])\n",
        "            row[\"prompt_num\"] = i\n",
        "            rows.append(row)\n",
        "        return pandas.DataFrame.from_dict(rows, orient=\"columns\")\n",
        "\n",
        "tinyLlama = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tinyLlama_model = ModelLLM(tinyLlama)\n",
        "print(tinyLlama_model.model)"
      ],
      "metadata": {
        "id": "ImXgmQDk-kMI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703,
          "referenced_widgets": [
            "668400cf86d14b45b6e41261c5288fed",
            "efa04bbae7d34c9396e9933070fe2fac",
            "6333db661f6340e2bcfd5f5410ff7994",
            "e9b7dd9d037740c79dfb39b2e6b57609",
            "77e249c5e48f449b9eb9adf20769e075",
            "47d7f0b27934449db06d5cbf04389921",
            "b747b5ae55904b92ac3d6143082094f8",
            "f280b9f01b9c4d6992aee37b7e50c09c",
            "54d29471364741bbac13876779ca662b",
            "2032edc58180445ea8854aa8b7cd1d80",
            "37b58aed4ffd46e7b86a12cbb81480b8",
            "5dc09e533d9a48e1a39a3a66108109b3",
            "5a5d384c28c44d7fada4bb138a140566",
            "579b3d71bb46448c8103736676ff9501",
            "7e36ea2c0af94a309ec4654da557e525",
            "f0de0fc2401e41c8b2eff35ee2585edc",
            "3a1f2b4f0cc9411ab44570e3bb03a0ab",
            "57333c356a6740549a216e9263599ef5",
            "c3ee9b48d72b48d38a8ef769de989ca6",
            "7b6defa061b44d99b340ae09d27a4bf3",
            "ca8199a1df254707929a9f82f7b14406",
            "c82676caa44e42fcb81954bf267320b2",
            "8e8677ceae414adebc7ba176e4a8efaa",
            "86559e59b27f4149a6324df7c960de1c",
            "db2798cc70b049c485dc027edf665826",
            "61ee3cfe26c64440ad95ab5300293775",
            "2e6bb82724944a9fa20b27a5401710af",
            "c76a3aadb1284f1ca8518757275b4219",
            "37cc52a13d1047e3b34fd48da6020c72",
            "175c85730a884f769a88611824b9c1b3",
            "6c83f0ac15e54ff69ae37e2a5bedfe37",
            "7e37ce5a22d3408c9caac04cc40a1de3",
            "653215c47580484d9ac8431180fac17d",
            "4462702d70074517a22b01206604540f",
            "ad33ace25163437ab4e1d69cbf57aae9",
            "666ed606434744a6be8ba73edd85c744",
            "d67160948de344df9e6e0a7ec3999261",
            "e5201f4a820b46dd8f009b0f4e957e82",
            "c7e4c24fdc604f88a28c34a600eaf83f",
            "5083ce495c2548c7981e5164424a3ffd",
            "bc4764c6ed0c49c7979cb92f3a4a1838",
            "88cc4e3166eb4d8c9799026620d7315b",
            "d6d795bf5fb64ffda84942d9503b76b7",
            "71a175b82395433cbfff8245c2354da7",
            "192e60dfc2134a9fbf9631dfbaecb082",
            "e34dabe272f94faba3f6c16ce1733717",
            "f3d297b88f63430591b4f7a475013639",
            "2067809f40504d88beee6c0a47d01ec8",
            "2668b91e39464f50aeed738fca6b05d8",
            "e095395593ea4cda998df88a2eec0c20",
            "bcd17b37b96645c29e9541dfe9accabe",
            "a144edf326044058b15c0add51178f9a",
            "7b5d2a756007473cb4ec432aee0bc319",
            "a8dcd5a586744e588af8a9627c3886ca",
            "3f37eb452d9b4cb6b029d35553f28122",
            "f34f88fb5dff41e9a6122a963080f666",
            "c41dd92d7a2e420f82688fb91946c8f0",
            "30b4380495bc4472b325ec76ad43b74f",
            "e2d553af676c42cfbd5e37e2951ce225",
            "4db71d2d96f04cbc96ca56d8d587ec74",
            "8c133df2a5714fd0843b0fdcacc3ef6a",
            "1760b4a615b34c3bb0e74074ee1cdfce",
            "816d174b60ea4a4487e9867c510c65e9",
            "8a2fa068a9414797b9502aa9e207f187",
            "0d7a72e64c6240958e04303514ee663d",
            "e4711474247b45b18e15b9f48393648e",
            "5eb4f22539f7463289e2ebb4498536a4",
            "22b64f41c1b6464f93b085bb39237280",
            "284cf15344c1447388c064d2cd0d7aba",
            "ff253a6e28c94043978f8666720cb729",
            "623e8f230a5f4a81a1255a36364d6820",
            "697d5c7d5e0c48f8ae1c05a05b094cce",
            "a809f2a66f8b4e7b84e2cdd4d1ad9113",
            "d1b7af2290624d27b7afa4b4dd94f28f",
            "4b41629cd14742128c2caf81cb9d8733",
            "4db400450d024030b0d793e64923dc80",
            "4e81b95201684040808cc4595d86fb6a"
          ]
        },
        "outputId": "6e8efd70-46ad-42a2-c003-dec71d4dda4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "668400cf86d14b45b6e41261c5288fed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5dc09e533d9a48e1a39a3a66108109b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e8677ceae414adebc7ba176e4a8efaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4462702d70074517a22b01206604540f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "192e60dfc2134a9fbf9631dfbaecb082"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f34f88fb5dff41e9a6122a963080f666"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5eb4f22539f7463289e2ebb4498536a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-21): 22 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
            "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_responses(responses, prompts):\n",
        "    for i, response in enumerate(responses):\n",
        "        print()\n",
        "        print()\n",
        "        print(\"Prompt\", i, \"-\", prompts[i])\n",
        "        print()\n",
        "        print(\"Response\", i, \"-\", response.replace(prompts[i]+\"<|assistant|>\", \"\")) # Delete the prompt itself from the response"
      ],
      "metadata": {
        "id": "uKA3gCKxv-T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_prompt = \"List 3 machine learning methods to analyze a large geospatial database and find geospatial hotspots or interesting patterns.\"\n",
        "few_shot_prompt = f\"\"\"Prompt: List 1 machine learning method to classify a large database of images. Answer: 1. Convolutional Neural Networks (CNNs) are deep learning models specifically designed for image data. They automatically learn spatial hierarchies of features through layers of convolutions, pooling, and nonlinear activations.\n",
        "Prompt: List 1 machine learning method to identify patterns across geospatial SQL rows. Answer: 1. Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups together points that are closely packed and marks points in low-density areas as outliers.\n",
        "Prompt: {zero_shot_prompt} Answer: ?\"\"\"\n",
        "chain_of_thought_prompt = f\"Think step by step. 1. What are good machine learning methods identify patterns and hotspots across any geospatial data? 2. What are good machine learning methods for Big Data? 3. What are machine learning or generative AI methods that can combine both of these techniques? {zero_shot_prompt}\""
      ],
      "metadata": {
        "id": "ojMrv74WUyvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [zero_shot_prompt, few_shot_prompt, chain_of_thought_prompt]\n",
        "responses = tinyLlama_model.multi_generate(prompts, cache=True, temperature=0.2)\n",
        "print_responses(responses, prompts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nTiUbCgbl4b",
        "outputId": "3ff9d9a9-82ea-46cd-b3aa-f41fd3806f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Prompt 0 - List 3 machine learning methods to analyze a large geospatial database and find geospatial hotspots or interesting patterns.\n",
            "\n",
            "Response 0 - \n",
            "\n",
            "1. K-means Clustering: K-means clustering is a popular technique for grouping data points based on their similarity. It is a non-parametric method that partitions the data into k clusters, where k is a hyperparameter that can be tuned. K-means clustering is a good choice for large geospatial datasets as it can handle large datasets efficiently.\n",
            "\n",
            "2. Hierarchical Clustering: Hierarchical clustering is a clustering technique that groups data points based on their similarity. It is a hierarchical clustering algorithm that partitions the data into k clusters, where k is a hyperparameter that can be tuned. Hierarchical clustering is a good choice for large geospatial datasets as it can handle large datasets efficiently.\n",
            "\n",
            "3. Decision Trees: Decision trees are a type of supervised learning algorithm that can be used to classify data points based on their features.\n",
            "\n",
            "\n",
            "Prompt 1 - Prompt: List 1 machine learning method to classify a large database of images. Answer: 1. Convolutional Neural Networks (CNNs) are deep learning models specifically designed for image data. They automatically learn spatial hierarchies of features through layers of convolutions, pooling, and nonlinear activations.\n",
            "Prompt: List 1 machine learning method to identify patterns across geospatial SQL rows. Answer: 1. Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups together points that are closely packed and marks points in low-density areas as outliers.\n",
            "Prompt: List 3 machine learning methods to analyze a large geospatial database and find geospatial hotspots or interesting patterns. Answer: ?\n",
            "\n",
            "Response 1 - \n",
            "\n",
            "Based on the given material, can you provide an example of how density-based spatial clustering of applications with noise (DBSCAN) can be used to group points together and mark outliers in a geospatial database?\n",
            "\n",
            "\n",
            "Prompt 2 - Think step by step. 1. What are good machine learning methods identify patterns and hotspots across any geospatial data? 2. What are good machine learning methods for Big Data? 3. What are machine learning or generative AI methods that can combine both of these techniques? List 3 machine learning methods to analyze a large geospatial database and find geospatial hotspots or interesting patterns.\n",
            "\n",
            "Response 2 - \n",
            "\n",
            "1. Clustering: Clustering is a technique that groups similar data points together based on their similarity. It is a good machine learning method for identifying patterns and hotspots across any geospatial data. Clustering can be used to identify areas with high population density, crime rates, or other relevant data points.\n",
            "\n",
            "2. Neural Networks: Neural networks are a type of machine learning algorithm that can learn patterns from data. They are particularly effective for analyzing large datasets with complex relationships. Neural networks can be used to identify geospatial hotspots or interesting patterns across a large geospatial database.\n",
            "\n",
            "3. Generative Adversarial Networks (GANs): GANs are a type of machine learning algorithm that can generate realistic images or other forms of data. They can be used to combine the advantages of clustering and neural networks to identify geospatial hotspots or interesting patterns. GAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **Multi-Domain Prompt Optimization**\n",
        "\n",
        "**Meta-prompting**: Use LLMs to suggest prompt improvements\n",
        "\n",
        "**Recursive self-improvement**: Multi-iteration refinement cycles\n",
        "\n",
        "**Template-based optimization**: Structured formats with placeholders\n",
        "\n",
        "**Context-aware decomposition**: Breaking complex tasks into sub-prompts\n",
        "\n",
        "- **Comparative Analysis:**\n",
        "  - Measure performance across domains using appropriate metrics (accuracy, creativity, coherence, etc.)\n",
        "  - Document the optimization process and effectiveness of each technique\n",
        "  - Analyze which techniques work best for different types of tasks\n",
        "### Creative prompt\n",
        "| Prompt optimization technique      | accuracy | creativity | coherence |\n",
        "|:-----------------------------------|:--------:|:----------:|:---------:|\n",
        "| Meta-prompting                     |    10    |     4      |     10    |\n",
        "| Recursive self-improvement         |    9     |     5      |     10    |\n",
        "| Template-based optimization        |    9     |     5      |     10    |\n",
        "| Context-aware decomposition        |    5     |     10     |     3     |\n",
        "\n",
        "### Data Analysis prompt\n",
        "| Prompt optimization technique      | accuracy | creativity | coherence |\n",
        "|:-----------------------------------|:--------:|:----------:|:---------:|\n",
        "| Meta-prompting                     |    6     |     0      |     10    |\n",
        "| Recursive self-improvement         |    9     |     0      |     10    |\n",
        "| Template-based optimization        |    9     |     0      |     10    |\n",
        "| Context-aware decomposition        |    5     |     0      |     3     |\n",
        "\n",
        "### Documentation prompt\n",
        "| Prompt optimization technique      | accuracy | creativity | coherence |\n",
        "|:-----------------------------------|:--------:|:----------:|:---------:|\n",
        "| Meta-prompting                     |    7     |     0      |     10    |\n",
        "| Recursive self-improvement         |    9     |     0      |     10    |\n",
        "| Template-based optimization        |    9     |     0      |     10    |\n",
        "| Context-aware decomposition        |    3     |     0      |     5     |\n",
        "\n",
        "\n",
        "## Various prompting strategies for different cognitive domians\n",
        " * **Meta-prompting** - Generally works well across all cognitive domains for any question that desires a specific format and type of answer that isn't as open-ended.\n",
        " * **Recursive self-improvement** - Generally works well across all cognitive fomains because it helps identify specific misunderstandings of prompts that model may have and tailor the prompt to correcting it continuously.\n",
        " * **Template-based optimization** - Works especially well for data analysis prompt and documentation prompt. But overly-structured response is suboptimal for a creative writing prompt.\n",
        " * **Context-aware decomposition** - In this experiment, I had to expand the token window so that the final sub-prompt to combine previous outputs can fit the token window. However, tinyLlama ended up generating incoherent code in data analysis prompt and documentation prompt (where it was filled with unecessary imports and repetitive code). Subdividing the creative writing prompt helped generated a very creative but sometimes inconsistent script.\n",
        "\n",
        "\n",
        "## A generally successful prompt structure\n",
        "A generally successful prompt structure is concise. It leaves no room for misinterpretation by providing short one or few-shot examples that can give the model insight on input and output formatting and desired style of answers that the prompter desires.\n",
        "\n",
        "## Systematic method to identify optimal prompt\n",
        "To evaluate the optimaliality of a prompt, one can evaluate the LLMs' response and check:\n",
        "* *Is LLM's response factually accurate?*\n",
        "* *Is LLM's response complying with instructions?*\n",
        "* *Is LLM's response coherent & relevant to the prompt? Is LLM generating irrelevant data?*\n",
        "* *Is LLM's response generally consistent across multiple generations using same prompt?*"
      ],
      "metadata": {
        "id": "0LpYO3mU-zDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original prompts\n",
        "prompt_creative_writing = \"Write a story using Outline → characters → scenes → dialogue.\"\n",
        "prompt_data_analysis = \"\"\"headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"]\n",
        "data = [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ] Does this data reveal anything?\"\"\"\n",
        "prompt_documentation = \"\"\"Convert code into a python function: embedding = model.encode(query, convert_to_tensor=True)\n",
        "hits = util.semantic_search(embedding, corpus_embeddings, top_k=5)[0]\n",
        "top_k_keys = \"Fetched features and values:\\n\"\n",
        "for hit in hits:\n",
        "    key = corpus_keys[hit['corpus_id']]\n",
        "    top_k_keys += f\"{key} - {response_map_descriptions[key][\"value\"]}\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "71yBWZBA-6jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Meta-prompting\n",
        "prompt_creative_writing = \"Write a story about a character with a certain role who must fulfill a need or want. But, they're prevented by an external or internal conflict.\"\n",
        "prompt_data_analysis = \"\"\"Analyze the trend in this contamination time series data and summarize the key findings.\n",
        "headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"]\n",
        "data = [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]\"\"\"\n",
        "prompt_documentation = \"\"\"Convert this into a well-structured Python function with proper documentation. embedding = model.encode(query, convert_to_tensor=True)\n",
        "hits = util.semantic_search(embedding, corpus_embeddings, top_k=5)[0]\n",
        "top_k_keys = \"Fetched features and values:\\n\"\n",
        "for hit in hits:\n",
        "    key = corpus_keys[hit['corpus_id']]\n",
        "    top_k_keys += f\"{key} - {response_map_descriptions[key][\"value\"]}\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vsDbngH_dD3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [prompt_creative_writing, prompt_data_analysis, prompt_documentation]\n",
        "responses = tinyLlama_model.multi_generate(prompts, cache=True, temperature=0.2)\n",
        "print_responses(responses, prompts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GxsA_ACwMox",
        "outputId": "83c18c83-6f81-42d7-ab89-015894c4c26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [04:03<00:00, 81.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Prompt 0 - Write a story about a character with a certain role who must fulfill a need or want. But, they're prevented by an external or internal conflict.\n",
            "\n",
            "Response 0 - \n",
            "\n",
            "Sophie was a successful businesswoman who had built her own empire from the ground up. She had a reputation for being tough and unyielding, but deep down, she was a kind and compassionate person. She had a heart of gold, and her employees loved her for it.\n",
            "\n",
            "One day, Sophie received a call from her boss, who informed her that her company was facing a major crisis. The company's main product had been discontinued, and they were struggling to find a replacement. Sophie knew that this was a critical need for her company, but she was prevented from fulfilling it by an internal conflict.\n",
            "\n",
            "Sophie's boss had been pushing for a new product that would be more profitable and popular. Sophie had been hesitant to go along with this plan, as she had invested a lot of time and resources into developing the original product.\n",
            "\n",
            "\n",
            "Prompt 1 - Analyze the trend in this contamination time series data and summarize the key findings.\n",
            "headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"]\n",
            "data = [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]\n",
            "\n",
            "Response 1 - \n",
            "\n",
            "# Import pandas library\n",
            "import pandas as pd\n",
            "\n",
            "# Load the contamination time series data into a pandas dataframe\n",
            "df = pd.read_csv(\"contamination_time_series.csv\", header=0)\n",
            "\n",
            "# Check the number of rows and columns in the dataframe\n",
            "print(df.shape)\n",
            "\n",
            "# Check the first few rows of the dataframe\n",
            "print(df.head())\n",
            "\n",
            "# Check the last few rows of the dataframe\n",
            "print(df.tail())\n",
            "\n",
            "# Check the column names in the dataframe\n",
            "print(df.columns)\n",
            "\n",
            "# Check the data types of the columns in the dataframe\n",
            "print(df.dtypes)\n",
            "\n",
            "# Check the trend in the contamination time series data\n",
            "trend = df.iloc[:, 1].diff()\n",
            "print(trend)\n",
            "\n",
            "# Summarize the key findings\n",
            "print(\"The trend in the contamination\n",
            "\n",
            "\n",
            "Prompt 2 - Convert this into a well-structured Python function with proper documentation. embedding = model.encode(query, convert_to_tensor=True)\n",
            "hits = util.semantic_search(embedding, corpus_embeddings, top_k=5)[0]\n",
            "top_k_keys = \"Fetched features and values:\n",
            "\"\n",
            "for hit in hits:\n",
            "    key = corpus_keys[hit['corpus_id']]\n",
            "    top_k_keys += f\"{key} - {response_map_descriptions[key][\"value\"]}\"\n",
            "\n",
            "\n",
            "Response 2 - \n",
            "```\n",
            "\n",
            "This function takes a query string, a list of corpus embeddings, and a list of top k results. It first converts the query string into a TensorFlow-compatible representation using `tf.convert_to_tensor`, and then uses `tf.nn.embedding_lookup` to map the query to a set of embeddings. It then uses `tf.nn.embedding_lookup` to retrieve the top k embeddings for each corpus, and returns a list of dictionaries containing the top k embeddings and their corresponding descriptions. The function also includes a helper function `response_map_descriptions` that maps response codes to their corresponding descriptions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recursive self-improvement and Template-based optimization\n",
        "prompt_creative_writing =  \"Write a story about [name], a [character role], who has a [goal]. But, they're prevented by this [antagonistic force].\"\n",
        "prompt_data_analysis = \"\"\"Analyze the trend in this contamination time series data and summarize the key findings.\n",
        "headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"]\n",
        "data = [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]\n",
        "Response should be [Geographical observations from spatial context] [Summary of contamination level and its implications]\"\"\"\n",
        "prompt_documentation = \"\"\"Convert code into  python function def cosine_similarity_fetch(args): [code] return top_k_keys.\n",
        "Code:\n",
        "embedding = model.encode(query, convert_to_tensor=True)\n",
        "hits = util.semantic_search(embedding, corpus_embeddings, top_k=5)[0]\n",
        "top_k_keys = \"Fetched features and values:\\n\"\n",
        "for hit in hits:\n",
        "    key = corpus_keys[hit['corpus_id']]\n",
        "    top_k_keys += f\"{key} - {response_map_descriptions[key][\"value\"]}\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0eSLEfohfjxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [prompt_creative_writing, prompt_data_analysis, prompt_documentation]\n",
        "responses = tinyLlama_model.multi_generate(prompts, cache=True, temperature=0.2)\n",
        "print_responses(responses, prompts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cVDnPuDQw1WZ",
        "outputId": "bc833b62-1a11-48f1-b7e4-8469f0ad6c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [04:23<00:00, 87.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Prompt 0 - Write a story about [name], a [character role], who has a [goal]. But, they're prevented by this [antagonistic force].\n",
            "\n",
            "Response 0 - \n",
            "\n",
            "Name: Emily\n",
            "\n",
            "Character Role: Student\n",
            "\n",
            "Goal: Get into a prestigious university\n",
            "\n",
            "Antagonistic Force: The college admissions process is rigorous and competitive, and Emily's application is rejected multiple times.\n",
            "\n",
            "Emily's parents are both successful businesspeople, and they have always encouraged her to pursue her dreams. Emily's passion for music has always been her greatest strength, and she dreams of becoming a professional musician. However, the college admissions process is a daunting task, and Emily's application is rejected multiple times.\n",
            "\n",
            "Emily's parents are devastated, but they know that Emily has what it takes to succeed. They encourage her to keep trying, and Emily begins to focus on her studies and her passion for music. She starts to attend music classes and auditions, and she even starts playing in a local band.\n",
            "\n",
            "\n",
            "Prompt 1 - Analyze the trend in this contamination time series data and summarize the key findings.\n",
            "headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"]\n",
            "data = [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]\n",
            "Response should be [Geographical observations from spatial context] [Summary of contamination level and its implications]\n",
            "\n",
            "Response 1 - \n",
            "\n",
            "The trend in this contamination time series data is increasing over time. The highest concentration of lead was detected on May 15, 2022, followed by May 22, 2023, and July 10, 2024. The concentration of lead varied between 245.7 and 398.1 ppb, with a median concentration of 292.4 ppb. The highest concentration of lead was detected in May 2022, followed by May 2023 and July 2024. The median concentration of lead was 292.4 ppb, indicating that lead levels are increasing over time.\n",
            "\n",
            "The trend in this contamination time series data suggests that lead contamination is increasing in this area. This trend is concerning as lead is a toxic metal that can cause health problems if consumed or in\n",
            "\n",
            "\n",
            "Prompt 2 - Convert code into  python function def cosine_similarity_fetch(args): [code] return top_k_keys. \n",
            "Code: \n",
            "embedding = model.encode(query, convert_to_tensor=True)\n",
            "hits = util.semantic_search(embedding, corpus_embeddings, top_k=5)[0]\n",
            "top_k_keys = \"Fetched features and values:\n",
            "\"\n",
            "for hit in hits:\n",
            "    key = corpus_keys[hit['corpus_id']]\n",
            "    top_k_keys += f\"{key} - {response_map_descriptions[key][\"value\"]}\"\n",
            "\n",
            "\n",
            "Response 2 - \n",
            "To convert the Cosine Similarity function into a Python function, we can use the `convert_to_tensor` method of the `Embedding` class. This method converts the input tensor to a TensorFlow `Tensor` object, which can be used in other functions.\n",
            "\n",
            "```python\n",
            "import tensorflow as tf\n",
            "\n",
            "class Embedding(tf.keras.layers.Layer):\n",
            "    def __init__(self, embedding_dim, input_dim, output_dim, name=None):\n",
            "        super().__init__(name=name)\n",
            "        self.embedding_dim = embedding_dim\n",
            "        self.input_dim = input_dim\n",
            "        self.output_dim = output_dim\n",
            "        self.embedding = tf.keras.layers.Embedding(input_dim=input_dim, output_dim=output_dim, input_length=1)\n",
            "\n",
            "    def call(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Context-aware decomposition\n",
        "prompts_creative_writing = [\"Introduce protagonist [name] as a [character role]. Establish their motivation for pursuing a [goal].\",\n",
        " \"Introduce [antagonistic force] and how moment when protagonist overcomes [antagonistic force]\",\n",
        " \"Assemble the previous outputs into one cohesive and intersting story premise.\"]\n",
        "\n",
        "\n",
        "geospatial_data = \"\"\"headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"]\n",
        "data = [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]\"\"\"\n",
        "prompts_data_analysis = [f\"Describe the geographical and temporal area covered by data like: Based on longitude and latitude, this data describes [geographical area] across [timeframe]. {geospatial_data}\",\n",
        " f\"Contextualize the soil quality data like: The underlying [pattern] has [implication] which can lead to [consequence]. {geospatial_data}\",\n",
        " \"Combine the previous outputs into a single, concise summary of the key findings.\"]\n",
        "\n",
        "\n",
        "code = \"\"\"embedding = model.encode(query, convert_to_tensor=True)\n",
        "hits = util.semantic_search(embedding, corpus_embeddings, top_k=5)[0]\n",
        "top_k_keys = \"Fetched features and values:\\n\"\n",
        "for hit in hits:\n",
        "    key = corpus_keys[hit['corpus_id']]\n",
        "    top_k_keys += f\"{key} - {response_map_descriptions[key][\"value\"]}\"\n",
        "\"\"\"\n",
        "prompts_documentation = [f\"List all variables in this code that should become function parameters and the final value that should be returned. {code}\",\n",
        "                         f\"Extract the core logic of the code, focusing on the search and retrieval steps, independent of string formatting and variables. {code}\",\n",
        "                         \"Assemble the identified parameters, core logic, and formatted output into the final function: def cosine_similarity_fetch(args)\"]"
      ],
      "metadata": {
        "id": "lrak0x-dtg4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses = tinyLlama_model.substep_generate(prompts_creative_writing, cache=True, temperature=1)\n",
        "print_responses(responses, prompts_creative_writing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VnjAq6HCbC5t",
        "outputId": "14d37969-0a48-4ffc-e261-385f7e0b25f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Prompt 0 - Introduce protagonist [name] as a [character role]. Establish their motivation for pursuing a [goal].\n",
            "\n",
            "Response 0 - \n",
            "\n",
            "ACT ONE\n",
            "INT. ABANDONED BUILDING - DAY\n",
            "[Scene: Introductions and set up for Jared, a 28-year-old who recently graduated from college. He's dressed in his school's letterman's jacket - the kind worn at the end of football games.]\n",
            "\n",
            "JARED (To himself, in a deep voice). I just have to finish this thesis. I'll see the light at the end of the tunnel.\n",
            "\n",
            "INT. CURRENT ABANDONED BUILDING - NIGHT\n",
            "\n",
            "MORGAN (In a cold, measured tone). Jared, I need you to step aside and let me handle this.\n",
            "\n",
            "JARED (In a defensive tone, not wanting to be taken from under the carnage). Who said I wanted a piece of you? You were always better than me.\n",
            "\n",
            "MORGAN (Punching Jared in the arm). Jared, give me a chance to explain. These are my students you're talking about. How many were there?\n",
            "\n",
            "JARED (Angry, defensive voice). You think you're better than us? All you care about is money and fame. You're just a glorified clown.\n",
            "\n",
            "MORGAN (Grabbing Jared's shoulder with a force that surprises Jared). Did you see the bodies? How many students are dead?\n",
            "\n",
            "JARED (Shook and sprints away). What do you want from me? You're nothing to me. You just want my money and attention.\n",
            "\n",
            "MORGAN (Panting and breathing heavily, holding Jared's shoulder). I got the call of the day. There's a cultist organization planning an attack on the school. Jared, find them\n",
            "\n",
            "\n",
            "Prompt 1 - Introduce [antagonistic force] and how moment when protagonist overcomes [antagonistic force]\n",
            "\n",
            "Response 1 - \n",
            "\n",
            "Scene 2\n",
            "\n",
            "The protagonist is sitting in a dimly lit room, surrounded by letters and documents.\n",
            "\n",
            "Protagonist: (frustrated) Why doesn’t anyone care about what I have to say? I’m not just a minor annoyance. I'm someone's sister. Someone's family.\n",
            "\n",
            "Scene 3\n",
            "\n",
            "As the protagonist is talking to her sister, her sister starts to cry.\n",
            "\n",
            "Protagonist: (surprised) Are you okay?\n",
            "\n",
            "Protagonist sister: (sobbing) I just can't believe that you're not even alive. I mean, I thought you were gone forever. But then you showed up out of the blue, and no one even knew.\n",
            "\n",
            "Protagonist: (tearfully) I know, I know. I made a mistake when I left. But that mistake had consequences. And now it's too late for me to undo.\n",
            "\n",
            "Scene 4\n",
            "\n",
            "As the protagonist is trying to come to terms with the consequences of her mistake, her sister starts to apologize.\n",
            "\n",
            "Protagonist sister: (sorrowfully) I'm sorry for not being there when you needed me the most. But you had your own reasons for leaving. I couldn't imagine facing the reality without you.\n",
            "\n",
            "Protagonist: (tearfully) It's okay. But I still haven't been able to fully forgive myself.\n",
            "\n",
            "Scene 5\n",
            "\n",
            "As the protagonist is contemplating her life choices, she receives a letter from her sister.\n",
            "\n",
            "Protagonist sister: Dear Protagonist,\n",
            "\n",
            "I hope this letter finds you well. Your decision to leave has been one of the hardest things I have ever had to bear.\n",
            "\n",
            "Protagonist: (surprised) Hi, I'\n",
            "\n",
            "\n",
            "Prompt 2 - Assemble the previous outputs into one cohesive and intersting story premise.output 1: Introduce protagonist [name] as a [character role]. Establish their motivation for pursuing a [goal].<|assistant|>\n",
            "\n",
            "ACT ONE\n",
            "INT. ABANDONED BUILDING - DAY\n",
            "[Scene: Introductions and set up for Jared, a 28-year-old who recently graduated from college. He's dressed in his school's letterman's jacket - the kind worn at the end of football games.]\n",
            "\n",
            "JARED (To himself, in a deep voice). I just have to finish this thesis. I'll see the light at the end of the tunnel.\n",
            "\n",
            "INT. CURRENT ABANDONED BUILDING - NIGHT\n",
            "\n",
            "MORGAN (In a cold, measured tone). Jared, I need you to step aside and let me handle this.\n",
            "\n",
            "JARED (In a defensive tone, not wanting to be taken from under the carnage). Who said I wanted a piece of you? You were always better than me.\n",
            "\n",
            "MORGAN (Punching Jared in the arm). Jared, give me a chance to explain. These are my students you're talking about. How many were there?\n",
            "\n",
            "JARED (Angry, defensive voice). You think you're better than us? All you care about is money and fame. You're just a glorified clown.\n",
            "\n",
            "MORGAN (Grabbing Jared's shoulder with a force that surprises Jared). Did you see the bodies? How many students are dead?\n",
            "\n",
            "JARED (Shook and sprints away). What do you want from me? You're nothing to me. You just want my money and attention.\n",
            "\n",
            "MORGAN (Panting and breathing heavily, holding Jared's shoulder). I got the call of the day. There's a cultist organization planning an attack on the school. Jared, find them. output 2: Introduce [antagonistic force] and how moment when protagonist overcomes [antagonistic force]<|assistant|>\n",
            "\n",
            "Scene 2\n",
            "\n",
            "The protagonist is sitting in a dimly lit room, surrounded by letters and documents.\n",
            "\n",
            "Protagonist: (frustrated) Why doesn’t anyone care about what I have to say? I’m not just a minor annoyance. I'm someone's sister. Someone's family.\n",
            "\n",
            "Scene 3\n",
            "\n",
            "As the protagonist is talking to her sister, her sister starts to cry.\n",
            "\n",
            "Protagonist: (surprised) Are you okay?\n",
            "\n",
            "Protagonist sister: (sobbing) I just can't believe that you're not even alive. I mean, I thought you were gone forever. But then you showed up out of the blue, and no one even knew.\n",
            "\n",
            "Protagonist: (tearfully) I know, I know. I made a mistake when I left. But that mistake had consequences. And now it's too late for me to undo.\n",
            "\n",
            "Scene 4\n",
            "\n",
            "As the protagonist is trying to come to terms with the consequences of her mistake, her sister starts to apologize.\n",
            "\n",
            "Protagonist sister: (sorrowfully) I'm sorry for not being there when you needed me the most. But you had your own reasons for leaving. I couldn't imagine facing the reality without you.\n",
            "\n",
            "Protagonist: (tearfully) It's okay. But I still haven't been able to fully forgive myself.\n",
            "\n",
            "Scene 5\n",
            "\n",
            "As the protagonist is contemplating her life choices, she receives a letter from her sister.\n",
            "\n",
            "Protagonist sister: Dear Protagonist,\n",
            "\n",
            "I hope this letter finds you well. Your decision to leave has been one of the hardest things I have ever had to bear.\n",
            "\n",
            "Protagonist: (surprised) Hi, I'.\n",
            "\n",
            "Response 2 - \n",
            "\n",
            "Scene 6\n",
            "\n",
            "As the protagonist is feeling guilty about her actions, she receives a letter from her past trauma.\n",
            "\n",
            "Protagonist past trauma: (sarcastically) The decision you've just made to leave has had consequences we can't undo. My life was ruined by your actions. I'll never be the same.\n",
            "\n",
            "Protagonist: (embarrassed) I'm so sorry. I couldn't imagine how difficult it must have been for you.\n",
            "\n",
            "Protagonist past trauma: (angrily) It's been more than a decade since you left. Can't you just let go of this guilt and move on?\n",
            "\n",
            "Protagonist: (apologetic) I know, and I understand. But I can't help wondering how you've turned out since then.\n",
            "\n",
            "Protagonist past trauma: (furious) Who do you think you are to ask such things? You should be proud that your actions have made me the person I am today.\n",
            "\n",
            "Protagonist: (emotionally) My mistake. It wasn't your fault. I was just so angry and confused at the time. I'm sorry for not being more understanding.\n",
            "\n",
            "Protagonist past trauma: (disinterested) Don't expect any of that from me. I've nothing but contempt and disdain for those who mess with my family.\n",
            "\n",
            "Scene 7\n",
            "\n",
            "As the protagonist is reflecting on her past mistakes, she receives a letter from the cult leader she confronted.\n",
            "\n",
            "Cult leader: (sarcastically) You underestimate my ability to manipulate and control your emotions. But let's be real. You never really gave it a fair shake.\n",
            "\n",
            "Culture leader: (laughing) And now you're\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responses = tinyLlama_model.substep_generate(prompts_data_analysis, cache=True, temperature=0.1)\n",
        "print_responses(responses, prompts_data_analysis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nfpmWOjkYaos",
        "outputId": "27c5d7d4-d6eb-47d1-f873-23be4d6ec472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [08:41<00:00, 173.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Prompt 0 - Describe the geographical and temporal area covered by data like: Based on longitude and latitude, this data describes [geographical area] across [timeframe]. headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"]\n",
            "data = [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]\n",
            "\n",
            "Response 0 - \n",
            "\n",
            "# 4. Data cleaning: Remove any duplicates, missing values, or invalid data. Use appropriate data cleaning techniques like:\n",
            "\n",
            "# 1. Dropping duplicates:\n",
            "data = data.drop_duplicates(subset=[\"Sample_ID\"])\n",
            "\n",
            "# 2. Removing missing values:\n",
            "data = data.dropna(subset=[\"Concentration_ppm\"])\n",
            "\n",
            "# 3. Validating data:\n",
            "data = data.dropna(subset=[\"Concentration_ppm\"])\n",
            "\n",
            "# 4. Checking for invalid data:\n",
            "data = data.dropna(subset=[\"Concentration_ppm\"])\n",
            "\n",
            "# 5. Checking for missing values:\n",
            "data = data.dropna(subset=[\"Concentration_ppm\"])\n",
            "\n",
            "# 6. Checking for duplicates:\n",
            "data = data.drop_duplicates(subset=[\"Sample_ID\"])\n",
            "\n",
            "# 7. Checking for invalid data:\n",
            "data = data.dropna(subset=[\"Concentration_ppm\"])\n",
            "\n",
            "# 8. Checking for missing values:\n",
            "data = data.dropna(subset=[\"Concentration_ppm\"])\n",
            "\n",
            "# 9. Checking for duplicates:\n",
            "data = data.drop_duplicates(subset=[\"Sample_ID\"])\n",
            "\n",
            "# 1\n",
            "\n",
            "\n",
            "Prompt 1 - Contextualize the soil quality data like: The underlying [pattern] has [implication] which can lead to [consequence]. headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"]\n",
            "data = [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]\n",
            "\n",
            "Response 1 - \n",
            "\n",
            "# 3. Visualize the data using a scatter plot\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "\n",
            "# Set the plotting style\n",
            "sns.set_style(\"darkgrid\")\n",
            "\n",
            "# Plot the scatter plot\n",
            "plt.scatter(data[0][1], data[0][2], c=data[0][3], s=100, label=\"Lead\")\n",
            "plt.scatter(data[1][1], data[1][2], c=data[1][3], s=100, label=\"Lead\")\n",
            "plt.scatter(data[2][1], data[2][2], c=data[2][3], s=100, label=\"Lead\")\n",
            "plt.xlabel(\"Latitude\")\n",
            "plt.ylabel(\"Longitude\")\n",
            "plt.title(\"Soil Quality Data\")\n",
            "plt.legend()\n",
            "plt.show()\n",
            "\n",
            "# 4. Calculate the correlation coefficient between the soil quality parameters and the concentration of lead\n",
            "from scipy.stats import spearmanr\n",
            "\n",
            "# Calculate the correlation coefficient\n",
            "correlation = spearmanr(data[0][3], data[1][3])\n",
            "\n",
            "# Print the correlation coefficient\n",
            "print(f\"The correlation coefficient between soil quality parameters and lead concentration is: {\n",
            "\n",
            "\n",
            "Prompt 2 - Combine the previous outputs into a single, concise summary of the key findings.output 1: Describe the geographical and temporal area covered by data like: Based on longitude and latitude, this data describes [geographical area] across [timeframe]. headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"]\n",
            "data = [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]<|assistant|>\n",
            "\n",
            "# 4. Data cleaning: Remove any duplicates, missing values, or invalid data. Use appropriate data cleaning techniques like:\n",
            "\n",
            "# 1. Dropping duplicates:\n",
            "data = data.drop_duplicates(subset=[\"Sample_ID\"])\n",
            "\n",
            "# 2. Removing missing values:\n",
            "data = data.dropna(subset=[\"Concentration_ppm\"])\n",
            "\n",
            "# 3. Validating data:\n",
            "data = data.dropna(subset=[\"Concentration_ppm\"])\n",
            "\n",
            "# 4. Checking for invalid data:\n",
            "data = data.dropna(subset=[\"Concentration_ppm\"])\n",
            "\n",
            "# 5. Checking for missing values:\n",
            "data = data.dropna(subset=[\"Concentration_ppm\"])\n",
            "\n",
            "# 6. Checking for duplicates:\n",
            "data = data.drop_duplicates(subset=[\"Sample_ID\"])\n",
            "\n",
            "# 7. Checking for invalid data:\n",
            "data = data.dropna(subset=[\"Concentration_ppm\"])\n",
            "\n",
            "# 8. Checking for missing values:\n",
            "data = data.dropna(subset=[\"Concentration_ppm\"])\n",
            "\n",
            "# 9. Checking for duplicates:\n",
            "data = data.drop_duplicates(subset=[\"Sample_ID\"])\n",
            "\n",
            "# 1. output 2: Contextualize the soil quality data like: The underlying [pattern] has [implication] which can lead to [consequence]. headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"]\n",
            "data = [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]<|assistant|>\n",
            "\n",
            "# 3. Visualize the data using a scatter plot\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "\n",
            "# Set the plotting style\n",
            "sns.set_style(\"darkgrid\")\n",
            "\n",
            "# Plot the scatter plot\n",
            "plt.scatter(data[0][1], data[0][2], c=data[0][3], s=100, label=\"Lead\")\n",
            "plt.scatter(data[1][1], data[1][2], c=data[1][3], s=100, label=\"Lead\")\n",
            "plt.scatter(data[2][1], data[2][2], c=data[2][3], s=100, label=\"Lead\")\n",
            "plt.xlabel(\"Latitude\")\n",
            "plt.ylabel(\"Longitude\")\n",
            "plt.title(\"Soil Quality Data\")\n",
            "plt.legend()\n",
            "plt.show()\n",
            "\n",
            "# 4. Calculate the correlation coefficient between the soil quality parameters and the concentration of lead\n",
            "from scipy.stats import spearmanr\n",
            "\n",
            "# Calculate the correlation coefficient\n",
            "correlation = spearmanr(data[0][3], data[1][3])\n",
            "\n",
            "# Print the correlation coefficient\n",
            "print(f\"The correlation coefficient between soil quality parameters and lead concentration is: {.\n",
            "\n",
            "Response 2 - }\")\n",
            "\n",
            "# 5. Calculate the correlation coefficient between the soil quality parameters and the concentration of other heavy metals\n",
            "from scipy.stats import spearmanr\n",
            "\n",
            "# Calculate the correlation coefficient\n",
            "correlation = spearmanr(data[2][3], data[3][3])\n",
            "\n",
            "# Print the correlation coefficient\n",
            "print(f\"The correlation coefficient between soil quality parameters and other heavy metals concentration is: {.<|assistant|>}\")\n",
            "\n",
            "# 6. Visualize the correlation between soil quality parameters and heavy metals concentration\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "\n",
            "# Set the plotting style\n",
            "sns.set_style(\"darkgrid\")\n",
            "\n",
            "# Plot the scatter plot\n",
            "plt.scatter(data[0][3], data[1][3], c=data[2][3], s=100, label=\"Lead\")\n",
            "plt.scatter(data[2][3], data[3][3], c=data[3][3], s=100, label=\"Other heavy metals\")\n",
            "plt.xlabel(\"Lead concentration\")\n",
            "plt.ylabel(\"Other heavy metals concentration\")\n",
            "plt.title(\"Correlation between Soil Quality Parameters and Heavy Metals Concentration\")\n",
            "plt.legend()\n",
            "plt.show()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responses = tinyLlama_model.substep_generate(prompts_documentation, cache=True, temperature=0.001)\n",
        "print_responses(responses, prompts_documentation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aXNNdE9WYkF8",
        "outputId": "d84627c1-5e08-4ce0-9929-cd49db6cf015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [08:37<00:00, 172.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Prompt 0 - List all variables in this code that should become function parameters and the final value that should be returned. embedding = model.encode(query, convert_to_tensor=True)\n",
            "hits = util.semantic_search(embedding, corpus_embeddings, top_k=5)[0]\n",
            "top_k_keys = \"Fetched features and values:\n",
            "\"\n",
            "for hit in hits:\n",
            "    key = corpus_keys[hit['corpus_id']]\n",
            "    top_k_keys += f\"{key} - {response_map_descriptions[key][\"value\"]}\"\n",
            "\n",
            "\n",
            "Response 0 - \n",
            "Here's the full code with the variables and their corresponding values:\n",
            "\n",
            "```python\n",
            "import tensorflow as tf\n",
            "import tensorflow_hub as hub\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import re\n",
            "import nltk\n",
            "from nltk.corpus import stopwords\n",
            "from nltk.stem import PorterStemmer\n",
            "from nltk.tokenize import word_tokenize\n",
            "from sklearn.feature_extraction.text import CountVectorizer\n",
            "from sklearn.metrics.pairwise import cosine_similarity\n",
            "from sklearn.metrics import jaccard_similarity_score\n",
            "from sklearn.metrics import precision_score\n",
            "from sklearn.metrics import recall_score\n",
            "from sklearn.metrics import f1_score\n",
            "from sklearn.metrics import accuracy_score\n",
            "from sklearn.metrics import confusion_matrix\n",
            "from sklearn.metrics import classification_report\n",
            "from sklearn.metrics import roc_auc_score\n",
            "from sklearn.metrics import roc_curve\n",
            "from sklearn.metrics import precision_recall_curve\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from sklearn.metrics import mean_absolute_error\n",
            "from sklearn.metrics import median_absolute_error\n",
            "from sklearn.metrics import r2_score\n",
            "from sklearn\n",
            "\n",
            "\n",
            "Prompt 1 - Extract the core logic of the code, focusing on the search and retrieval steps, independent of string formatting and variables. embedding = model.encode(query, convert_to_tensor=True)\n",
            "hits = util.semantic_search(embedding, corpus_embeddings, top_k=5)[0]\n",
            "top_k_keys = \"Fetched features and values:\n",
            "\"\n",
            "for hit in hits:\n",
            "    key = corpus_keys[hit['corpus_id']]\n",
            "    top_k_keys += f\"{key} - {response_map_descriptions[key][\"value\"]}\"\n",
            "\n",
            "\n",
            "Response 1 - \n",
            "Here's the revised code that extracts the core logic of the search and retrieval steps:\n",
            "\n",
            "```python\n",
            "import tensorflow as tf\n",
            "import tensorflow_hub as hub\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import re\n",
            "import string\n",
            "import nltk\n",
            "from nltk.corpus import stopwords\n",
            "from nltk.stem import PorterStemmer\n",
            "from nltk.tokenize import word_tokenize\n",
            "from tensorflow.keras.preprocessing.text import Tokenizer\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional\n",
            "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
            "from tensorflow.keras.optimizers import Adam\n",
            "from tensorflow.keras.losses import binary_crossentropy\n",
            "from tensorflow.keras.metrics import accuracy\n",
            "from tensorflow.keras.utils import to_categorical\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras.preprocessing.text import Tokenizer\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.\n",
            "\n",
            "\n",
            "Prompt 2 - Assemble the identified parameters, core logic, and formatted output into the final function: def cosine_similarity_fetch(args)output 1: List all variables in this code that should become function parameters and the final value that should be returned. embedding = model.encode(query, convert_to_tensor=True)\n",
            "hits = util.semantic_search(embedding, corpus_embeddings, top_k=5)[0]\n",
            "top_k_keys = \"Fetched features and values:\n",
            "\"\n",
            "for hit in hits:\n",
            "    key = corpus_keys[hit['corpus_id']]\n",
            "    top_k_keys += f\"{key} - {response_map_descriptions[key][\"value\"]}\"\n",
            "<|assistant|>\n",
            "Here's the full code with the variables and their corresponding values:\n",
            "\n",
            "```python\n",
            "import tensorflow as tf\n",
            "import tensorflow_hub as hub\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import re\n",
            "import nltk\n",
            "from nltk.corpus import stopwords\n",
            "from nltk.stem import PorterStemmer\n",
            "from nltk.tokenize import word_tokenize\n",
            "from sklearn.feature_extraction.text import CountVectorizer\n",
            "from sklearn.metrics.pairwise import cosine_similarity\n",
            "from sklearn.metrics import jaccard_similarity_score\n",
            "from sklearn.metrics import precision_score\n",
            "from sklearn.metrics import recall_score\n",
            "from sklearn.metrics import f1_score\n",
            "from sklearn.metrics import accuracy_score\n",
            "from sklearn.metrics import confusion_matrix\n",
            "from sklearn.metrics import classification_report\n",
            "from sklearn.metrics import roc_auc_score\n",
            "from sklearn.metrics import roc_curve\n",
            "from sklearn.metrics import precision_recall_curve\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from sklearn.metrics import mean_absolute_error\n",
            "from sklearn.metrics import median_absolute_error\n",
            "from sklearn.metrics import r2_score\n",
            "from sklearn. output 2: Extract the core logic of the code, focusing on the search and retrieval steps, independent of string formatting and variables. embedding = model.encode(query, convert_to_tensor=True)\n",
            "hits = util.semantic_search(embedding, corpus_embeddings, top_k=5)[0]\n",
            "top_k_keys = \"Fetched features and values:\n",
            "\"\n",
            "for hit in hits:\n",
            "    key = corpus_keys[hit['corpus_id']]\n",
            "    top_k_keys += f\"{key} - {response_map_descriptions[key][\"value\"]}\"\n",
            "<|assistant|>\n",
            "Here's the revised code that extracts the core logic of the search and retrieval steps:\n",
            "\n",
            "```python\n",
            "import tensorflow as tf\n",
            "import tensorflow_hub as hub\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import re\n",
            "import string\n",
            "import nltk\n",
            "from nltk.corpus import stopwords\n",
            "from nltk.stem import PorterStemmer\n",
            "from nltk.tokenize import word_tokenize\n",
            "from tensorflow.keras.preprocessing.text import Tokenizer\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional\n",
            "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
            "from tensorflow.keras.optimizers import Adam\n",
            "from tensorflow.keras.losses import binary_crossentropy\n",
            "from tensorflow.keras.metrics import accuracy\n",
            "from tensorflow.keras.utils import to_categorical\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras.preprocessing.text import Tokenizer\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras..\n",
            "\n",
            "Response 2 - \n",
            "Here's the full code with the variables and their corresponding values:\n",
            "\n",
            "```python\n",
            "import tensorflow as tf\n",
            "import tensorflow_hub as hub\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import re\n",
            "import string\n",
            "import nltk\n",
            "from nltk.corpus import stopwords\n",
            "from nltk.stem import PorterStemmer\n",
            "from nltk.tokenize import word_tokenize\n",
            "from tensorflow.keras.preprocessing.text import Tokenizer\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional\n",
            "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
            "from tensorflow.keras.optimizers import Adam\n",
            "from tensorflow.keras.losses import binary_crossentropy\n",
            "from tensorflow.keras.metrics import accuracy\n",
            "from tensorflow.keras.utils import to_categorical\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras.preprocessing.text import Tokenizer\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Dense, Dropout\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Prompt Interpretability and Failure Analysis**\n",
        "\n",
        "## Attention patterns and model behavior with different prompt structures\n",
        "* *Prepend instruction to prompt* - Generates undesired code for data analysis prompt.\n",
        "* *Append instruction to prompt* - Improved output for data analysis prompt, generated output followed provided template structure in instructions.\n",
        "* *Insert instruction to prompt* - Generates incoherent and repetitive data analysis prompt.\n",
        "\n",
        "##  When and why do optimized prompts fail?\n",
        "Optimized prompts may still fail to generate cohereant, accurate, and quality responses if the task at hand is too complex to understand (see data analysis prompt below) because it may require too many steps for the model to keep track of. Or, the prompt may fail if it requires a timely answer that the model can't provide based on the date of its training data.\n",
        "\n",
        "## Prompt complexity and model interpretability\n",
        "In the case of tinyLlama, the more complex the prompt is, the less interpretable the model's output is. This may be because if the model generates a suboptimal token towards the beginning the token has a higher chance of propagation that error into future tokens and because complex tasks build on top of each other, the model may generate something that is uninterpretable and incorrect.\n",
        "\n",
        "## Systematic Error Analysis\n",
        "* *Where prompt engineering breaks down* - Problems involving complex calculations, development of mathematical proofs, logic puzzles, numerical data analysis & processing (through this experiment)\n",
        "* *An optimal prompt* - Fits within the token window, it includes specified examples of various potential input-output pairs and it uses concise language that has low possibility of being misinterpreted. (as demonstrated by first creative writing prompt which generated an acceptable output that followed the instructions of the prompt)\n",
        "* *Biased prompts* - A prompt may accidentally introduce bias by providing assumptions that are untrue. LLMs are created to generate the next most likely token at each pass through the transformer architecture, the biased assumptions may result in a bias token being generated, and thus the bias propels through the rest of the generated output through self-attention mechanisms.  \n",
        "## Model reasoning transparency using prompt techniques\n",
        "By using techniques such as context-aware decomposition, users are narrow the problem point down to a specific step. This makes it easier to follow the model's path of reasoning step-by-step and debug.\n",
        "## Prompt-based optimization limitations compared to fine-tuning.\n",
        "In prompt-based optimization, the instructions for how to process and execute a task is constrained to the size of the input window and it must be provided at each new generation. When there is a large amount of data that requires processing, fine-tuning a model may improve the ability for model to generate the desired output.  \n",
        "## Genuine understanding? Or spurious correlations?\n",
        "The experiment below demonstrates the inability for the model to consistently understand the required task. For example, even a minor alteration of the prompt (prepending the instructions after the data instead of after it) causes the model to assume the python list brackets [ ] means the prompt is asking for code to analyse the list data. In other cases, it may be that the model hallucinates and provides incorrect and or incoherent information."
      ],
      "metadata": {
        "id": "Tem9vp8Q_Bfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Given the data, note geographical observations from spatial context and a summary of contamination level and its implications\"\n",
        "data1 = \"\"\"headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"]\"\"\"\n",
        "data2 = \"\"\"[ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]\"\"\"\n",
        "prompt_prepend_instruction = f\"{instruction} {data1} {data2}\"\n",
        "prompt_append_instruction = f\"{data1} {data2} {instruction}\"\n",
        "prompt_insert_instruction = f\"{data1} {instruction} {data2}\"\n",
        "prompts = [prompt_prepend_instruction, prompt_append_instruction, prompt_insert_instruction]"
      ],
      "metadata": {
        "id": "9LdYac3-_CkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses = tinyLlama_model.multi_generate(prompts, cache=True, temperature=0.1)\n",
        "print_responses(responses, prompts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rzcCvXsF8bF",
        "outputId": "8a4eb67f-02ab-43d5-f59c-6f915013da01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [08:00<00:00, 160.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Prompt 0 - Given the data, note geographical observations from spatial context and a summary of contamination level and its implications headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"] [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]\n",
            "\n",
            "Response 0 - \n",
            "\n",
            "# 4. Calculate the mean and standard deviation of the detected lead concentration for each sample\n",
            "mean_lead = sum(lead_concentration_data) / len(lead_concentration_data)\n",
            "std_lead = math.sqrt(sum((lead_concentration_data - mean_lead) ** 2) / len(lead_concentration_data))\n",
            "\n",
            "# 5. Plot the contamination level and its implications\n",
            "plt.scatter(lead_concentration_data[:, 0], lead_concentration_data[:, 1], c=\"red\", s=100)\n",
            "plt.xlabel(\"Latitude\")\n",
            "plt.ylabel(\"Longitude\")\n",
            "plt.title(\"Contamination Level and Its Implications\")\n",
            "plt.show()\n",
            "\n",
            "# 6. Save the contamination level and its implications as a CSV file\n",
            "with open(\"contamination_level.csv\", \"w\") as file:\n",
            "    file.write(\"Sample_ID,Latitude,Longitude,Collection_Date,Detected,Concentration_ppm\\n\")\n",
            "    for row in lead_concentration_data:\n",
            "        file.write(f\"{row[0]},{row[1\n",
            "\n",
            "\n",
            "Prompt 1 - headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"] [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ] Given the data, note geographical observations from spatial context and a summary of contamination level and its implications\n",
            "\n",
            "Response 1 - \n",
            "\n",
            "Geographical observations from spatial context:\n",
            "\n",
            "- The data is collected from multiple locations across the United States, indicating that the contamination is not limited to a single location.\n",
            "- The data is collected over a period of several years, indicating that the contamination is ongoing and not a one-time event.\n",
            "\n",
            "Summary of contamination level and its implications:\n",
            "\n",
            "- The detected concentration of lead in the data is high, indicating that the contamination is significant and poses a risk to human health.\n",
            "- The concentration of lead in the data is higher in areas with higher population density, indicating that the contamination is more prevalent in urban areas.\n",
            "- The concentration of lead in the data is higher in areas with higher income levels, indicating that the contamination is more prevalent in wealthier neighborhoods.\n",
            "- The concentration of lead in the data is higher in areas with higher levels of pollution, indicating that the contamination is more prevalent in areas with higher levels of air and water pollution.\n",
            "- The concentration of lead in the data is higher in areas with higher levels of industrial activity, indicating that the contamination is more prevalent in areas with higher levels of industrial activity.\n",
            "- The concentration of lead in the data is higher in areas with higher levels of poverty, indicating that the contamination is more prevalent in areas with higher levels of\n",
            "\n",
            "\n",
            "Prompt 2 - headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"] Given the data, note geographical observations from spatial context and a summary of contamination level and its implications [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]\n",
            "\n",
            "Response 2 - \n",
            "\n",
            "1. Identify the geographical observations from spatial context:\n",
            "\n",
            "- The first observation has latitude and longitude values of 40.7 and -74.5, respectively.\n",
            "- The second observation has latitude and longitude values of 40.7 and -74.8, respectively.\n",
            "- The third observation has latitude and longitude values of 40.6 and -74.4, respectively.\n",
            "\n",
            "2. Summarize the contamination level and its implications:\n",
            "\n",
            "- The first observation has a concentration of 245.7 ppm of lead.\n",
            "- The second observation has a concentration of 312.4 ppm of lead.\n",
            "- The third observation has a concentration of 398.1 ppm of lead.\n",
            "\n",
            "3. Identify the spatial context:\n",
            "\n",
            "- The first observation is located in New York City, while the second and third observations are located in New Jersey.\n",
            "\n",
            "4. Identify the geographical location of the first observation:\n",
            "\n",
            "- The first observation is located in the vicinity of the Hudson River.\n",
            "\n",
            "5. Identify the geographical location of the second observation:\n",
            "\n",
            "- The second observation is located in the vicinity of the Hudson River.\n",
            "\n",
            "6. Identify the geographical location of the third observation:\n",
            "\n",
            "- The third observation is located in the vicinity of the Hudson River.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **Automated Prompt Evaluation and Scaling**\n",
        "\n",
        "Design metrics for prompt quality that go beyond task-specific performance - As stated previously, evaluating the response given by the LLM is a good metric for judging the quality of any prompt:\n",
        "* *Is LLM's response factually accurate?*\n",
        "* *Is LLM's response complying with instructions?*\n",
        "* *Is LLM's response coherent & relevant to the prompt? Is LLM generating irrelevant data?*\n",
        "* *Is LLM's response generally consistent across multiple generations using same prompt?*\n",
        "\n",
        "## Computational costs of different optimization approaches\n",
        "| Prompt optimization technique      |Computational costs (1-10)|\n",
        "|------------------------------------|:------------------------:|\n",
        "| Meta-prompting                     |             2            |\n",
        "| Recursive self-improvement         |             7            |\n",
        "| Template-based optimization        |             1            |\n",
        "| Context-aware decomposition        |             7            |\n",
        "\n",
        "## techniques for prompt transfer across related tasks\n",
        "* Append one-shot or few-shot examples at the end of a prompt to tailor model's generation to the needs of the specific task.\n",
        "* Fine-tuning to personalize the model towards completing a similar set of tasks\n",
        "\n",
        "**automated prompt generation**: Program the same or another model to generate prompts to be inputted into a model to complete a certain task. This is more efficient because the 1st model can be asked to generate multiple variations of the same prompt efficiently.\n",
        "\n",
        "**refinement systems**: Program  the same or another model to do recursive self-improvement and run through multi-iteration refinement cycles before stopping the model when an acceptable prompt is generated.\n"
      ],
      "metadata": {
        "id": "lNfOb2cTNzHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_output_quality(actual_output, desired_output, model):\n",
        "    # Generate embeddings\n",
        "    actual_embedding = model.encode(actual_output, convert_to_tensor=True)\n",
        "    desired_output_embedding = model.encode(desired_output, convert_to_tensor=True)\n",
        "\n",
        "    # Cosine semantic search to compare embeddings\n",
        "    hits = util.semantic_search(actual_embedding, desired_output_embedding, top_k=1)[0]\n",
        "\n",
        "    similarity_score = hits[0]['score'] # Return similarity score\n",
        "    return similarity_score"
      ],
      "metadata": {
        "id": "imj3_DdHMnd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "desired_output = \"\"\"The samples collected show increasing lead contamination levels which pose a clear risk to public health, particularly in an urban area like New York City, near Manhattan.\n",
        "Lead is a toxic substance, especially dangerous to young children and pregnant women. Chronic exposure to lead can result in developmental delays, cognitive impairment, and other serious health conditions.\n",
        "This level of lead contamination is significantly above safe thresholds for drinking water.\"\"\""
      ],
      "metadata": {
        "id": "On7TSBE9OOWX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluation system to compare prompt effectiveness:\")\n",
        "print(\"Prompt 0 = prepend instructions. Prompt 1 = append instructions. Prompt 2 = insert instructions\")\n",
        "print()\n",
        "for i, response in enumerate(responses):\n",
        "    similarity = compare_output_quality(response, desired_output, miniLM_model)\n",
        "    print(f\"Cosine Similarity Score of actual output vs desired output from using prompt {i}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxcMCFj2NhIV",
        "outputId": "b68f9a2b-e760-4102-a6fb-4eda14d5e9be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation system to compare prompt effectiveness:\n",
            "Prompt 0 = prepend instructions. Prompt 1 = append instructions. Prompt 2 = insert instructions\n",
            "\n",
            "Cosine Similarity Score of actual output vs desired output from using prompt 0: 0.4316\n",
            "Cosine Similarity Score of actual output vs desired output from using prompt 1: 0.6827\n",
            "Cosine Similarity Score of actual output vs desired output from using prompt 2: 0.5443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Methods for detecting prompt robustness across edge cases: Introducing text noise to prompts!\n",
        "eda_augmenter = EasyDataAugmenter(transformations_per_example=1) # Initialize the EasyDataAugmenter\n",
        "augmented_prompts = [eda_augmenter.augment(prompt)[0] for prompt in prompts] # Generate typographical errors like random inserts, deletions, or synonym replacements\n",
        "responses = tinyLlama_model.multi_generate(augmented_prompts, cache=True, temperature=0.1)\n",
        "print_responses(augmented_prompts, responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHjlTf2eQ3vq",
        "outputId": "cb303198-95e6-48ec-df6e-3b71ddef5466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "100%|██████████| 3/3 [07:09<00:00, 143.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Prompt 0 - Given the data, note geographical observations from spatial parallel context and a summary of contamination level and its implications headers = [\"twoscore Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"] [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"pollution Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]<|assistant|>\n",
            "\n",
            "# 4. Calculate the average concentration of lead in the given data\n",
            "avg_lead = sum(map(float, data[0])) / len(data[0])\n",
            "print(\"Average concentration of lead in the data:\", avg_lead)\n",
            "\n",
            "# 5. Calculate the standard deviation of lead concentration in the data\n",
            "std_lead = math.sqrt(sum((map(float, data[0]) - avg_lead) ** 2) / (len(data[0]) - 1))\n",
            "print(\"Standard deviation of lead concentration in the data:\", std_lead)\n",
            "\n",
            "# 6. Calculate the correlation coefficient between the concentration of lead and the geographical location\n",
            "correlation_coefficient = math.sqrt(sum((map(float, data[0]) - avg_lead) ** 2) / (len(data[0]) - 1)) / math.sqrt(sum((map(float, data[1]) - avg_latitude) ** 2) / (len(data[1]) - 1))\n",
            "print(\"Correlation coefficient between concentration of lead and geographical location:\", correlation_coefficient)\n",
            "\n",
            "# 7. Calculate the correlation coefficient between the concentration of lead and the contamination level\n",
            "correlation_coefficient = math.sqrt(sum((map(float\n",
            "\n",
            "Response 0 - Given the data, note geographical observations from spatial parallel context and a summary of contamination level and its implications headers = [\"twoscore Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"] [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"pollution Lead\", 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]\n",
            "\n",
            "\n",
            "Prompt 1 - headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"\", \"Concentration_ppm\"] [ [1, 40., -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\",.4], [3, 40.6, -74.4, \"2024-07-10\", \"\", 398.1] ] Given the data, note geographical observations from spatial context and a summary contamination level and its implications<|assistant|>\n",
            "\n",
            "Geographical observations from spatial context:\n",
            "\n",
            "- The data is collected in New York City, which is located in the northeastern United States.\n",
            "- The data is collected in the summer months, which are typically hot and humid in the region.\n",
            "- The data is collected in the suburbs of New York City, which are typically more densely populated and have higher concentrations of pollutants than the city center.\n",
            "\n",
            "Summary contamination level and its implications:\n",
            "\n",
            "- The data shows that the concentration of lead in the suburbs of New York City is significantly higher than the concentration in the city center. This indicates that the suburbs are more heavily polluted than the city center.\n",
            "- The implications of this finding are that the suburbs are more vulnerable to lead pollution than the city center, which could have implications for public health and safety.\n",
            "- The data also shows that the concentration of lead in the suburbs is higher than the concentration in the surrounding areas, which could indicate that the suburbs are more heavily polluted than the surrounding areas.\n",
            "- The data also shows that the concentration of lead in the suburbs is higher than the concentration in the surrounding areas, which could indicate that the suburbs are more heavily polluted than the surrounding areas.\n",
            "- The data suggests that the suburbs of New York City are more vulnerable to lead pollution\n",
            "\n",
            "Response 1 - headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"\", \"Concentration_ppm\"] [ [1, 40., -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\",.4], [3, 40.6, -74.4, \"2024-07-10\", \"\", 398.1] ] Given the data, note geographical observations from spatial context and a summary contamination level and its implications\n",
            "\n",
            "\n",
            "Prompt 2 - headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"] Given the data, note geographical observations from spatial context and a summary of contamination lxxiv level iv and its implications [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", ii 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]<|assistant|>\n",
            "\n",
            "1. Identify the geographical observations from spatial context: The data includes geographical observations such as latitude, longitude, and collection date. These observations can provide insights into the location of the contamination and its temporal and spatial context.\n",
            "\n",
            "2. Summarize the contamination levels: The data includes the detected, concentrated, and level IV contamination levels. These levels provide information on the severity and extent of contamination.\n",
            "\n",
            "3. Identify the implications: The data can provide insights into the potential health risks associated with contamination levels. For example, high levels of lead can lead to neurological damage, while high levels of cadmium can cause cancer.\n",
            "\n",
            "4. Analyze the data: The data can be analyzed to identify patterns and trends in contamination levels. For example, the data can be used to identify areas with high levels of contamination and to develop strategies to reduce contamination levels.\n",
            "\n",
            "5. Evaluate the data: The data can be evaluated to determine the effectiveness of strategies to reduce contamination levels. For example, strategies such as monitoring and testing can be evaluated to determine their effectiveness in reducing contamination levels.\n",
            "\n",
            "6. Implement strategies: Based on the analysis and evaluation of the data, strategies can be implemented to reduce contamination levels. For example, monitoring\n",
            "\n",
            "Response 2 - headers = [\"Sample_ID\", \"Latitude\", \"Longitude\", \"Collection_Date\", \"Detected\", \"Concentration_ppm\"] Given the data, note geographical observations from spatial context and a summary of contamination lxxiv level iv and its implications [ [1, 40.7, -74.5, \"2022-05-15\", \"Lead\", 245.7], [2, 40.7, -74.8, \"2023-06-22\", \"Lead\", ii 312.4], [3, 40.6, -74.4, \"2024-07-10\", \"Lead\", 398.1] ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluation system to compare prompt effectiveness:\")\n",
        "print(\"Prompt 0 = prepend instructions. Prompt 1 = append instructions. Prompt 2 = insert instructions\")\n",
        "print()\n",
        "for i, response in enumerate(responses):\n",
        "    similarity = compare_output_quality(response, desired_output, miniLM_model)\n",
        "    print(f\"Cosine Similarity Score of actual output vs desired output from using prompt {i}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LdjaKc6Thze",
        "outputId": "0b51cd91-8fcb-4d5d-cc68-fe8da8ace6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation system to compare prompt effectiveness:\n",
            "Prompt 0 = prepend instructions. Prompt 1 = append instructions. Prompt 2 = insert instructions\n",
            "\n",
            "Cosine Similarity Score of actual output vs desired output from using prompt 0: 0.4152\n",
            "Cosine Similarity Score of actual output vs desired output from using prompt 1: 0.5847\n",
            "Cosine Similarity Score of actual output vs desired output from using prompt 2: 0.5793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. **Future Directions and Theoretical Implications**\n",
        "\n",
        "## Broad principles of prompt engineering\n",
        "1. Draft minimal workflow\n",
        "2. Identify missing components, adding iteratively\n",
        "3. Give few-shot examples if outputs deviate\n",
        "4. Split tasks across chained prompts\n",
        "5. Validate with another LLM or rule-based checks\n",
        "6. Automate workflow with LangChain or LlamaIndex\n",
        "## Limitations of current prompting approaches\n",
        "In production, outputs must be reliable and controlled. This prevents failures and maintains quality of system. However, generative models usually generate inconsistent outputs. Present and proposed solutions use external extensions to add templates to guide output format of generative AI.\n",
        "\n",
        "## Human-AI interaction design\n",
        "Human-in-the-loop is a practice that can be utilized to ensure that if a generative AI outputs something undesirable, the prompt can be manually adjusted by the human and re-inputted back to the AI.\n",
        "\n",
        "## **Critical Analysis:**\n",
        "* Prompt engineering has already evolved in multiple aspects throughout the years but in the future, as already seen with the state-of-art models today. LLMs are increasingly able to make accurate assumptions about what users want given little to no instructions or context. In the future, there could be a smaller AI or just one sophisticated AI that can handle the task of prompt-engineering and the generation itself without the need for human input.\n",
        "* An ethical implication to consider is that if there's incorrect bias in the highly optimized prompt, how can it be captured downstream in production environments if the technological direction is moving further and further into automation and AI, removing  \n",
        "* Similar to humans, generative AI can have more difficulties in properly executing and creating responses if the prompt lacks clarity and requirements for desired response formats.\n",
        "* Prompt engineering can specify **guardrails** which are instructions meant to deter the model from responding to harmful, hurtful, or irrelevant prompts."
      ],
      "metadata": {
        "id": "RLfyabR8_C-I"
      }
    }
  ]
}