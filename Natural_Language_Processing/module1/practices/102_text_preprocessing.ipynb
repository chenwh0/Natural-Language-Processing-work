{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing in Natural Language Processing: A Comprehensive Guide\n",
    "\n",
    "## Introduction to Text Preprocessing\n",
    "\n",
    "**Purpose:**\n",
    "Text preprocessing is the critical first step in any Natural Language Processing (NLP) pipeline. Raw text data is inherently messy and inconsistent—it contains capitalization variations, punctuation, stopwords, and different forms of the same word. Preprocessing transforms this unstructured text into a clean, standardized format that machine learning algorithms can effectively analyze.\n",
    "\n",
    "**Why Text Preprocessing Matters:**\n",
    "- **Consistency**: Ensures uniform treatment of similar words (e.g., \"Running\" and \"running\")\n",
    "- **Noise reduction**: Removes irrelevant elements that don't contribute to meaning\n",
    "- **Feature optimization**: Creates better input features for machine learning models\n",
    "- **Performance improvement**: Reduces computational complexity and improves model accuracy\n",
    "- **Standardization**: Enables fair comparison between different text samples\n",
    "\n",
    "**Common Preprocessing Challenges:**\n",
    "- Handling different text formats and encodings\n",
    "- Preserving meaningful information while removing noise\n",
    "- Balancing preprocessing depth with computational efficiency\n",
    "- Maintaining context when simplifying text structure\n",
    "\n",
    "## Essential Libraries and Setup\n",
    "\n",
    "Before diving into preprocessing techniques, let's understand the key libraries and their roles:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK downloads (run once)\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# spaCy model (install once)\n",
    "# !python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "**Library Overview:**\n",
    "- **NLTK (Natural Language Toolkit)**: Comprehensive toolkit with traditional NLP methods\n",
    "- **spaCy**: Industrial-strength NLP library optimized for production use\n",
    "- **re (Regular Expressions)**: Pattern matching for complex text cleaning tasks\n",
    "- **string**: Built-in Python module for string manipulation utilities\n",
    "- **collections.Counter**: Efficient counting of token frequencies\n",
    "\n",
    "## 1. Comprehensive Text Tokenization and Cleaning\n",
    "\n",
    "**Purpose:**\n",
    "Tokenization breaks text into individual meaningful units (tokens), while cleaning removes elements that typically don't contribute to analysis. This foundational step affects all subsequent processing stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    \"\"\"\n",
    "    Preprocess a list of texts by:\n",
    "    - Tokenizing each text\n",
    "    - Removing stopwords, digits, and punctuation\n",
    "    - Lowercasing the tokens\n",
    "    Returns a list of lists of cleaned tokens.\n",
    "    \"\"\"\n",
    "    # Load English stopwords once for efficiency\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    def remove_stops_digits(tokens):\n",
    "        \"\"\"\n",
    "        Remove stopwords, digits, and punctuation from a list of tokens.\n",
    "        Convert remaining tokens to lowercase.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            token.lower()\n",
    "            for token in tokens\n",
    "            if token.lower() not in mystopwords   # Remove stopwords (case-insensitive)\n",
    "            and not token.isdigit()               # Remove digits\n",
    "            and token not in punctuation          # Remove punctuation\n",
    "        ]\n",
    "    \n",
    "    # Tokenize and clean each text in the input list\n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
    "\n",
    "# Example usage\n",
    "texts = [\n",
    "    \"This is an example sentence, showing off the stop words filtration.\",\n",
    "    \"NLTK is a leading platform for building Python programs to work with human language data.\",\n",
    "    \"In 2025, Natural Language Processing is widely used!\"\n",
    "]\n",
    "cleaned_tokens = preprocess_corpus(texts)\n",
    "\n",
    "print(\"Original Texts:\")\n",
    "for t in texts:\n",
    "    print(\"-\", t)\n",
    "print(\"\\nProcessed Tokens:\")\n",
    "for tokens in cleaned_tokens:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts Explained:**\n",
    "\n",
    "- **Tokenization**: Splits continuous text into discrete words or subwords\n",
    "- **Stopwords**: Common words (\"the\", \"is\", \"and\") that often don't carry significant meaning\n",
    "- **Case normalization**: Converting to lowercase prevents \"Apple\" and \"apple\" being treated differently\n",
    "- **Punctuation removal**: Eliminates non-alphabetic characters that usually don't contribute to semantic meaning\n",
    "\n",
    "**When to Modify This Approach:**\n",
    "- **Keep numbers**: For financial or scientific texts where numbers are meaningful\n",
    "- **Preserve punctuation**: For sentiment analysis where \"!\" or \"?\" might indicate emotion\n",
    "- **Custom stopwords**: Add domain-specific common words (e.g., \"patient\" in medical texts)\n",
    "- **Keep capitalization**: For named entity recognition tasks\n",
    "\n",
    "**Advanced Tokenization Considerations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def advanced_tokenization(text):\n",
    "    \"\"\"Enhanced tokenization handling contractions and special patterns\"\"\"\n",
    "    # Expand contractions\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'re\", \" are\", text)\n",
    "    text = re.sub(r\"'ve\", \" have\", text)\n",
    "    text = re.sub(r\"'ll\", \" will\", text)\n",
    "    text = re.sub(r\"'d\", \" would\", text)\n",
    "    \n",
    "    # Handle URLs and email addresses\n",
    "    text = re.sub(r'http\\S+', '<URL>', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)\n",
    "    \n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stemming: Reducing Words to Root Forms\n",
    "\n",
    "**Purpose:**\n",
    "Stemming algorithmically reduces words to their root form (stem) by removing suffixes. While the results may not always be valid dictionary words, stemming helps group related words together for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Initialize the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example stems\n",
    "words = [\"cars\", \"revolution\", \"running\", \"flies\"]\n",
    "for word in words:\n",
    "    print(f\"{word} -> {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Stemming Results:**\n",
    "- **\"cars\" → \"car\"**: Perfect reduction to base form\n",
    "- **\"revolution\" → \"revolut\"**: Aggressive cutting that creates non-word\n",
    "- **\"running\" → \"run\"**: Successful removal of suffix\n",
    "- **\"flies\" → \"fli\"**: Over-stemming creates meaningless stem\n",
    "\n",
    "**Stemming Algorithm Types:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "\n",
    "# Compare different stemming algorithms\n",
    "stemmers = {\n",
    "    'Porter': PorterStemmer(),\n",
    "    'Snowball': SnowballStemmer('english'),\n",
    "    'Lancaster': LancasterStemmer()\n",
    "}\n",
    "\n",
    "test_words = [\"caring\", \"cares\", \"carefully\", \"careful\"]\n",
    "\n",
    "print(f\"{'Word':<12} {'Porter':<10} {'Snowball':<10} {'Lancaster':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for word in test_words:\n",
    "    stems = [stemmers[name].stem(word) for name in stemmers]\n",
    "    print(f\"{word:<12} {stems[0]:<10} {stems[1]:<10} {stems[2]:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to Use Stemming:**\n",
    "- **Information retrieval**: Search systems where \"running\" and \"runs\" should match\n",
    "- **Text classification**: When word variations don't significantly impact categories\n",
    "- **Large-scale processing**: When speed is more important than linguistic accuracy\n",
    "- **Resource-constrained environments**: Stemming requires less computational power than lemmatization\n",
    "\n",
    "**Limitations of Stemming:**\n",
    "- May create non-words that lose semantic meaning\n",
    "- Can be overly aggressive (e.g., \"university\" → \"univers\")\n",
    "- Language-specific rules don't capture all irregularities\n",
    "- May merge words with different meanings (e.g., \"arm\" from both \"arms\" and \"army\")\n",
    "\n",
    "## 3. Lemmatization with NLTK: Dictionary-Based Word Reduction\n",
    "\n",
    "**Purpose:**\n",
    "Lemmatization reduces words to their dictionary base form (lemma) using vocabulary and morphological analysis. Unlike stemming, lemmatization always produces valid words, making it more linguistically accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')  # Uncomment if running for the first time\n",
    "nltk.download('omw-1.4')  # Uncomment if running for the first time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize with part-of-speech specification for accuracy\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))  # 'a' for adjective; Output: good\n",
    "print(lemmatizer.lemmatize(\"running\", pos=\"v\")) # 'v' for verb; Output: run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Part-of-Speech (POS) Tags for Lemmatization:**\n",
    "- **'n' (noun)**: Default if no POS specified\n",
    "- **'v' (verb)**: For action words\n",
    "- **'a' (adjective)**: For descriptive words  \n",
    "- **'r' (adverb)**: For words modifying verbs, adjectives, or other adverbs\n",
    "\n",
    "**Enhanced Lemmatization with POS Tagging:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    \"\"\"Convert NLTK POS tags to WordNet POS tags\"\"\"\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "def advanced_lemmatization(tokens):\n",
    "    \"\"\"Lemmatize tokens using their POS tags for improved accuracy\"\"\"\n",
    "    pos_tokens = pos_tag(tokens)\n",
    "    lemmatized = []\n",
    "    \n",
    "    for token, pos in pos_tokens:\n",
    "        wordnet_pos = get_wordnet_pos(pos)\n",
    "        lemma = lemmatizer.lemmatize(token.lower(), pos=wordnet_pos)\n",
    "        lemmatized.append(lemma)\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "# Example usage\n",
    "tokens = [\"The\", \"dogs\", \"were\", \"running\", \"better\", \"than\", \"expected\"]\n",
    "lemmatized_tokens = advanced_lemmatization(tokens)\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Lemmatized:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of Lemmatization:**\n",
    "- **Linguistic accuracy**: Always produces valid dictionary words\n",
    "- **Contextual awareness**: Considers word meaning and context\n",
    "- **Semantic preservation**: Maintains word meaning better than stemming\n",
    "- **Consistency**: Same lemma for all inflected forms of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Industrial-Strength Processing with spaCy\n",
    "\n",
    "**Purpose:**\n",
    "spaCy provides fast, accurate, and production-ready NLP processing. Its lemmatization is integrated with part-of-speech tagging, named entity recognition, and other linguistic features, making it ideal for comprehensive text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# !python -m spacy download en_core_web_sm  # Run if the model isn't installed\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = u\"Missouri is known for its beautiful rivers and vibrant cities.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print original text and its lemma for each token\n",
    "for token in doc:\n",
    "    print(f\"{token.text:10s} -> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy's Integrated Approach:**\n",
    "Unlike NLTK's separate tools, spaCy processes text through a single pipeline that simultaneously performs:\n",
    "- Tokenization\n",
    "- Part-of-speech tagging\n",
    "- Lemmatization\n",
    "- Named entity recognition\n",
    "- Dependency parsing\n",
    "\n",
    "\n",
    "\n",
    "**Performance Comparison:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Compare processing speed between NLTK and spaCy\n",
    "sample_texts = [\"This is a sample sentence for processing.\"] * 1000\n",
    "\n",
    "# NLTK approach\n",
    "start_time = time.time()\n",
    "for text in sample_texts:\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmas = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(pos)) for token, pos in pos_tags]\n",
    "nltk_time = time.time() - start_time\n",
    "\n",
    "# spaCy approach\n",
    "start_time = time.time()\n",
    "for text in sample_texts:\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "spacy_time = time.time() - start_time\n",
    "\n",
    "print(f\"NLTK processing time: {nltk_time:.3f} seconds\")\n",
    "print(f\"spaCy processing time: {spacy_time:.3f} seconds\")\n",
    "print(f\"spaCy is {nltk_time/spacy_time:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Linguistic Analysis with spaCy\n",
    "\n",
    "**Purpose:**\n",
    "Beyond basic preprocessing, spaCy provides detailed linguistic information that can inform preprocessing decisions and enable advanced NLP applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = u\"Missouri is known for its beautiful rivers and vibrant cities. It became a state in 1821.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print detailed linguistic features for each token\n",
    "print(f\"{'Text':15} {'Lemma':15} {'POS':10} {'Shape':10} {'Alpha':5} {'Stop':5}\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:15} {token.lemma_:15} {token.pos_:10} {token.shape_:10} {str(token.is_alpha):5} {str(token.is_stop):5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Token Attributes:**\n",
    "- **text**: Original token as it appears in text\n",
    "- **lemma_**: Dictionary base form of the token\n",
    "- **pos_**: Part-of-speech tag (detailed grammatical category)\n",
    "- **shape_**: Pattern of capitalization and character types (X=uppercase, x=lowercase, d=digit)\n",
    "- **is_alpha**: Boolean indicating if token contains only alphabetic characters\n",
    "- **is_stop**: Boolean indicating if token is a stopword\n",
    "\n",
    "**Practical Applications:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_preprocessing(text, keep_entities=True, keep_numbers=False):\n",
    "    \"\"\"\n",
    "    Advanced preprocessing using spaCy's linguistic analysis\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Skip punctuation and spaces\n",
    "        if token.is_punct or token.is_space:\n",
    "            continue\n",
    "            \n",
    "        # Keep named entities if specified\n",
    "        if keep_entities and token.ent_type_:\n",
    "            processed_tokens.append(token.text.lower())\n",
    "            continue\n",
    "            \n",
    "        # Handle numbers based on parameter\n",
    "        if not keep_numbers and (token.like_num or token.pos_ == 'NUM'):\n",
    "            continue\n",
    "            \n",
    "        # Skip stopwords but keep meaningful words\n",
    "        if not token.is_stop and token.is_alpha and len(token.text) > 1:\n",
    "            processed_tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Apple Inc. was founded in 1976 and is worth $3 trillion today.\"\n",
    "print(\"Standard preprocessing:\", intelligent_preprocessing(sample_text))\n",
    "print(\"Keep entities:\", intelligent_preprocessing(sample_text, keep_entities=True))\n",
    "print(\"Keep numbers:\", intelligent_preprocessing(sample_text, keep_numbers=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Fundamental Text Cleaning Techniques\n",
    "\n",
    "### Case Normalization\n",
    "\n",
    "**Purpose:**\n",
    "Converting text to consistent casing prevents the algorithm from treating \"Apple\" and \"apple\" as different tokens, improving feature consistency and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing\"\n",
    "lowercased_text = text.lower()\n",
    "print(lowercased_text)  # Output: natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case Handling Strategies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_case_handling(text, preserve_entities=False):\n",
    "    \"\"\"\n",
    "    Intelligent case handling that can preserve named entities\n",
    "    \"\"\"\n",
    "    if preserve_entities:\n",
    "        # Use spaCy to identify named entities\n",
    "        doc = nlp(text)\n",
    "        result = []\n",
    "        for token in doc:\n",
    "            if token.ent_type_ in ['PERSON', 'ORG', 'GPE']:  # Preserve certain entity types\n",
    "                result.append(token.text)\n",
    "            else:\n",
    "                result.append(token.text.lower())\n",
    "        return ' '.join(result)\n",
    "    else:\n",
    "        return text.lower()\n",
    "\n",
    "# Example\n",
    "text = \"Apple Inc. develops innovative products in California.\"\n",
    "print(\"Standard lowercasing:\", text.lower())\n",
    "print(\"Entity-preserving:\", smart_case_handling(text, preserve_entities=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation Removal\n",
    "\n",
    "**Purpose:**\n",
    "Removing punctuation focuses analysis on meaningful words while eliminating noise. However, context-sensitive removal can preserve important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "text = \"Hello, world! How are you?\"\n",
    "cleaned_text = ''.join([char for char in text if char not in string.punctuation])\n",
    "print(cleaned_text)  # Output: Hello world How are you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context-Aware Punctuation Handling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_punctuation_removal(text, preserve_sentiment=False, preserve_structure=False):\n",
    "    \"\"\"\n",
    "    Remove punctuation with options to preserve meaningful elements\n",
    "    \"\"\"\n",
    "    if preserve_sentiment:\n",
    "        # Keep emotionally significant punctuation\n",
    "        sentiment_punct = {'!', '?', '...'}\n",
    "        cleaned = ''.join([char if char not in string.punctuation or char in sentiment_punct else ' ' for char in text])\n",
    "    elif preserve_structure:\n",
    "        # Keep sentence-ending punctuation\n",
    "        structural_punct = {'.', '!', '?'}\n",
    "        cleaned = ''.join([char if char not in string.punctuation or char in structural_punct else ' ' for char in text])\n",
    "    else:\n",
    "        # Remove all punctuation\n",
    "        cleaned = ''.join([char if char not in string.punctuation else ' ' for char in text])\n",
    "    \n",
    "    # Clean up multiple spaces\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    return cleaned\n",
    "\n",
    "# Examples\n",
    "text = \"Wow! This is amazing... Really? Yes, absolutely!\"\n",
    "print(\"All punctuation removed:\", contextual_punctuation_removal(text))\n",
    "print(\"Sentiment preserved:\", contextual_punctuation_removal(text, preserve_sentiment=True))\n",
    "print(\"Structure preserved:\", contextual_punctuation_removal(text, preserve_structure=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number and Special Character Handling\n",
    "\n",
    "**Purpose:**\n",
    "Numbers often add noise to text analysis, but they can be crucial in certain domains. Strategic handling improves model focus while preserving important information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"There are 123 apples and 45 oranges.\"\n",
    "cleaned_text = re.sub(r'\\d+', '', text)\n",
    "print(cleaned_text)  # Output: There are  apples and  oranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Number Processing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_number_processing(text, strategy='remove'):\n",
    "    \"\"\"\n",
    "    Process numbers in text using different strategies\n",
    "    \n",
    "    Strategies:\n",
    "    - 'remove': Remove all numbers\n",
    "    - 'replace': Replace numbers with placeholder\n",
    "    - 'normalize': Normalize number formats\n",
    "    - 'keep_important': Keep years, percentages, currencies\n",
    "    \"\"\"\n",
    "    \n",
    "    if strategy == 'remove':\n",
    "        return re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    elif strategy == 'replace':\n",
    "        return re.sub(r'\\d+', '<NUM>', text)\n",
    "    \n",
    "    elif strategy == 'normalize':\n",
    "        # Normalize different number formats\n",
    "        text = re.sub(r'\\d{1,3}(,\\d{3})+', lambda m: m.group().replace(',', ''), text)  # Remove commas\n",
    "        text = re.sub(r'\\$\\d+', '<CURRENCY>', text)  # Currency placeholder\n",
    "        text = re.sub(r'\\d+%', '<PERCENTAGE>', text)  # Percentage placeholder\n",
    "        return text\n",
    "    \n",
    "    elif strategy == 'keep_important':\n",
    "        # Keep years (4 digits), percentages, and currencies\n",
    "        important_patterns = [r'\\d{4}', r'\\d+%', r'\\$\\d+']\n",
    "        temp_text = text\n",
    "        placeholders = {}\n",
    "        \n",
    "        # Temporarily replace important numbers\n",
    "        for i, pattern in enumerate(important_patterns):\n",
    "            matches = re.findall(pattern, temp_text)\n",
    "            for j, match in enumerate(matches):\n",
    "                placeholder = f\"__IMPORTANT_{i}_{j}__\"\n",
    "                placeholders[placeholder] = match\n",
    "                temp_text = temp_text.replace(match, placeholder, 1)\n",
    "        \n",
    "        # Remove remaining numbers\n",
    "        temp_text = re.sub(r'\\d+', '', temp_text)\n",
    "        \n",
    "        # Restore important numbers\n",
    "        for placeholder, original in placeholders.items():\n",
    "            temp_text = temp_text.replace(placeholder, original)\n",
    "        \n",
    "        return temp_text\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Examples\n",
    "text = \"In 2023, the company earned $1,250,000 with a 15% profit margin on 500 products.\"\n",
    "print(\"Remove all:\", intelligent_number_processing(text, 'remove'))\n",
    "print(\"Replace with placeholder:\", intelligent_number_processing(text, 'replace'))\n",
    "print(\"Normalize:\", intelligent_number_processing(text, 'normalize'))\n",
    "print(\"Keep important:\", intelligent_number_processing(text, 'keep_important'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Building a Complete Preprocessing Pipeline\n",
    "\n",
    "**Purpose:**\n",
    "Combining all preprocessing techniques into a flexible, configurable pipeline that can be adapted for different NLP tasks and domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing pipeline with configurable options\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lowercase=True, \n",
    "                 remove_punctuation=True, \n",
    "                 remove_stopwords=True, \n",
    "                 remove_numbers=True,\n",
    "                 lemmatize=True, \n",
    "                 min_token_length=2,\n",
    "                 custom_stopwords=None):\n",
    "        \n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.lemmatize = lemmatize\n",
    "        self.min_token_length = min_token_length\n",
    "        \n",
    "        # Load resources\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        if custom_stopwords:\n",
    "            self.stopwords.update(custom_stopwords)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Process a single text string\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return []\n",
    "        \n",
    "        # Process with spaCy\n",
    "        doc = self.nlp(text)\n",
    "        tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Skip punctuation and whitespace\n",
    "            if token.is_punct or token.is_space:\n",
    "                continue\n",
    "            \n",
    "            # Get token text\n",
    "            token_text = token.lemma_ if self.lemmatize else token.text\n",
    "            \n",
    "            # Apply lowercase\n",
    "            if self.lowercase:\n",
    "                token_text = token_text.lower()\n",
    "            \n",
    "            # Check stopwords\n",
    "            if self.remove_stopwords and token_text.lower() in self.stopwords:\n",
    "                continue\n",
    "            \n",
    "            # Check if alphabetic (removes numbers if specified)\n",
    "            if self.remove_numbers and not token.is_alpha:\n",
    "                continue\n",
    "            \n",
    "            # Check minimum length\n",
    "            if len(token_text) < self.min_token_length:\n",
    "                continue\n",
    "            \n",
    "            tokens.append(token_text)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def preprocess_corpus(self, texts):\n",
    "        \"\"\"Process a list of texts\"\"\"\n",
    "        return [self.preprocess_text(text) for text in texts]\n",
    "    \n",
    "    def get_vocabulary(self, texts):\n",
    "        \"\"\"Extract vocabulary from processed texts\"\"\"\n",
    "        all_tokens = []\n",
    "        processed_texts = self.preprocess_corpus(texts)\n",
    "        for tokens in processed_texts:\n",
    "            all_tokens.extend(tokens)\n",
    "        return Counter(all_tokens)\n",
    "\n",
    "# Example usage\n",
    "preprocessor = TextPreprocessor(\n",
    "    lowercase=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_stopwords=True,\n",
    "    remove_numbers=True,\n",
    "    lemmatize=True,\n",
    "    min_token_length=2\n",
    ")\n",
    "\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Natural Language Processing is fascinating and complex!\",\n",
    "    \"In 2023, AI technologies advanced rapidly.\"\n",
    "]\n",
    "\n",
    "processed = preprocessor.preprocess_corpus(sample_texts)\n",
    "vocabulary = preprocessor.get_vocabulary(sample_texts)\n",
    "\n",
    "print(\"Processed texts:\")\n",
    "for i, tokens in enumerate(processed):\n",
    "    print(f\"Text {i+1}: {tokens}\")\n",
    "\n",
    "print(f\"\\nTop 10 most common tokens:\")\n",
    "for token, count in vocabulary.most_common(10):\n",
    "    print(f\"{token}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain-Specific Preprocessing Considerations\n",
    "\n",
    "### Medical Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medical_text_preprocessor(text):\n",
    "    \"\"\"Specialized preprocessing for medical texts\"\"\"\n",
    "    # Preserve medical abbreviations and terminology\n",
    "    medical_stopwords = ['patient', 'medical', 'treatment', 'diagnosis']\n",
    "    \n",
    "    # Normalize medical abbreviations\n",
    "    medical_abbrevs = {\n",
    "        'pts': 'patients',\n",
    "        'dx': 'diagnosis', \n",
    "        'tx': 'treatment',\n",
    "        'hx': 'history'\n",
    "    }\n",
    "    \n",
    "    for abbrev, full_form in medical_abbrevs.items():\n",
    "        text = re.sub(rf'\\b{abbrev}\\b', full_form, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Social Media Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_media_preprocessor(text):\n",
    "    \"\"\"Specialized preprocessing for social media texts\"\"\"\n",
    "    # Handle hashtags, mentions, and URLs\n",
    "    text = re.sub(r'#\\w+', '<HASHTAG>', text)  # Replace hashtags\n",
    "    text = re.sub(r'@\\w+', '<MENTION>', text)  # Replace mentions\n",
    "    text = re.sub(r'http\\S+', '<URL>', text)   # Replace URLs\n",
    "    \n",
    "    # Handle elongated words (e.g., \"sooooo\" -> \"so\")\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    # Handle emoticons and emojis (preserve or replace)\n",
    "    # This would require emoji library for comprehensive handling\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization and Best Practices\n",
    "\n",
    "### Memory-Efficient Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_preprocess(texts, batch_size=1000, preprocessor=None):\n",
    "    \"\"\"Process large text collections in batches to manage memory\"\"\"\n",
    "    if preprocessor is None:\n",
    "        preprocessor = TextPreprocessor()\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        processed_batch = preprocessor.preprocess_corpus(batch)\n",
    "        \n",
    "        # Yield processed batch instead of storing all in memory\n",
    "        for processed_text in processed_batch:\n",
    "            yield processed_text\n",
    "\n",
    "# Example usage for large datasets\n",
    "large_text_collection = [\"Sample text\"] * 10000\n",
    "for processed_text in batch_preprocess(large_text_collection):\n",
    "    # Process one text at a time without loading all into memory\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import functools\n",
    "\n",
    "def parallel_preprocess(texts, n_processes=4):\n",
    "    \"\"\"Preprocess texts using multiple CPU cores\"\"\"\n",
    "    preprocessor = TextPreprocessor()\n",
    "    \n",
    "    # Create partial function with fixed preprocessor\n",
    "    preprocess_func = functools.partial(preprocessor.preprocess_text)\n",
    "    \n",
    "    with Pool(n_processes) as pool:\n",
    "        results = pool.map(preprocess_func, texts)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Best Practices\n",
    "\n",
    "Effective text preprocessing is both an art and a science. The techniques you choose should align with your specific NLP task, domain, and data characteristics. Here are key principles to remember:\n",
    "\n",
    "### Strategic Decision Making:\n",
    "- **Task-driven choices**: Classification tasks might benefit from aggressive preprocessing, while named entity recognition requires preservation of capitalization and punctuation\n",
    "- **Domain awareness**: Medical texts, social media, and legal documents each have unique preprocessing needs\n",
    "- **Data exploration**: Always examine your data before deciding on preprocessing steps\n",
    "\n",
    "### Quality Assurance:\n",
    "- **Before/after comparison**: Always review samples of your preprocessed data\n",
    "- **Iterative refinement**: Adjust preprocessing based on model performance and error analysis\n",
    "- **Validation**: Test preprocessing choices on held-out data to ensure generalization\n",
    "\n",
    "### Performance Considerations:\n",
    "- **Pipeline efficiency**: Use spaCy for comprehensive processing when you need multiple linguistic features\n",
    "- **Memory management**: Process large datasets in batches to avoid memory issues\n",
    "- **Caching**: Store preprocessed data when working with the same dataset multiple times\n",
    "\n",
    "### Common Pitfalls to Avoid:\n",
    "- **Over-preprocessing**: Removing too much information can hurt model performance\n",
    "- **Under-preprocessing**: Insufficient cleaning can introduce noise and inconsistency\n",
    "- **Ignoring domain specifics**: Generic preprocessing may not work for specialized domains\n",
    "- **Forgetting to validate**: Always check that your preprocessing preserves meaningful information\n",
    "\n",
    "The preprocessing pipeline you build becomes the foundation for all downstream NLP tasks. Invest time in getting it right, and your models will perform significantly better.\n",
    "\n",
    "## Further Learning and Resources\n",
    "\n",
    "### Advanced Topics to Explore:\n",
    "1. **Subword tokenization**: BPE, WordPiece, and SentencePiece for handling out-of-vocabulary words\n",
    "2. **Language-specific preprocessing**: Handling non-English languages with different writing systems\n",
    "3. **Named entity preservation**: Advanced techniques for maintaining important entities during preprocessing\n",
    "4. **Custom tokenizers**: Building domain-specific tokenization rules\n",
    "5. **Preprocessing for specific architectures**: How transformer models change preprocessing requirements\n",
    "\n",
    "### Recommended Practice Exercises:\n",
    "1. Build preprocessors for different domains (news, social media, academic papers)\n",
    "2. Compare preprocessing impact on classification accuracy\n",
    "3. Create custom stopword lists for specific domains\n",
    "4. Implement preprocessing for multilingual texts\n",
    "5. Optimize preprocessing pipelines for speed and memory efficiency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
