{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, Stemming, Lemmatization, and POS Tagging: A Comprehensive NLP Guide\n",
    "\n",
    "## Introduction to Core NLP Preprocessing\n",
    "\n",
    "**Purpose:**\n",
    "This tutorial covers the fundamental preprocessing techniques that form the foundation of any Natural Language Processing pipeline. We'll explore tokenization (breaking text into manageable units), stemming and lemmatization (reducing words to their base forms), stopword removal (eliminating common but uninformative words), and part-of-speech tagging (identifying grammatical roles). These techniques are essential for preparing text data for machine learning models, information retrieval, and linguistic analysis.\n",
    "\n",
    "**Why These Techniques Matter:**\n",
    "- **Tokenization**: Converts continuous text into discrete units that algorithms can process\n",
    "- **Normalization**: Reduces vocabulary size and groups related words together\n",
    "- **Noise Reduction**: Removes common words that add little semantic value\n",
    "- **Linguistic Analysis**: Identifies grammatical structure for advanced NLP tasks\n",
    "\n",
    "**Library Comparison Overview:**\n",
    "- **NLTK**: Comprehensive academic toolkit with extensive documentation and educational resources\n",
    "- **spaCy**: Production-ready library optimized for speed and accuracy in real-world applications\n",
    "\n",
    "## Installation and Environment Setup\n",
    "\n",
    "Before beginning, ensure all necessary libraries and resources are properly installed and configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (run this if not already installed)\n",
    "# !pip install nltk spacy matplotlib pandas numpy\n",
    "\n",
    "# Download spaCy language model\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# Import core libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Essential NLTK Downloads:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data (run once)\n",
    "nltk.download('punkt')           # Tokenizer models\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')       # Stopword lists\n",
    "nltk.download('wordnet')         # WordNet database for lemmatization\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagger\n",
    "nltk.download('omw-1.4')        # Open Multilingual Wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing and Understanding the Text Corpus\n",
    "\n",
    "**Purpose:**\n",
    "Working with a representative text sample helps demonstrate how different preprocessing techniques affect real-world content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus with varied linguistic features\n",
    "corpus_original = (\n",
    "    \"The University of Missouri, located in Columbia, Missouri, is the state's largest public research university. \"\n",
    "    \"Missouri Tigers are known for their school spirit and academic excellence. \"\n",
    "    \"The university was founded in 1839 and has been educating students for over 180 years. \"\n",
    "    \"Students are pursuing degrees in engineering, journalism, medicine, and business. \"\n",
    "    \"The campus features beautiful buildings, modern laboratories, and extensive libraries.\"\n",
    ")\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(corpus_original)\n",
    "print(f\"\\nText Statistics:\")\n",
    "print(f\"Length: {len(corpus_original)} characters\")\n",
    "print(f\"Word estimate: {len(corpus_original.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text Normalization Strategy\n",
    "\n",
    "**Purpose:**\n",
    "Standardize text format while preserving important information. Different normalization levels serve different analytical purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text, level='moderate'):\n",
    "    \"\"\"\n",
    "    Normalize text with different intensity levels\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to normalize\n",
    "        level (str): 'light', 'moderate', or 'aggressive'\n",
    "    \n",
    "    Returns:\n",
    "        str: Normalized text\n",
    "    \"\"\"\n",
    "    if level == 'light':\n",
    "        # Minimal processing - preserve most structure\n",
    "        return text.strip()\n",
    "    \n",
    "    elif level == 'moderate':\n",
    "        # Standard preprocessing\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = ' '.join(text.split())  # Normalize whitespace\n",
    "        return text\n",
    "        \n",
    "    elif level == 'aggressive':\n",
    "        # Heavy preprocessing - maximum normalization\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only letters and spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "        return text.strip()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Level must be 'light', 'moderate', or 'aggressive'\")\n",
    "\n",
    "# Demonstrate different normalization levels\n",
    "for level in ['light', 'moderate', 'aggressive']:\n",
    "    normalized = normalize_text(corpus_original, level)\n",
    "    print(f\"\\n{level.title()} Normalization:\")\n",
    "    print(normalized[:100] + \"...\" if len(normalized) > 100 else normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Tokenization Techniques\n",
    "\n",
    "**Purpose:**\n",
    "Tokenization splits text into meaningful units. Different approaches suit different analytical needs and text types.\n",
    "\n",
    "### NLTK Tokenization Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, wordpunct_tokenize\n",
    "\n",
    "def demonstrate_nltk_tokenization(text):\n",
    "    \"\"\"Compare different NLTK tokenization approaches\"\"\"\n",
    "    \n",
    "    tokenizers = {\n",
    "        'Word Tokenize': word_tokenize(text),\n",
    "        'Sentence Tokenize': sent_tokenize(text),\n",
    "        'Word Punct Tokenize': wordpunct_tokenize(text),\n",
    "        'Regex Tokenize (words only)': regexp_tokenize(text, r'\\w+'),\n",
    "        'Regex Tokenize (alphanumeric)': regexp_tokenize(text, r'[A-Za-z0-9]+')\n",
    "    }\n",
    "    \n",
    "    for method, tokens in tokenizers.items():\n",
    "        print(f\"\\n{method}:\")\n",
    "        if method == 'Sentence Tokenize':\n",
    "            for i, sent in enumerate(tokens[:3], 1):  # Show first 3 sentences\n",
    "                print(f\"  {i}. {sent}\")\n",
    "        else:\n",
    "            print(f\"  First 10 tokens: {tokens[:10]}\")\n",
    "            print(f\"  Total tokens: {len(tokens)}\")\n",
    "    \n",
    "    return tokenizers\n",
    "\n",
    "# Demonstrate on our corpus\n",
    "corpus_moderate = normalize_text(corpus_original, 'moderate')\n",
    "nltk_tokens = demonstrate_nltk_tokenization(corpus_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Tokenization and Linguistic Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_spacy_tokenization(text):\n",
    "    \"\"\"Explore spaCy's integrated tokenization and linguistic analysis\"\"\"\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    print(\"spaCy Tokenization with Linguistic Features:\")\n",
    "    print(f\"{'Token':<15} {'POS':<10} {'Lemma':<15} {'Is Alpha':<10} {'Is Stop':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for token in doc[:20]:  # Show first 20 tokens\n",
    "        print(f\"{token.text:<15} {token.pos_:<10} {token.lemma_:<15} \"\n",
    "              f\"{str(token.is_alpha):<10} {str(token.is_stop):<10}\")\n",
    "    \n",
    "    # Token statistics\n",
    "    print(f\"\\nToken Statistics:\")\n",
    "    print(f\"Total tokens: {len(doc)}\")\n",
    "    print(f\"Alphabetic tokens: {sum(1 for token in doc if token.is_alpha)}\")\n",
    "    print(f\"Stop words: {sum(1 for token in doc if token.is_stop)}\")\n",
    "    print(f\"Punctuation: {sum(1 for token in doc if token.is_punct)}\")\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Demonstrate spaCy tokenization\n",
    "spacy_doc = demonstrate_spacy_tokenization(corpus_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Stopword Management\n",
    "\n",
    "**Purpose:**\n",
    "Intelligent stopword removal preserves meaningful content while eliminating noise. Different approaches suit different analytical goals.\n",
    "\n",
    "### Comparing Stopword Lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "def compare_stopword_lists():\n",
    "    \"\"\"Compare NLTK and spaCy stopword lists\"\"\"\n",
    "    \n",
    "    nltk_stops = set(stopwords.words('english'))\n",
    "    spacy_stops = spacy.load('en_core_web_sm').Defaults.stop_words\n",
    "    \n",
    "    print(f\"NLTK stopwords: {len(nltk_stops)}\")\n",
    "    print(f\"spaCy stopwords: {len(spacy_stops)}\")\n",
    "    \n",
    "    # Find differences\n",
    "    only_nltk = nltk_stops - spacy_stops\n",
    "    only_spacy = spacy_stops - nltk_stops\n",
    "    common = nltk_stops & spacy_stops\n",
    "    \n",
    "    print(f\"Common stopwords: {len(common)}\")\n",
    "    print(f\"Only in NLTK: {len(only_nltk)} - Examples: {list(only_nltk)[:10]}\")\n",
    "    print(f\"Only in spaCy: {len(only_spacy)} - Examples: {list(only_spacy)[:10]}\")\n",
    "    \n",
    "    return nltk_stops, spacy_stops\n",
    "\n",
    "nltk_stops, spacy_stops = compare_stopword_lists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Stopword Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_stopword_removal(tokens, method='adaptive', custom_stops=None):\n",
    "    \"\"\"\n",
    "    Advanced stopword removal with multiple strategies\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of tokens\n",
    "        method: 'standard', 'adaptive', 'frequency_based', or 'custom'\n",
    "        custom_stops: Set of custom stopwords to add/use\n",
    "    \n",
    "    Returns:\n",
    "        List of filtered tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'standard':\n",
    "        # Use NLTK standard stopwords\n",
    "        stops = set(stopwords.words('english'))\n",
    "        return [token for token in tokens if token.lower() not in stops]\n",
    "    \n",
    "    elif method == 'adaptive':\n",
    "        # Combine NLTK and spaCy stopwords\n",
    "        stops = set(stopwords.words('english')) | spacy_stops\n",
    "        if custom_stops:\n",
    "            stops.update(custom_stops)\n",
    "        return [token for token in tokens if token.lower() not in stops]\n",
    "    \n",
    "    elif method == 'frequency_based':\n",
    "        # Remove most frequent words (assumed to be stopwords)\n",
    "        token_freq = Counter(tokens)\n",
    "        most_common = {word for word, count in token_freq.most_common(20)}\n",
    "        return [token for token in tokens if token not in most_common]\n",
    "    \n",
    "    elif method == 'custom':\n",
    "        # Use only custom stopwords\n",
    "        if not custom_stops:\n",
    "            return tokens\n",
    "        return [token for token in tokens if token.lower() not in custom_stops]\n",
    "    \n",
    "    else:\n",
    "        return tokens\n",
    "\n",
    "# Demonstrate different stopword removal strategies\n",
    "tokens = word_tokenize(corpus_moderate)\n",
    "domain_stops = {'university', 'missouri', 'student', 'campus'}\n",
    "\n",
    "for method in ['standard', 'adaptive', 'frequency_based', 'custom']:\n",
    "    filtered = intelligent_stopword_removal(tokens, method, domain_stops)\n",
    "    print(f\"\\n{method.title()} stopword removal:\")\n",
    "    print(f\"Original tokens: {len(tokens)}\")\n",
    "    print(f\"After removal: {len(filtered)}\")\n",
    "    print(f\"Removed: {len(tokens) - len(filtered)} tokens\")\n",
    "    print(f\"Sample result: {filtered[:15]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Comprehensive Stemming Analysis\n",
    "\n",
    "**Purpose:**\n",
    "Stemming reduces words to root forms through algorithmic suffix removal. Understanding different stemmers helps choose the right approach for specific tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "\n",
    "def comprehensive_stemming_analysis(tokens):\n",
    "    \"\"\"Compare multiple stemming algorithms\"\"\"\n",
    "    \n",
    "    stemmers = {\n",
    "        'Porter': PorterStemmer(),\n",
    "        'Snowball': SnowballStemmer('english'), \n",
    "        'Lancaster': LancasterStemmer()\n",
    "    }\n",
    "    \n",
    "    # Test words that demonstrate stemmer differences\n",
    "    test_words = ['running', 'ran', 'easily', 'fairly', 'fishing', \n",
    "                 'fished', 'university', 'universities', 'studying', 'studies']\n",
    "    \n",
    "    print(\"Stemming Algorithm Comparison:\")\n",
    "    print(f\"{'Word':<15} {'Porter':<15} {'Snowball':<15} {'Lancaster':<15}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    stemming_results = {}\n",
    "    for word in test_words:\n",
    "        results = {}\n",
    "        for name, stemmer in stemmers.items():\n",
    "            stem = stemmer.stem(word)\n",
    "            results[name] = stem\n",
    "        \n",
    "        stemming_results[word] = results\n",
    "        print(f\"{word:<15} {results['Porter']:<15} {results['Snowball']:<15} {results['Lancaster']:<15}\")\n",
    "    \n",
    "    # Apply to our corpus\n",
    "    print(f\"\\nStemming Corpus Results:\")\n",
    "    corpus_stems = {}\n",
    "    for name, stemmer in stemmers.items():\n",
    "        stems = [stemmer.stem(token) for token in tokens]\n",
    "        corpus_stems[name] = stems\n",
    "        print(f\"{name}: {len(set(stems))} unique stems from {len(set(tokens))} unique tokens\")\n",
    "    \n",
    "    return stemming_results, corpus_stems\n",
    "\n",
    "# Perform comprehensive stemming analysis\n",
    "stemming_results, corpus_stems = comprehensive_stemming_analysis(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_stemming_quality(original_tokens, stemmed_tokens):\n",
    "    \"\"\"Analyze stemming effectiveness and potential issues\"\"\"\n",
    "    \n",
    "    # Count reductions\n",
    "    original_unique = len(set(original_tokens))\n",
    "    stemmed_unique = len(set(stemmed_tokens))\n",
    "    reduction_rate = (original_unique - stemmed_unique) / original_unique * 100\n",
    "    \n",
    "    # Find over-stemming examples (stems that don't look like words)\n",
    "    over_stemmed = []\n",
    "    stem_groups = defaultdict(list)\n",
    "    \n",
    "    for orig, stem in zip(original_tokens, stemmed_tokens):\n",
    "        stem_groups[stem].append(orig)\n",
    "        if len(stem) < 3 or not stem.isalpha():\n",
    "            over_stemmed.append((orig, stem))\n",
    "    \n",
    "    # Find large stem groups (potential over-stemming)\n",
    "    large_groups = {stem: words for stem, words in stem_groups.items() \n",
    "                   if len(set(words)) > 3}\n",
    "    \n",
    "    print(f\"Stemming Quality Assessment:\")\n",
    "    print(f\"Original vocabulary: {original_unique} words\")\n",
    "    print(f\"Stemmed vocabulary: {stemmed_unique} words\")\n",
    "    print(f\"Reduction rate: {reduction_rate:.1f}%\")\n",
    "    print(f\"Potential over-stemming cases: {len(over_stemmed)}\")\n",
    "    \n",
    "    if over_stemmed:\n",
    "        print(\"Over-stemming examples:\", over_stemmed[:5])\n",
    "    \n",
    "    if large_groups:\n",
    "        print(\"Large stem groups (potential over-stemming):\")\n",
    "        for stem, words in list(large_groups.items())[:3]:\n",
    "            unique_words = list(set(words))\n",
    "            print(f\"  '{stem}': {unique_words}\")\n",
    "    \n",
    "    return reduction_rate, stem_groups\n",
    "\n",
    "# Assess Porter stemmer quality\n",
    "porter_stems = corpus_stems['Porter']\n",
    "assess_stemming_quality(tokens, porter_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lemmatization Techniques\n",
    "\n",
    "**Purpose:**\n",
    "Lemmatization provides linguistically accurate word reduction by using vocabulary and morphological analysis, producing valid dictionary words.\n",
    "\n",
    "### NLTK Lemmatization with POS Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(nltk_pos):\n",
    "    \"\"\"Convert NLTK POS tags to WordNet POS tags\"\"\"\n",
    "    if nltk_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "def advanced_lemmatization(tokens):\n",
    "    \"\"\"Perform lemmatization with POS tag awareness\"\"\"\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Get POS tags for tokens\n",
    "    pos_tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Lemmatize with and without POS tags\n",
    "    basic_lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    pos_lemmas = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(pos)) \n",
    "                  for token, pos in pos_tokens]\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"Lemmatization Comparison (Basic vs POS-aware):\")\n",
    "    print(f\"{'Original':<15} {'Basic Lemma':<15} {'POS Lemma':<15} {'POS Tag':<10}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    differences = 0\n",
    "    for i in range(min(20, len(tokens))):  # Show first 20\n",
    "        basic = basic_lemmas[i]\n",
    "        pos_lemma = pos_lemmas[i]\n",
    "        pos_tag_val = pos_tokens[i][1]\n",
    "        \n",
    "        if basic != pos_lemma:\n",
    "            differences += 1\n",
    "            marker = \" *\"\n",
    "        else:\n",
    "            marker = \"\"\n",
    "            \n",
    "        print(f\"{tokens[i]:<15} {basic:<15} {pos_lemma:<15} {pos_tag_val:<10}{marker}\")\n",
    "    \n",
    "    print(f\"\\nDifferences found: {differences}\")\n",
    "    print(f\"Basic lemmatization unique words: {len(set(basic_lemmas))}\")\n",
    "    print(f\"POS-aware lemmatization unique words: {len(set(pos_lemmas))}\")\n",
    "    \n",
    "    return basic_lemmas, pos_lemmas\n",
    "\n",
    "# Perform advanced lemmatization\n",
    "basic_lemmas, pos_lemmas = advanced_lemmatization(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Integrated Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatization_analysis(text):\n",
    "    \"\"\"Analyze spaCy's integrated lemmatization with linguistic features\"\"\"\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Collect lemmatization data\n",
    "    lemma_data = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop:  # Focus on meaningful words\n",
    "            lemma_data.append({\n",
    "                'original': token.text,\n",
    "                'lemma': token.lemma_,\n",
    "                'pos': token.pos_,\n",
    "                'tag': token.tag_,\n",
    "                'changed': token.text.lower() != token.lemma_.lower()\n",
    "            })\n",
    "    \n",
    "    # Analysis\n",
    "    df = pd.DataFrame(lemma_data)\n",
    "    \n",
    "    print(\"spaCy Lemmatization Analysis:\")\n",
    "    print(f\"Total meaningful tokens analyzed: {len(df)}\")\n",
    "    print(f\"Tokens changed by lemmatization: {df['changed'].sum()}\")\n",
    "    print(f\"Change rate: {df['changed'].mean()*100:.1f}%\")\n",
    "    \n",
    "    # Show changes by POS\n",
    "    print(\"\\nChanges by Part of Speech:\")\n",
    "    pos_changes = df.groupby('pos')['changed'].agg(['count', 'sum', 'mean'])\n",
    "    pos_changes['change_rate'] = pos_changes['mean'] * 100\n",
    "    print(pos_changes.round(1))\n",
    "    \n",
    "    # Show specific examples of changes\n",
    "    print(\"\\nLemmatization Changes:\")\n",
    "    changed_examples = df[df['changed']].head(10)\n",
    "    for _, row in changed_examples.iterrows():\n",
    "        print(f\"  {row['original']} → {row['lemma']} ({row['pos']})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze spaCy lemmatization\n",
    "spacy_lemma_df = spacy_lemmatization_analysis(corpus_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Comprehensive Part-of-Speech Tagging\n",
    "\n",
    "**Purpose:**\n",
    "POS tagging identifies grammatical roles of words, enabling syntactic analysis and advanced preprocessing decisions.\n",
    "\n",
    "### Detailed POS Analysis with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_pos_analysis(text):\n",
    "    \"\"\"Comprehensive POS tagging analysis using NLTK\"\"\"\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # POS tag frequency analysis\n",
    "    pos_freq = Counter(tag for word, tag in pos_tags)\n",
    "    \n",
    "    print(\"Part-of-Speech Distribution:\")\n",
    "    print(f\"{'POS Tag':<10} {'Count':<8} {'Percentage':<12} {'Description':<30}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Common POS tag descriptions\n",
    "    pos_descriptions = {\n",
    "        'NN': 'Noun, singular',\n",
    "        'NNS': 'Noun, plural',\n",
    "        'NNP': 'Proper noun, singular',\n",
    "        'NNPS': 'Proper noun, plural',\n",
    "        'VB': 'Verb, base form',\n",
    "        'VBD': 'Verb, past tense',\n",
    "        'VBG': 'Verb, gerund/present participle',\n",
    "        'VBN': 'Verb, past participle',\n",
    "        'VBP': 'Verb, non-3rd person singular present',\n",
    "        'VBZ': 'Verb, 3rd person singular present',\n",
    "        'JJ': 'Adjective',\n",
    "        'JJR': 'Adjective, comparative',\n",
    "        'JJS': 'Adjective, superlative',\n",
    "        'RB': 'Adverb',\n",
    "        'DT': 'Determiner',\n",
    "        'IN': 'Preposition/subordinating conjunction',\n",
    "        'CC': 'Coordinating conjunction',\n",
    "        'PRP': 'Personal pronoun',\n",
    "        'PRP$': 'Possessive pronoun',\n",
    "        'CD': 'Cardinal number',\n",
    "        ',': 'Comma',\n",
    "        '.': 'Sentence-final punctuation'\n",
    "    }\n",
    "    \n",
    "    total_tokens = len(pos_tags)\n",
    "    for tag, count in pos_freq.most_common():\n",
    "        percentage = (count / total_tokens) * 100\n",
    "        description = pos_descriptions.get(tag, 'Other')\n",
    "        print(f\"{tag:<10} {count:<8} {percentage:<12.1f} {description:<30}\")\n",
    "    \n",
    "    return pos_tags, pos_freq\n",
    "\n",
    "# Perform detailed POS analysis\n",
    "pos_tags, pos_freq = detailed_pos_analysis(corpus_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy POS Tagging with Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pos_dependencies(text):\n",
    "    \"\"\"Analyze POS tags and dependency relationships with spaCy\"\"\"\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    print(\"spaCy POS Tags with Dependencies:\")\n",
    "    print(f\"{'Token':<15} {'POS':<10} {'Tag':<10} {'Dep':<15} {'Head':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for token in doc[:25]:  # Show first 25 tokens\n",
    "        print(f\"{token.text:<15} {token.pos_:<10} {token.tag_:<10} \"\n",
    "              f\"{token.dep_:<15} {token.head.text:<15}\")\n",
    "    \n",
    "    # Analyze sentence structure\n",
    "    sentences = list(doc.sents)\n",
    "    print(f\"\\nSentence Analysis:\")\n",
    "    print(f\"Number of sentences: {len(sentences)}\")\n",
    "    \n",
    "    for i, sent in enumerate(sentences[:2], 1):  # Analyze first 2 sentences\n",
    "        print(f\"\\nSentence {i}: {sent.text}\")\n",
    "        \n",
    "        # Find main verb (root)\n",
    "        root = [token for token in sent if token.dep_ == 'ROOT'][0]\n",
    "        print(f\"Main verb (ROOT): {root.text} ({root.pos_})\")\n",
    "        \n",
    "        # Find subjects and objects\n",
    "        subjects = [token for token in sent if 'subj' in token.dep_]\n",
    "        objects = [token for token in sent if 'obj' in token.dep_]\n",
    "        \n",
    "        if subjects:\n",
    "            print(f\"Subjects: {[token.text for token in subjects]}\")\n",
    "        if objects:\n",
    "            print(f\"Objects: {[token.text for token in objects]}\")\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Analyze with spaCy\n",
    "spacy_doc_detailed = spacy_pos_dependencies(corpus_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance and Accuracy Comparison\n",
    "\n",
    "**Purpose:**\n",
    "Compare NLTK and spaCy performance across different preprocessing tasks to guide library selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def performance_comparison(text, iterations=100):\n",
    "    \"\"\"Compare NLTK vs spaCy processing speed and output\"\"\"\n",
    "    \n",
    "    # NLTK setup\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # spaCy setup\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # NLTK timing\n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        tokens = word_tokenize(text)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    nltk_time = time.time() - start_time\n",
    "    \n",
    "    # spaCy timing\n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text for token in doc]\n",
    "        pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "    spacy_time = time.time() - start_time\n",
    "    \n",
    "    print(\"Performance Comparison:\")\n",
    "    print(f\"NLTK processing time: {nltk_time:.3f} seconds\")\n",
    "    print(f\"spaCy processing time: {spacy_time:.3f} seconds\")\n",
    "    print(f\"Speed ratio (NLTK/spaCy): {nltk_time/spacy_time:.2f}x\")\n",
    "    \n",
    "    # Accuracy comparison on sample\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    nltk_lemmas = [lemmatizer.lemmatize(token) for token in nltk_tokens]\n",
    "    \n",
    "    spacy_doc = nlp(text)\n",
    "    spacy_tokens = [token.text for token in spacy_doc]\n",
    "    spacy_lemmas = [token.lemma_ for token in spacy_doc]\n",
    "    \n",
    "    print(f\"\\nOutput Comparison:\")\n",
    "    print(f\"NLTK tokens: {len(nltk_tokens)}\")\n",
    "    print(f\"spaCy tokens: {len(spacy_tokens)}\")\n",
    "    print(f\"Common tokens: {len(set(nltk_tokens) & set(spacy_tokens))}\")\n",
    "    \n",
    "    return nltk_time, spacy_time\n",
    "\n",
    "# Run performance comparison\n",
    "nltk_time, spacy_time = performance_comparison(corpus_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Complete Preprocessing Pipeline\n",
    "\n",
    "**Purpose:**\n",
    "Integrate all preprocessing techniques into a flexible, production-ready pipeline that can be customized for different NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline with configurable options\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 library='spacy',           # 'nltk' or 'spacy'\n",
    "                 tokenization=True,\n",
    "                 lowercase=True,\n",
    "                 remove_punctuation=True,\n",
    "                 remove_stopwords=True,\n",
    "                 custom_stopwords=None,\n",
    "                 stemming=False,\n",
    "                 lemmatization=True,\n",
    "                 pos_tagging=True,\n",
    "                 min_token_length=2,\n",
    "                 max_token_length=50):\n",
    "        \n",
    "        self.library = library\n",
    "        self.tokenization = tokenization\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.custom_stopwords = set(custom_stopwords) if custom_stopwords else set()\n",
    "        self.stemming = stemming\n",
    "        self.lemmatization = lemmatization\n",
    "        self.pos_tagging = pos_tagging\n",
    "        self.min_token_length = min_token_length\n",
    "        self.max_token_length = max_token_length\n",
    "        \n",
    "        # Initialize tools\n",
    "        if library == 'spacy':\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "            self.stopwords = self.nlp.Defaults.stop_words | self.custom_stopwords\n",
    "        else:  # NLTK\n",
    "            self.stemmer = PorterStemmer() if stemming else None\n",
    "            self.lemmatizer = WordNetLemmatizer() if lemmatization else None\n",
    "            self.stopwords = set(stopwords.words('english')) | self.custom_stopwords\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Process text through the complete pipeline\"\"\"\n",
    "        \n",
    "        if self.library == 'spacy':\n",
    "            return self._process_with_spacy(text)\n",
    "        else:\n",
    "            return self._process_with_nltk(text)\n",
    "    \n",
    "    def _process_with_spacy(self, text):\n",
    "        \"\"\"Process using spaCy\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        results = {\n",
    "            'original_text': text,\n",
    "            'tokens': [],\n",
    "            'lemmas': [],\n",
    "            'pos_tags': [],\n",
    "            'processed_tokens': []\n",
    "        }\n",
    "        \n",
    "        for token in doc:\n",
    "            # Basic filtering\n",
    "            if token.is_space:\n",
    "                continue\n",
    "                \n",
    "            if self.remove_punctuation and token.is_punct:\n",
    "                continue\n",
    "            \n",
    "            token_text = token.text.lower() if self.lowercase else token.text\n",
    "            \n",
    "            # Length filtering\n",
    "            if len(token_text) < self.min_token_length or len(token_text) > self.max_token_length:\n",
    "                continue\n",
    "            \n",
    "            # Stopword filtering\n",
    "            if self.remove_stopwords and token_text.lower() in self.stopwords:\n",
    "                continue\n",
    "            \n",
    "            # Store results\n",
    "            results['tokens'].append(token.text)\n",
    "            results['lemmas'].append(token.lemma_)\n",
    "            results['pos_tags'].append((token.text, token.pos_))\n",
    "            \n",
    "            # Final processed token\n",
    "            final_token = token.lemma_ if self.lemmatization else token_text\n",
    "            results['processed_tokens'].append(final_token)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _process_with_nltk(self, text):\n",
    "        \"\"\"Process using NLTK\"\"\"\n",
    "        results = {\n",
    "            'original_text': text,\n",
    "            'tokens': [],\n",
    "            'lemmas': [],\n",
    "            'pos_tags': [],\n",
    "            'processed_tokens': []\n",
    "        }\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text) if self.tokenization else text.split()\n",
    "        \n",
    "        # POS tagging\n",
    "        if self.pos_tagging:\n",
    "            pos_tags = pos_tag(tokens)\n",
    "        else:\n",
    "            pos_tags = [(token, 'UNKNOWN') for token in tokens]\n",
    "        \n",
    "        for token, pos in pos_tags:\n",
    "            # Basic filtering\n",
    "            if self.remove_punctuation and not token.isalnum():\n",
    "                continue\n",
    "            \n",
    "            token_text = token.lower() if self.lowercase else token\n",
    "            \n",
    "            # Length filtering\n",
    "            if len(token_text) < self.min_token_length or len(token_text) > self.max_token_length:\n",
    "                continue\n",
    "            \n",
    "            # Stopword filtering\n",
    "            if self.remove_stopwords and token_text.lower() in self.stopwords:\n",
    "                continue\n",
    "            \n",
    "            # Store results\n",
    "            results['tokens'].append(token)\n",
    "            results['pos_tags'].append((token, pos))\n",
    "            \n",
    "            # Apply morphological processing\n",
    "            processed_token = token_text\n",
    "            if self.stemming and self.stemmer:\n",
    "                processed_token = self.stemmer.stem(processed_token)\n",
    "            elif self.lemmatization and self.lemmatizer:\n",
    "                processed_token = self.lemmatizer.lemmatize(processed_token)\n",
    "            \n",
    "            results['lemmas'].append(processed_token)\n",
    "            results['processed_tokens'].append(processed_token)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_statistics(self, results):\n",
    "        \"\"\"Generate processing statistics\"\"\"\n",
    "        original_words = len(results['original_text'].split())\n",
    "        final_tokens = len(results['processed_tokens'])\n",
    "        \n",
    "        stats = {\n",
    "            'original_word_count': original_words,\n",
    "            'final_token_count': final_tokens,\n",
    "            'reduction_rate': (original_words - final_tokens) / original_words * 100,\n",
    "            'unique_tokens': len(set(results['processed_tokens'])),\n",
    "            'vocabulary_size': len(set(results['processed_tokens']))\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Demonstrate the complete pipeline\n",
    "def demonstrate_complete_pipeline():\n",
    "    \"\"\"Show the complete preprocessing pipeline in action\"\"\"\n",
    "    \n",
    "    # Create different pipeline configurations\n",
    "    configs = {\n",
    "        'Basic spaCy': {'library': 'spacy', 'stemming': False, 'lemmatization': True},\n",
    "        'Basic NLTK': {'library': 'nltk', 'stemming': False, 'lemmatization': True},\n",
    "        'Aggressive spaCy': {'library': 'spacy', 'lemmatization': True, 'custom_stopwords': ['university', 'student']},\n",
    "        'Stemming NLTK': {'library': 'nltk', 'stemming': True, 'lemmatization': False}\n",
    "    }\n",
    "    \n",
    "    print(\"Complete Pipeline Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        print(f\"\\n{name} Configuration:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        processor = ComprehensiveTextPreprocessor(**config)\n",
    "        results = processor.process_text(corpus_original)\n",
    "        stats = processor.get_statistics(results)\n",
    "        \n",
    "        print(f\"Processed tokens: {results['processed_tokens'][:10]}...\")\n",
    "        print(f\"Token count: {stats['final_token_count']}\")\n",
    "        print(f\"Vocabulary size: {stats['vocabulary_size']}\")\n",
    "        print(f\"Reduction rate: {stats['reduction_rate']:.1f}%\")\n",
    "\n",
    "# Run the complete demonstration\n",
    "demonstrate_complete_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing Decision Framework\n",
    "\n",
    "**Purpose:**\n",
    "Guide the selection of appropriate preprocessing techniques based on specific NLP tasks and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_decision_guide():\n",
    "    \"\"\"Provide guidance for choosing preprocessing techniques\"\"\"\n",
    "    \n",
    "    guide = {\n",
    "        'Text Classification': {\n",
    "            'tokenization': 'Essential',\n",
    "            'lowercase': 'Recommended',\n",
    "            'stopwords': 'Usually remove',\n",
    "            'lemmatization': 'Recommended over stemming',\n",
    "            'pos_tagging': 'Optional, for feature engineering'\n",
    "        },\n",
    "        'Named Entity Recognition': {\n",
    "            'tokenization': 'Essential',\n",
    "            'lowercase': 'Avoid (preserves entity capitalization)',\n",
    "            'stopwords': 'Keep (may be part of entities)',\n",
    "            'lemmatization': 'Avoid (may break entity boundaries)',\n",
    "            'pos_tagging': 'Highly beneficial'\n",
    "        },\n",
    "        'Topic Modeling': {\n",
    "            'tokenization': 'Essential',\n",
    "            'lowercase': 'Recommended',\n",
    "            'stopwords': 'Remove',\n",
    "            'lemmatization': 'Highly recommended',\n",
    "            'pos_tagging': 'For noun extraction'\n",
    "        },\n",
    "        'Sentiment Analysis': {\n",
    "            'tokenization': 'Essential',\n",
    "            'lowercase': 'Usually beneficial',\n",
    "            'stopwords': 'Be careful (some stopwords carry sentiment)',\n",
    "            'lemmatization': 'Recommended',\n",
    "            'pos_tagging': 'Beneficial for aspect-based sentiment'\n",
    "        },\n",
    "        'Information Retrieval': {\n",
    "            'tokenization': 'Essential',\n",
    "            'lowercase': 'Recommended',\n",
    "            'stopwords': 'Remove for efficiency',\n",
    "            'lemmatization': 'Recommended (groups related terms)',\n",
    "            'pos_tagging': 'For query expansion'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Preprocessing Decision Guide by NLP Task:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for task, recommendations in guide.items():\n",
    "        print(f\"\\n{task}:\")\n",
    "        print(\"-\" * len(task))\n",
    "        for technique, advice in recommendations.items():\n",
    "            print(f\"  {technique.title()}: {advice}\")\n",
    "    \n",
    "    return guide\n",
    "\n",
    "# Display the decision guide\n",
    "preprocessing_guide = preprocessing_decision_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Common Pitfalls\n",
    "\n",
    "### Performance Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_optimization_tips():\n",
    "    \"\"\"Best practices for efficient text preprocessing\"\"\"\n",
    "    \n",
    "    tips = {\n",
    "        'Batch Processing': [\n",
    "            'Process multiple texts together when possible',\n",
    "            'Use spaCy\\'s nlp.pipe() for large datasets',\n",
    "            'Implement batch processing for NLTK operations'\n",
    "        ],\n",
    "        'Memory Management': [\n",
    "            'Use generators for large text corpora',\n",
    "            'Process texts in chunks to avoid memory overflow',\n",
    "            'Clear unnecessary variables and objects'\n",
    "        ],\n",
    "        'Library Selection': [\n",
    "            'Use spaCy for production systems (speed + accuracy)',\n",
    "            'Use NLTK for research and educational purposes',\n",
    "            'Consider hybrid approaches for specific needs'\n",
    "        ],\n",
    "        'Caching and Persistence': [\n",
    "            'Cache preprocessing results for repeated analysis',\n",
    "            'Serialize preprocessed data to avoid recomputation',\n",
    "            'Use memory mapping for very large datasets'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"Performance Optimization Best Practices:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for category, practices in tips.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for tip in practices:\n",
    "            print(f\"  • {tip}\")\n",
    "\n",
    "performance_optimization_tips()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Pitfalls to Avoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_pitfalls():\n",
    "    \"\"\"Identify and explain common preprocessing mistakes\"\"\"\n",
    "    \n",
    "    pitfalls = {\n",
    "        'Over-preprocessing': [\n",
    "            'Removing too much information (e.g., all numbers in financial texts)',\n",
    "            'Aggressive stemming that loses semantic meaning',\n",
    "            'Removing stopwords that are important for the task'\n",
    "        ],\n",
    "        'Under-preprocessing': [\n",
    "            'Not handling case sensitivity appropriately',\n",
    "            'Ignoring punctuation when it carries meaning',\n",
    "            'Failing to normalize different forms of the same word'\n",
    "        ],\n",
    "        'Tool Misuse': [\n",
    "            'Using stemming when lemmatization is more appropriate',\n",
    "            'Not considering POS tags for better lemmatization',\n",
    "            'Ignoring language-specific preprocessing needs'\n",
    "        ],\n",
    "        'Evaluation Issues': [\n",
    "            'Not evaluating preprocessing impact on downstream tasks',\n",
    "            'Applying same preprocessing to all domains',\n",
    "            'Not preserving some data for preprocessing comparison'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"Common Preprocessing Pitfalls to Avoid:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for category, issues in pitfalls.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  ⚠ {issue}\")\n",
    "\n",
    "common_pitfalls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Comparison Table\n",
    "\n",
    "| Technique | NLTK | spaCy | When to Use | Best For |\n",
    "|-----------|------|-------|-------------|----------|\n",
    "| **Tokenization** | `word_tokenize()` | `nlp(text)` | Always | All NLP tasks |\n",
    "| **Stopword Removal** | `stopwords.words()` | `token.is_stop` | Text classification, IR | Reducing noise |\n",
    "| **Stemming** | `PorterStemmer()` | Not available | High-speed processing | Information retrieval |\n",
    "| **Lemmatization** | `WordNetLemmatizer()` | `token.lemma_` | Quality over speed | Most NLP tasks |\n",
    "| **POS Tagging** | `pos_tag()` | `token.pos_` | Syntax-aware processing | NER, parsing |\n",
    "\n",
    "## Conclusion and Next Steps\n",
    "\n",
    "This comprehensive tutorial has covered the fundamental preprocessing techniques that form the backbone of NLP pipelines. Key takeaways include:\n",
    "\n",
    "### **Strategic Insights:**\n",
    "- **Library choice matters**: spaCy for production, NLTK for research and learning\n",
    "- **Task-driven decisions**: Different NLP applications require different preprocessing approaches\n",
    "- **Quality vs. speed tradeoffs**: Lemmatization is more accurate than stemming but slower\n",
    "- **Pipeline thinking**: Integrate multiple techniques for optimal results\n",
    "\n",
    "### **Practical Recommendations:**\n",
    "1. **Start simple**: Begin with basic tokenization and gradually add complexity\n",
    "2. **Measure impact**: Evaluate how each preprocessing step affects your downstream task\n",
    "3. **Domain adaptation**: Customize preprocessing for your specific domain and data characteristics\n",
    "4. **Performance monitoring**: Track processing time and memory usage for large datasets\n",
    "\n",
    "### **Advanced Topics to Explore:**\n",
    "- **Subword tokenization**: BPE, WordPiece for handling out-of-vocabulary words\n",
    "- **Language-specific preprocessing**: Handling non-English languages and special scripts\n",
    "- **Neural preprocessing**: Using transformer models for context-aware tokenization\n",
    "- **Custom preprocessing**: Building domain-specific preprocessing pipelines\n",
    "\n",
    "### **Further Learning Resources:**\n",
    "- Explore NLTK's extensive corpus collection for practicing on different text types\n",
    "- Investigate spaCy's advanced features like custom pipeline components\n",
    "- Practice with real-world datasets from your domain of interest\n",
    "- Experiment with preprocessing for different NLP tasks to understand the impact"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tokenization_Stemming_lemmatization_stopword_postagging.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
