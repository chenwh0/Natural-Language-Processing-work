{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsZspRI6j92d"
   },
   "source": [
    "# Extracting Text from Images with Python and Pytesseract: Enhanced Tutorial\n",
    "\n",
    "## Introduction to OCR with Pytesseract\n",
    "\n",
    "**Purpose:**\n",
    "Optical Character Recognition (OCR) converts images of text into machine-readable strings. Pytesseract is a Python wrapper for Google’s Tesseract-OCR engine, enabling seamless integration with Python image-processing libraries. This tutorial covers installation, fundamental workflows, advanced preprocessing, configuration, and best practices to maximize OCR accuracy.\n",
    "\n",
    "**Why OCR Matters:**\n",
    "- **Digitization**: Transform scanned documents, PDFs, and photographs into searchable text\n",
    "- **Data Extraction**: Automate data entry from invoices, forms, and reports\n",
    "- **Accessibility**: Convert visual text to speech or Braille for visually impaired users\n",
    "- **Analytics**: Enable text analytics on image-based content (e.g., signage, social media images)\n",
    "\n",
    "**Common OCR Challenges:**\n",
    "- Poor image quality (low resolution, noise)\n",
    "- Complex layouts (multiple text blocks, tables)\n",
    "- Non-uniform lighting and skewed text\n",
    "- Varied fonts, sizes, and languages\n",
    "\n",
    "## 1. Installation and Setup\n",
    "\n",
    "### Tesseract Engine\n",
    "\n",
    "**Windows:** Download the MSI installer from the UB Mannheim repository and note the install path (e.g., `C:\\Program Files\\Tesseract-OCR`). Add it to your `PATH` or configure `pytesseract.pytesseract.tesseract_cmd`.\n",
    "\n",
    "**Linux/macOS:**\n",
    "```bash\n",
    "sudo apt install tesseract-ocr      # Debian/Ubuntu\n",
    "brew install tesseract             # macOS with Homebrew\n",
    "```\n",
    "\n",
    "### Python Dependencies\n",
    "\n",
    "```bash\n",
    "pip install pytesseract pillow opencv-python matplotlib\n",
    "```\n",
    "\n",
    "**Verification:**\n",
    "```python\n",
    "import pytesseract\n",
    "print(pytesseract.get_tesseract_version())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic OCR Workflow\n",
    "\n",
    "**Purpose:**\n",
    "Quickly extract text from a clean, high-resolution image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luHyVkJm2Mnm",
    "outputId": "086e1bed-3982-48f9-c573-b29c502f4446"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pytesseract\n",
    "\n",
    "#download image\n",
    "!wget \"https://muidsi.missouri.edu/wp-content/uploads/2023/11/F_Edu_Best-Masters-Data-Science_top10-US_2023.png\" -O sample_document.png\n",
    "# Load an image from file\n",
    "image = Image.open('sample_document.png')\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text using pytesseract\n",
    "text = pytesseract.image_to_string(image)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJRBNjMf2Ju0"
   },
   "source": [
    "**Notes:**\n",
    "- Defaults use LSTM engine (`--oem 3`) and automatic page segmentation (`--psm 3`).\n",
    "- Accuracy heavily depends on image quality and preprocessing.\n",
    "\n",
    "## 3. Advanced Image Preprocessing\n",
    "\n",
    "**Purpose:**\n",
    "Enhance image quality to reduce noise and improve Tesseract’s recognition accuracy.\n",
    "\n",
    "| Step                | Description                                      | OpenCV Example                                           |\n",
    "|---------------------|--------------------------------------------------|----------------------------------------------------------|\n",
    "| Grayscale           | Simplifies image, reduces color noise            | `cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)`                 |\n",
    "| Resize              | Upscale small text regions                       | `cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)` |\n",
    "| Noise Removal       | Smooth out artifacts                              | `cv2.medianBlur(gray, 5)`                               |\n",
    "| Contrast Enhancement| Improve text-background contrast                  | `clahe = cv2.createCLAHE(clipLimit=2); clahe.apply(gray)`|\n",
    "| Thresholding        | Binarize image for clear text regions             | `cv2.threshold(gray,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)`|\n",
    "| Adaptive Threshold  | Local threshold for uneven lighting               | `cv2.adaptiveThreshold`                                  |\n",
    "| Deskew              | Correct tilted text                              | Custom deskew function (below)                          |\n",
    "| Morphology          | Dilation/erosion to connect/disconnect components| `cv2.dilate` / `cv2.erode`                               |\n",
    "\n",
    "### Deskewing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmNeJkC_2yVF",
    "outputId": "9ba949c7-f58b-48c3-a05a-6a2e7208141d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def deskew(image):\n",
    "    coords = np.column_stack(np.where(image > 0))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a7bkfFc_2Ju1",
    "outputId": "def32add-4f42-4dd8-b6d9-973b88e7273f"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_image(\n",
    "    image_path,\n",
    "    grayscale=True,\n",
    "    resize=True,\n",
    "    blur=True,\n",
    "    threshold=False,  # Turn off by default\n",
    "    adaptive_threshold=False,\n",
    "    clahe=True,\n",
    "    resize_fx=2,\n",
    "    resize_fy=2,\n",
    "    blur_ksize=(5, 5)\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the image with optional steps.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if grayscale:\n",
    "        # Convert to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    if resize:\n",
    "        # Resize image to enlarge text\n",
    "        img = cv2.resize(img, None, fx=resize_fx, fy=resize_fy,\n",
    "                         interpolation=cv2.INTER_CUBIC)\n",
    "    if blur:\n",
    "        # Apply Gaussian blur to reduce noise\n",
    "        img = cv2.GaussianBlur(img, blur_ksize, 0)\n",
    "    if clahe:\n",
    "        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "        clahe_obj = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        img = clahe_obj.apply(img)\n",
    "    if threshold:\n",
    "        # Apply global thresholding (Otsu's method)\n",
    "        _, img = cv2.threshold(\n",
    "            img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    if adaptive_threshold:\n",
    "        # Apply adaptive thresholding\n",
    "        img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                    cv2.THRESH_BINARY, 31, 10)\n",
    "    return img\n",
    "\n",
    "# Load an image from file using PIL for display\n",
    "image = Image.open('sample_course_path.png')\n",
    "# Preprocess the image using the defined function\n",
    "preprocessed_img = preprocess_image('sample_course_path.png')\n",
    "\n",
    "# Display original and preprocessed images side by side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title('Original Image')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(preprocessed_img, cmap='gray')\n",
    "ax[1].set_title('Preprocessed Image')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extract text from the original image using pytesseract\n",
    "text = pytesseract.image_to_string(image)\n",
    "print(text)\n",
    "\n",
    "# Extract text from the preprocessed image with custom config\n",
    "text = pytesseract.image_to_string(preprocessed_img)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhnufzsw2Ju1"
   },
   "source": [
    "\n",
    "## 4. Custom OCR Configuration\n",
    "\n",
    "**Purpose:**\n",
    "Fine-tune Tesseract parameters for your image’s layout and content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxexiH3j2Ju1",
    "outputId": "fd2ab34f-e4d3-40ee-c57d-78243224b08f"
   },
   "outputs": [],
   "source": [
    "# Use custom OCR configuration: --oem 3 (LSTM OCR Engine), --psm 6 (Assume a single uniform block of text)\n",
    "config = '--oem 3 --psm 6'\n",
    "text = pytesseract.image_to_string(image, config=config)\n",
    "print(text)\n",
    "\n",
    "# Note: This configuration works best for images with a single block of text.\n",
    "# Our sample image may contain complex layouts or graphics, so results may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Flags:**\n",
    "- `--oem [0–3]`: OCR engine mode (legacy, LSTM, combined)\n",
    "- `--psm [0–13]`: Page segmentation mode (single line, block, sparse text)\n",
    "- `-c tessedit_char_whitelist=ABC...`: Restrict character set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2p5NOIn2Ju1"
   },
   "source": [
    "## 5. Multilingual OCR\n",
    "\n",
    "**Purpose:**\n",
    "Recognize text in multiple languages or scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fcbeWRw2Ju2",
    "outputId": "d87146e7-0686-41d2-8f1f-b355ad597cc6"
   },
   "outputs": [],
   "source": [
    "# English + French\n",
    "text = pytesseract.image_to_string(preprocessed_img, lang='eng+fra')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzfKBztW7RUP"
   },
   "source": [
    "## 6. Postprocessing and Error Correction\n",
    "\n",
    "**Purpose:**\n",
    "Clean and correct OCR output using heuristics or NLP techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pytesseract\n",
    "\n",
    "def clean_ocr(text):\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Fix common OCR misrecognitions\n",
    "    corrections = {\n",
    "        '0': 'O',\n",
    "        '1': 'I',\n",
    "        '5': 'S'\n",
    "    }\n",
    "    for k, v in corrections.items():\n",
    "        text = text.replace(k, v)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example use — assuming `pre` is an image processed for OCR\n",
    "raw = pytesseract.image_to_string(preprocessed_img)\n",
    "print(clean_ocr(raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Alternative OCR Libraries\n",
    "\n",
    "- **EasyOCR**: Multilingual, supports 80+ languages, handwriting\n",
    "- **OCRmyPDF**: OCR PDFs and embed text layer\n",
    "- **Google Cloud Vision**: High accuracy, cloud-based\n",
    "\n",
    "## 8. Best Practices and Troubleshooting\n",
    "\n",
    "- **High DPI images** (>300 DPI) for clarity\n",
    "- **Consistent lighting and contrast** to reduce noise\n",
    "- **Avoid overly complex layouts** or split into separate images\n",
    "- **Experiment with preprocessing combinations** (resize + threshold + deskew)\n",
    "- **Use ROI cropping** to focus OCR on relevant areas\n",
    "- **Profile performance** on sample images before batch processing\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Optimizing OCR involves a combination of image preprocessing, Tesseract configuration, and postprocessing. This tutorial equips you to build robust OCR pipelines, from basic extraction to advanced, domain-specific solutions. Experiment, measure accuracy, and iterate to achieve the best results for your use case."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
