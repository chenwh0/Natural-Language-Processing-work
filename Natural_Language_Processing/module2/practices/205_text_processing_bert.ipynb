{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5792cb96",
   "metadata": {
    "id": "5792cb96"
   },
   "source": [
    "# BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "For Practice Notebook 205, please download the notebook and run it on [Google Colab](https://colab.research.google.com/), as it requires GPU access. As of August 28, 2025, our GPU infrastructure is still under development. We will notify you once GPU-enabled containers become available.\n",
    "\n",
    "</div>\n",
    "\n",
    "We began with traditional word embeddings like Word2Vec (CBOW/Skip-gram). Now, let's examine **BERT**—Google's transformer-based model that has fundamentally changed how we represent text by generating **contextualized embeddings**. Unlike static embeddings, which assign a single vector to each word, BERT creates a unique representation for each word based on its context within a sentence.\n",
    "\n",
    "## Comparing Word2Vec and BERT\n",
    "\n",
    "| Feature | Word2Vec/GloVe | **BERT** |\n",
    "| :-- | :-- | :-- |\n",
    "| **Representation** | Static: one vector per word | Contextual: vector changes with context |\n",
    "| **Context Handling** | Ignores context | Fully bidirectional (uses all context) |\n",
    "| **Architecture** | Shallow neural network | Deep transformer with self-attention |\n",
    "| **Tokenization** | Word-level | Subword-level (WordPiece) |\n",
    "| **Out-of-Vocabulary** | Cannot handle unseen words well | Can compose subwords for unseen words |\n",
    "| **Computation** | Fast, lightweight | Computationally intensive |\n",
    "| **Typical Use Cases** | Quick similarity, basic tasks | Complex tasks: NER, QA, sentiment, etc. |\n",
    "| **Example** | \"bank\" always same vector | \"bank\" in \"river bank\" vs \"bank account\" |\n",
    "\n",
    "### What does this mean in practice?\n",
    "\n",
    "- **Word2Vec**: The word \"Missouri\" always has the same vector, whether it appears in \"University of Missouri\" or \"Missouri River.\"\n",
    "- **BERT**: The vector for \"Missouri\" changes depending on whether it's used in an academic, geographic, or sports context.\n",
    "\n",
    "\n",
    "## How BERT Works (Key Implementation Details)\n",
    "\n",
    "1. **Tokenization**\n",
    "    - BERT uses WordPiece tokenization, splitting rare words into subwords (e.g., \"Columbia\" → `columb`, `##ia` if not in vocabulary).\n",
    "2. **Special Tokens**\n",
    "    - `[CLS]` at the start (used for classification tasks)\n",
    "    - `[SEP]` between sentences\n",
    "3. **Output**\n",
    "    - Produces a 768-dimensional vector for each token (in `bert-base-uncased`)\n",
    "    - Utilizes 12 transformer layers, each providing contextual representations\n",
    "\n",
    "## Why This Matters for Our Missouri-Focused NLP Work\n",
    "\n",
    "When we analyze a sentence like:\n",
    "\n",
    "> \"The University of Missouri-Columbia researchers studied Missouri river ecosystems.\"\n",
    "\n",
    "- **Word2Vec** gives the same embedding for \"Missouri\" in both \"University of Missouri\" and \"Missouri river.\"\n",
    "- **BERT** generates different embeddings:\n",
    "    - \"Missouri\" in \"University of Missouri\" (academic context)\n",
    "    - \"Missouri\" in \"Missouri river\" (geographical context)\n",
    "    - \"Columbia\" in \"Missouri-Columbia\" (campus) vs. \"Columbia\" as a city\n",
    "\n",
    "This **contextual awareness** enables us to:\n",
    "\n",
    "- Distinguish between the university and the state in entity recognition\n",
    "- Analyze sentiment about the University of Missouri-Columbia versus the state as a whole\n",
    "- Classify regional topics with greater accuracy\n",
    "\n",
    "\n",
    "## Practical Considerations\n",
    "\n",
    "- **Word2Vec** is fast and efficient, suitable for simple tasks or when computational resources are limited.\n",
    "- **BERT** is more powerful for nuanced understanding but requires more computation and is best for tasks where context matters (e.g., distinguishing between \"Columbia\" as a city vs. a university campus).\n",
    "\n",
    "We will implement these concepts in our next notebook section, using Missouri-centric examples to show how BERT's contextual embeddings provide richer, more accurate text representations than traditional static embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126372f6",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "## 1. Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tYjrwwbFaKWW",
   "metadata": {
    "id": "tYjrwwbFaKWW"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers  # Only run if needed\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddc388c",
   "metadata": {
    "id": "eddc388c"
   },
   "source": [
    "**Why we do this:**\n",
    "- We use `bert-base-uncased` because it's a standard starting point (case-insensitive, general vocabulary)\n",
    "- The tokenizer handles WordPiece tokenization automatically\n",
    "- The model provides 12 transformer layers with 768-dimensional outputs\n",
    "\n",
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868a0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    # Compute cosine similarity between two vectors a and b\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_bert_for_token(string, term):\n",
    "    # Tokenize the input string using the BERT tokenizer\n",
    "    inputs = tokenizer(string, return_tensors=\"pt\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    try:\n",
    "        # Find the index of the target token (term) in the tokenized list\n",
    "        term_idx = tokens.index(term)\n",
    "    except ValueError:\n",
    "        # Raise an error if the token is not found in the sentence\n",
    "        raise ValueError(f\"Token '{term}' not found in: {tokens}\")\n",
    "    # Pass the tokenized input through the BERT model\n",
    "    outputs = model(**inputs)\n",
    "    # Extract and return the embedding for the specified token\n",
    "    return outputs.last_hidden_state[0][term_idx].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d06a576",
   "metadata": {},
   "source": [
    "**Key design choices:**\n",
    "1. **Cosine similarity**: Measures angular distance between vectors (better than Euclidean for embeddings)\n",
    "2. **Token-matching approach**: \n",
    "   - Forces exact token match (BERT tokenizes \"Columbia\" as `columbia` but \"Columbia\" might be `columb` + `##ia`)\n",
    "   - Uses first occurrence only (simplifies demonstration)\n",
    "3. **Error handling**: Skips sentences where term isn't found exactly\n",
    "\n",
    "## 3. Missouri-Focused Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53db2598",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"The University of Missouri-Columbia is located in Missouri.\"\n",
    "comp_sents = [\"Missouri is known for its beautiful rivers and forests.\",         # State context\n",
    "              \"I attended the University of Missouri-Columbia...\",               # Academic context\n",
    "              \"The Missouri Tigers are... University of Missouri.\",              # Sports context\n",
    "              \"Columbia is a vibrant college town in Missouri.\",                 # Geographic context\n",
    "              \"The University of Missouri-Columbia has... journalism program.\",  # Program context\n",
    "              \"Kansas City is... largest cities in Missouri.\"                    # Urban context\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e6c9ab",
   "metadata": {},
   "source": [
    "We deliberately choose sentences where \"Missouri\" refers to different things: the state, the university, sports teams, and geographic locations. This allows us to observe how BERT adapts the meaning of \"Missouri\" based on its context.\n",
    "\n",
    "## 4. Contextual Comparison\n",
    "\n",
    "\n",
    "We use a helper function to extract the embedding for the word \"Missouri\" from each sentence using BERT. For each sentence, we:\n",
    "\n",
    "- Tokenize the sentence (using WordPiece tokenization)\n",
    "- Find the position(s) of the token \"missouri\"\n",
    "- Extract the corresponding vector from BERT's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a43a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940\tThe University of Missouri-Columbia is located in Missouri.\tThe University of Missouri-Columbia has... journalism program.\n",
      "0.902\tThe University of Missouri-Columbia is located in Missouri.\tI attended the University of Missouri-Columbia...\n",
      "0.784\tThe University of Missouri-Columbia is located in Missouri.\tColumbia is a vibrant college town in Missouri.\n",
      "0.757\tThe University of Missouri-Columbia is located in Missouri.\tThe Missouri Tigers are... University of Missouri.\n",
      "0.698\tThe University of Missouri-Columbia is located in Missouri.\tKansas City is... largest cities in Missouri.\n",
      "0.553\tThe University of Missouri-Columbia is located in Missouri.\tMissouri is known for its beautiful rivers and forests.\n"
     ]
    }
   ],
   "source": [
    "# Get the BERT embedding for the token \"missouri\" in the query sentence\n",
    "query_rep = get_bert_for_token(query, \"missouri\")\n",
    "\n",
    "vals = []\n",
    "# For each comparison sentence, compute the cosine similarity of \"missouri\" embeddings\n",
    "for sent in comp_sents:\n",
    "    try:\n",
    "        # Get the BERT embedding for \"missouri\" in the current sentence\n",
    "        comp_rep = get_bert_for_token(sent, \"missouri\")\n",
    "        # Compute cosine similarity between query and comparison embedding\n",
    "        cos_sim = cosine_similarity(query_rep, comp_rep)\n",
    "        # Store the similarity and sentences for later sorting/printing\n",
    "        vals.append((cos_sim, query, sent))\n",
    "    except ValueError:\n",
    "        # Skip sentences where \"missouri\" token is not found\n",
    "        continue\n",
    "\n",
    "# Sort results by similarity (highest first), then print them\n",
    "for c, q, s in reversed(sorted(vals)):\n",
    "    print(f\"{c:.3f}\\t{q}\\t{s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc68a9",
   "metadata": {},
   "source": [
    "**How to interpret these results:**\n",
    "\n",
    "- **Highest similarity (0.940, 0.902):**\n",
    "Sentences where \"Missouri\" is used in an academic/university context are most similar to the query. BERT recognizes that \"Missouri\" in \"University of Missouri-Columbia\" is closely related to \"Missouri\" in other university-related sentences.\n",
    "- **Moderate similarity (0.784, 0.757):**\n",
    "Sentences referencing \"Missouri\" in a geographic or institutional context (like \"Columbia\" as a town or \"Missouri Tigers\" as sports teams) are somewhat similar, but less so than the academic context.\n",
    "- **Lowest similarity (0.698, 0.553):**\n",
    "Sentences where \"Missouri\" clearly refers to the state as a geographic or demographic entity (rivers, cities) are least similar to the university context. BERT captures this distinction, assigning a more different embedding.\n",
    "\n",
    "\n",
    "## Why BERT is Better Than Word2Vec Here\n",
    "\n",
    "- **Word2Vec** would give the *same* vector for \"Missouri\" in all these sentences, so cosine similarity would always be high, regardless of context.\n",
    "- **BERT** adapts the embedding of \"Missouri\" to each context, allowing us to distinguish when \"Missouri\" refers to the university, the state, or something else.\n",
    "\n",
    "\n",
    "## Key Takeaways for Students\n",
    "\n",
    "- **Contextual embeddings** let us distinguish between different meanings of the same word, which is crucial for tasks like entity recognition, sentiment analysis, and topic classification.\n",
    "- BERT's approach is especially valuable for real-world text, where words often have multiple meanings depending on context—just like \"Missouri\" in our university and state examples.\n",
    "\n",
    "**Next Steps:**\n",
    "We encourage you to try this approach with other tokens, such as \"Columbia\" or \"University,\" and see how their meanings shift across different sentences. You can also extend this analysis to full sentence embeddings or experiment with visualizing similarities using dimensionality reduction techniques."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
