{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WMfms9RqCv9"
   },
   "source": [
    "# Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0KhlTUN0loL"
   },
   "source": [
    "## Types of Datasets\n",
    "   - **Numerical Dataset:** Contains measurable quantities and can be analyzed mathematically. Examples include temperature, humidity, and test scores.\n",
    "   - **Categorical Dataset:** Comprises a set of categories or groups. Examples are colors, product categories, and yes/no responses.\n",
    "   - **Time Series Dataset:** Captures data points at successive time intervals. Useful for analyzing trends over time.\n",
    "   - **Image Dataset:** Consists of image files, often used in computer vision tasks to identify patterns or objects.\n",
    "   - **Text Dataset:** Includes collections of words, sentences, or documents, typically analyzed for linguistic patterns or content.\n",
    "\n",
    "   and many more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGv7gQunOqRW"
   },
   "source": [
    "## Text Data\n",
    "\n",
    "1. **Understanding Text Data:**\n",
    "   - Text data is composed of sequences of characters, forming words, sentences, or paragraphs.\n",
    "   - It varies in length and complexity, often containing nuanced linguistic features.\n",
    "\n",
    "2. **Text vs. Categorical Data:**\n",
    "   - Strings of characters can represent different types of data.\n",
    "   - Categorical data is derived from a predefined set of options, such as 'red' or 'blue', 'yes' or 'no'.\n",
    "   - Text data, however, is more fluid, often forming meaningful phrases or sentences that convey complex ideas.\n",
    "\n",
    "3. **Analyzing Text: Corpus and Documents:**\n",
    "   - Text analysis typically involves examining a large body of text, known as a [corpus](https://en.wikipedia.org/wiki/Text_corpus).\n",
    "   - Within this corpus, each individual text entry, whether an article, social media post, or review, is termed a **document**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q74ClDrxPQk1"
   },
   "source": [
    "## What is bag-of-words?\n",
    "\n",
    "Bag-of-words is a technique in natural language processing where we ignore the structure of input text and focus solely on word occurrences. Itâ€™s like mentally holding a bag of words and counting how many times each word appears in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EOXWuoQPz2y"
   },
   "source": [
    "## Steps for bag-of-words representation\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - **Tokenization** is the process of splitting each document (text) into individual words or tokens.\n",
    "   - We achieve this by breaking the text at whitespace, punctuation marks, or other delimiters.\n",
    "\n",
    "2. **Vocabulary Building**:\n",
    "   - Next, we create a **vocabulary** containing all unique words (tokens) that appear in any of the documents.\n",
    "   - Each word is assigned a unique **index** (usually in alphabetical order).\n",
    "\n",
    "3. **Encoding**:\n",
    "   - For each document, we count how often each word from the vocabulary appears in that document.\n",
    "   - The resulting vector represents the word frequencies (counts) for that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1q3i1R-7q36"
   },
   "source": [
    "## Implementing Bag-of-Words\n",
    "\n",
    "In the scikit-learn library, the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is utilized to transform text data into a bag-of-words representation.\n",
    "\n",
    "The `CountVectorizer` method standardizes all text data to lowercase, ensuring that words with identical spellings are recognized as the same token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwaO-Qz2v1K_"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font> Let's create a simple example of the Bag-of-Words (BoW) model using text related to Calgary. We'll follow the steps mentioned earlier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lfKx3Ih0zK5"
   },
   "source": [
    "Each Tokenization can be done as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVgOiI_H0OVN"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Step 0: Collect Data\n",
    "# Define the documents\n",
    "docs = [\"Columbia, Missouri is known for its vibrant college town atmosphere.\",\n",
    "        \"The University of Missouri in Columbia is a major research institution.\",\n",
    "        \"Columbia's weather can be unpredictable, especially in spring.\"\n",
    "        ]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "tokenizer = vectorizer.build_analyzer()\n",
    "for (i, doc) in enumerate(docs, 1):\n",
    "    print(f'\\n\\033[1m\\033[34mDoc {i}:\\033[0m')  # Bold and blue\n",
    "    print(\"Original document:\")\n",
    "    print(doc)\n",
    "    print(\"Tokenized document:\")\n",
    "    print(tokenizer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0NUjcLFv2jx"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Step 0: Collect Data\n",
    "# Define the documents\n",
    "docs = [\"Columbia, Missouri is known for its vibrant college town atmosphere.\",\n",
    "        \"The University of Missouri in Columbia is a major research institution.\",\n",
    "        \"Columbia's weather can be unpredictable, especially in spring.\"\n",
    "        ]\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "# Transform the documents into a bag-of-words matrix\n",
    "bag_of_words = vectorizer.transform(docs)\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print('Feature Names (Vocabulary): ' + ', '.join(feature_names))\n",
    "\n",
    "# Display the vocabulary size and content\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"Vocabulary content: {vectorizer.vocabulary_}\")\n",
    "\n",
    "# Display the dense representation of the bag_of_words\n",
    "# print(\"Dense representation of bag_of_words:\\n{}\".format(bag_of_words.toarray()))\n",
    "\n",
    "# Create a DataFrame from the BoW matrix\n",
    "df_bow = pd.DataFrame(bag_of_words.toarray(), columns=feature_names)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Example with Repeating Words\n",
    "\n",
    "- **Sentences used:**\n",
    "  - \"Columbia, Columbia, a city so vibrant, so vibrant.\"\n",
    "  - \"The Missouri River, the Missouri River, so scenic, so scenic.\"\n",
    "\n",
    "- **How it works:**\n",
    "  - The CountVectorizer tokenizes the text and counts how many times each word appears in each sentence.\n",
    "  - The result is a matrix (shown below as a table) where each row is a sentence and each column is a word from the combined vocabulary.\n",
    "\n",
    "### Vocabulary Learned\n",
    "\n",
    "| city | columbia | missouri | river | scenic | so | the | vibrant |\n",
    "|------|----------|----------|-------|--------|----|-----|---------|\n",
    "|  0   |    1     |    2     |   3   |   4    | 5  |  6  |    7    |\n",
    "\n",
    "### Bag-of-Words Matrix\n",
    "\n",
    "|        | city | columbia | missouri | river | scenic | so | the | vibrant |\n",
    "|--------|------|----------|----------|-------|--------|----|-----|---------|\n",
    "| **Doc 1** |  1   |    2     |    0     |   0   |   0    | 2  |  0  |    2    |\n",
    "| **Doc 2** |  0   |    0     |    2     |   2   |   2    | 2  |  2  |    0    |\n",
    "\n",
    "- For example, in Doc 1, \"columbia\" appears 2 times, \"city\" once, \"so\" twice, and \"vibrant\" twice.\n",
    "- In Doc 2, \"missouri\" and \"river\" each appear 2 times, as do \"so,\" \"scenic,\" and \"the\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKkejZAs2JSL"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font> Example: Bag-of-Words (BoW) Model Using Text Related to Calgary with **repeating words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j45W0xAg2GTe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define a list of sentences with repeating words related to Columbia, Missouri\n",
    "repeating_words = [\"Columbia, Columbia, a city so vibrant, so vibrant.\",\n",
    "                   \"The Missouri River, the Missouri River, so scenic, so scenic.\"]\n",
    "\n",
    "# Initialize a CountVectorizer object to convert the text data into a matrix of token counts\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the list of sentences to build the vocabulary\n",
    "vect.fit(repeating_words)\n",
    "\n",
    "# Display the vocabulary that has been learned from the input documents\n",
    "vocab = vect.vocabulary_\n",
    "print(f\"Vocabulary learned from the documents: {vocab}\")\n",
    "\n",
    "# Transform the list of sentences into a bag-of-words matrix\n",
    "bag_of_words = vect.transform(repeating_words)\n",
    "\n",
    "# Convert the bag-of-words matrix into a pandas DataFrame for better visualization\n",
    "df_bow = pd.DataFrame(bag_of_words.toarray(),\n",
    "                      columns=vect.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame that shows the frequency of each word in the given sentences\n",
    "print(\"DataFrame showing the Bag-of-Words matrix:\")\n",
    "display(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-zHJMcW3nVr"
   },
   "source": [
    "## Enhancing Bag-of-Words: Stopword Removal\n",
    "\n",
    "In the Bag-of-Words (BoW) model, certain words are so common that they carry minimal useful information about the actual content of the document. These words, known as 'stopwords', can be removed to improve the analysis. There are two primary methods to eliminate stopwords:\n",
    "\n",
    "1. Utilizing a predefined list of stopwords specific to a language.\n",
    "2. Excluding words that appear too frequently across the documents.\n",
    "\n",
    "The scikit-learn library provides a built-in English stopword list in the `feature_extraction.text` module. This list can be used to filter out stopwords from the text data during the vectorization process, resulting in a more meaningful BoW representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__VObSvv2OH2"
   },
   "outputs": [],
   "source": [
    "# Import the set of English stop words from scikit-learn's feature_extraction.text module\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Display the total number of stop words provided in the scikit-learn's list\n",
    "print(f\"Number of stop words: {len(ENGLISH_STOP_WORDS)}\")\n",
    "\n",
    "# To provide a sample of this list, print every 20th stop word\n",
    "# This gives an idea of what kind of words are considered as stop words\n",
    "print(\"Every 20th stopword:\")\n",
    "print(list(ENGLISH_STOP_WORDS)[::20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yknRhNKz3pnJ"
   },
   "outputs": [],
   "source": [
    "docs_ext = docs + repeating_words\n",
    "print(f\"Total number of documents: {len(docs_ext)}\")\n",
    "print(docs_ext)\n",
    "vect = CountVectorizer(stop_words = \"english\")\n",
    "vect.fit(docs_ext)\n",
    "bag_of_words = vect.transform(docs_ext)\n",
    "pd.DataFrame(bag_of_words.toarray(), columns=vect.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
