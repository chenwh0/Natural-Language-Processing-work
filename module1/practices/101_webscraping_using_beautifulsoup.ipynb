{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with BeautifulSoup4: A Comprehensive Tutorial\n",
    "\n",
    "## Introduction to BeautifulSoup4\n",
    "\n",
    "**Purpose:**\n",
    "BeautifulSoup4 (BS4) is a powerful Python library designed for parsing HTML and XML documents with ease. It's particularly valuable for web scraping tasks, text extraction, and data mining from websites. BeautifulSoup excels at handling messy or poorly formatted HTML by converting raw HTML into a structured parse tree, making it intuitive to navigate and extract the specific data you need.\n",
    "\n",
    "**Why BeautifulSoup?**\n",
    "- **Intuitive syntax**: Uses familiar Python methods to navigate HTML structures\n",
    "- **Robust parsing**: Handles malformed HTML gracefully\n",
    "- **Flexible selectors**: Supports CSS selectors, tag searching, and attribute-based filtering\n",
    "- **Integration-friendly**: Works seamlessly with other Python data science libraries\n",
    "\n",
    "## Installation and Setup\n",
    "\n",
    "**Purpose:**\n",
    "This section guides you through installing BeautifulSoup4 and its companion libraries that enhance web scraping capabilities and enable advanced data processing workflows.\n",
    "\n",
    "```python\n",
    "# Install BeautifulSoup4 and related libraries\n",
    "pip install beautifulsoup4 requests lxml html5lib nltk spacy\n",
    "```\n",
    "\n",
    "**Library Overview:**\n",
    "\n",
    "- **`beautifulsoup4`**: The core library for parsing HTML/XML documents\n",
    "- **`requests`**: Modern HTTP library for downloading web pages with robust error handling\n",
    "- **`lxml`**: High-performance XML and HTML parser, ideal for processing large documents\n",
    "- **`html5lib`**: Standards-compliant parser that handles malformed HTML and ensures valid HTML5 output\n",
    "- **`nltk`, `spacy`**: Natural Language Processing libraries for advanced text analysis after scraping\n",
    "\n",
    "**Pro Tip:** While `requests` is recommended for production use, this tutorial uses `urllib` (built into Python) to minimize dependencies.\n",
    "\n",
    "## Choosing the Right Parser\n",
    "\n",
    "**Purpose:**\n",
    "Understanding parser differences helps you select the optimal tool for your specific scraping needs, balancing speed, accuracy, and compatibility requirements.\n",
    "\n",
    "**Parser Comparison:**\n",
    "\n",
    "- **`html.parser`**: Python's built-in parser\n",
    "  - *Pros*: No additional installation required, reliable\n",
    "  - *Cons*: Slower than alternatives, less forgiving of malformed HTML\n",
    "  - *Best for*: Simple scraping tasks, learning environments\n",
    "\n",
    "- **`lxml`**: Third-party XML/HTML parser\n",
    "  - *Pros*: Fastest option, feature-rich, excellent for large documents\n",
    "  - *Cons*: External dependency, may be overkill for simple tasks\n",
    "  - *Best for*: High-volume scraping, performance-critical applications\n",
    "\n",
    "- **`html5lib`**: Standards-compliant HTML5 parser\n",
    "  - *Pros*: Most accurate parsing, handles broken HTML gracefully\n",
    "  - *Cons*: Slowest option due to thoroughness\n",
    "  - *Best for*: Parsing heavily malformed HTML, ensuring standards compliance\n",
    "\n",
    "**Selection Guide:** Start with `'lxml'` for speed, fall back to `'html5lib'` if you encounter parsing errors with malformed HTML.\n",
    "\n",
    "## Setting Up Your First Web Scraping Project\n",
    "**Purpose:**\n",
    "This foundational example demonstrates the complete workflow: importing libraries, fetching web content, and creating a BeautifulSoup object ready for data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P610gMZrd8SE"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint            # For pretty-printing data structures\n",
    "from bs4 import BeautifulSoup        # Import BeautifulSoup for HTML parsing\n",
    "from urllib.request import urlopen   # For opening URLs\n",
    "\n",
    "# Specify the URL of the webpage to scrape\n",
    "myurl = \"https://muidsi.missouri.edu/academic-programs/m-s-data-science-and-analytics-program/masters/curriculum/\"\n",
    "\n",
    "# Download the HTML content of the web page\n",
    "html = urlopen(myurl).read()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup with the chosen parser\n",
    "soupified = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-by-Step Breakdown:**\n",
    "\n",
    "1. **Import statements**: We import essential modules for web scraping and data presentation\n",
    "2. **URL definition**: Target the specific webpage containing the data you need\n",
    "3. **Content retrieval**: Download the raw HTML content from the server\n",
    "4. **HTML parsing**: Transform raw HTML into a navigable BeautifulSoup object\n",
    "\n",
    "**Important Notes:**\n",
    "- The `soupified` object now contains the entire webpage structure in a searchable format\n",
    "- This approach works for static content; dynamic content may require tools like Selenium\n",
    "- Always respect robots.txt and website terms of service when scraping\n",
    "\n",
    "## Exploring and Understanding HTML Structure\n",
    "\n",
    "**Purpose:**\n",
    "Before extracting specific data, it's crucial to understand the webpage's structure. This exploration phase helps you identify the right HTML elements, classes, and IDs to target in your scraping code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the entire prettified HTML (can be very large)\n",
    "pprint(soupified.prettify())\n",
    "\n",
    "# Print only the first 2000 characters for a quick overview\n",
    "pprint(soupified.prettify()[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What This Reveals:**\n",
    "- **Document structure**: How the HTML is organized with nested tags\n",
    "- **CSS classes and IDs**: Identifiers you can use to target specific elements\n",
    "- **Content organization**: Where different types of information are located\n",
    "- **Potential extraction targets**: Tables, lists, headings, and text containers\n",
    "\n",
    "**Best Practices for HTML Exploration:**\n",
    "- Start with a small sample (first 1000-2000 characters) to avoid overwhelming output\n",
    "- Look for patterns in class names and IDs that indicate content types\n",
    "- Identify container elements that wrap the data you're interested in\n",
    "- Note any JavaScript-generated content that might require different scraping approaches\n",
    "\n",
    "**Developer Tools Tip:** Use your browser's \"Inspect Element\" feature alongside this code to better understand the HTML structure visually.\n",
    "\n",
    "## Extracting Page Metadata\n",
    "\n",
    "**Purpose:**\n",
    "The page title often provides valuable context about the content you're scraping and can be used for validation, categorization, or documentation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the title of the web page\n",
    "print(soupified.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Extract Titles?**\n",
    "- **Content validation**: Confirm you're scraping the intended page\n",
    "- **Data organization**: Use titles as headers in your output files\n",
    "- **Debugging**: Quickly identify when you've navigated to unexpected pages\n",
    "- **Metadata collection**: Build comprehensive datasets with descriptive information\n",
    "\n",
    "**Extension Ideas:**\n",
    "- Extract meta descriptions for additional context\n",
    "- Collect keywords and author information\n",
    "- Gather publication dates when available\n",
    "\n",
    "## Comprehensive Link Extraction\n",
    "\n",
    "**Purpose:**\n",
    "Extracting all hyperlinks from a webpage serves multiple purposes: building site maps, discovering related content, creating web crawlers, or analyzing link structures for SEO or research purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and print all hyperlinks from the HTML\n",
    "for link in soupified.find_all('a', href=True):\n",
    "    print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Understanding the Output:**\n",
    "- **Absolute URLs**: Complete web addresses (e.g., `https://example.com/page`)\n",
    "- **Relative URLs**: Paths relative to the current domain (e.g., `/about`, `../contact`)\n",
    "- **Fragment URLs**: Links to sections within the same page (e.g., `#section1`)\n",
    "- **Protocol URLs**: Special protocols like `mailto:` or `tel:`\n",
    "\n",
    "**Advanced Link Processing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sophisticated link extraction with categorization\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "base_url = \"https://muidsi.missouri.edu/\"\n",
    "internal_links = []\n",
    "external_links = []\n",
    "\n",
    "for link in soupified.find_all('a', href=True):\n",
    "    href = link['href']\n",
    "    full_url = urljoin(base_url, href)  # Convert relative to absolute URLs\n",
    "    \n",
    "    if urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
    "        internal_links.append(full_url)\n",
    "    else:\n",
    "        external_links.append(full_url)\n",
    "\n",
    "print(f\"Found {len(internal_links)} internal links and {len(external_links)} external links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Clean Text Content\n",
    "\n",
    "**Purpose:**\n",
    "Raw text extraction removes all HTML formatting, scripts, and styling to provide clean content suitable for natural language processing, content analysis, or text mining applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all visible text from the HTML page\n",
    "print(soupified.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Cleaning Considerations:**\n",
    "- **Whitespace handling**: The output may contain excessive spacing or line breaks\n",
    "- **Hidden content**: Some text might be hidden via CSS but still extracted\n",
    "- **Script content**: JavaScript code might appear in the text output\n",
    "\n",
    "**Enhanced Text Extraction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More controlled text extraction with cleanup\n",
    "def extract_clean_text(soup_object):\n",
    "    # Remove script and style elements\n",
    "    for script in soup_object([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get text and clean up whitespace\n",
    "    text = soup_object.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    clean_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "clean_content = extract_clean_text(soupified)\n",
    "print(clean_content[:500] + \"...\")  # First 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural Content Extraction: Headings\n",
    "\n",
    "**Purpose:**\n",
    "Headings (`<h1>`, `<h2>`, etc.) form the backbone of document structure. Extracting them helps you understand content hierarchy, create automatic table of contents, or identify key topics and sections within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all headings (h1, h2, h3, etc.) from the HTML\n",
    "for heading in soupified.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "    print(f\"{heading.name}: {heading.get_text(strip=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applications for Heading Extraction:**\n",
    "- **Document outlining**: Generate automatic table of contents\n",
    "- **Content summarization**: Identify main topics and subtopics\n",
    "- **Navigation creation**: Build page navigation menus\n",
    "- **Content analysis**: Study document structure and organization patterns\n",
    "\n",
    "**Advanced Heading Analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hierarchical structure of headings\n",
    "def create_heading_hierarchy(soup_object):\n",
    "    headings = soup_object.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    hierarchy = []\n",
    "    \n",
    "    for heading in headings:\n",
    "        level = int(heading.name[1])  # Extract number from h1, h2, etc.\n",
    "        text = heading.get_text(strip=True)\n",
    "        hierarchy.append({\n",
    "            'level': level,\n",
    "            'text': text,\n",
    "            'indent': '  ' * (level - 1)\n",
    "        })\n",
    "    \n",
    "    return hierarchy\n",
    "\n",
    "heading_structure = create_heading_hierarchy(soupified)\n",
    "for item in heading_structure:\n",
    "    print(f\"{item['indent']}{item['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Data Extraction\n",
    "\n",
    "**Purpose:**\n",
    "HTML tables contain structured data that's perfect for analysis. This technique extracts tabular information and prepares it for export to spreadsheets, databases, or data analysis frameworks like pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and print data from all tables in the HTML\n",
    "for table in soupified.find_all('table'):\n",
    "    for row in table.find_all('tr'):\n",
    "        cells = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]\n",
    "        print(cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Different Table Structures:**\n",
    "- **Header rows**: Tables may have `<th>` elements for headers\n",
    "- **Merged cells**: Some cells may span multiple rows or columns\n",
    "- **Nested tables**: Tables within tables require special handling\n",
    "- **Empty cells**: Missing data should be handled appropriately\n",
    "\n",
    "**Enhanced Table Processing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_data(soup_object):\n",
    "    tables_data = []\n",
    "    \n",
    "    for table_index, table in enumerate(soup_object.find_all('table')):\n",
    "        table_data = []\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        for row in rows:\n",
    "            # Handle both header (th) and data (td) cells\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "            if row_data:  # Only add non-empty rows\n",
    "                table_data.append(row_data)\n",
    "        \n",
    "        if table_data:  # Only add non-empty tables\n",
    "            tables_data.append({\n",
    "                'table_index': table_index,\n",
    "                'data': table_data,\n",
    "                'rows': len(table_data),\n",
    "                'columns': len(table_data[0]) if table_data else 0\n",
    "            })\n",
    "    \n",
    "    return tables_data\n",
    "\n",
    "table_info = extract_table_data(soupified)\n",
    "for table in table_info:\n",
    "    print(f\"Table {table['table_index']}: {table['rows']} rows Ã— {table['columns']} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Professional Web Scraping Best Practices\n",
    "\n",
    "### Ethical and Legal Considerations\n",
    "- **Respect robots.txt**: Always check `website.com/robots.txt` before scraping\n",
    "- **Rate limiting**: Add delays between requests to avoid overwhelming servers\n",
    "- **Terms of service**: Review and comply with website terms of use\n",
    "- **Copyright awareness**: Respect intellectual property and fair use guidelines\n",
    "\n",
    "### Technical Best Practices\n",
    "\n",
    "**Error Handling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "def safe_scrape(url, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Add random delay to appear more human-like\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            return BeautifulSoup(response.content, 'lxml')\n",
    "            \n",
    "        except RequestException as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "            print(f\"Attempt {attempt + 1} failed, retrying...\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-Agent Headers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "url = \"https://muidsi.missouri.edu/\"\n",
    "response = requests.get(url, headers=headers)\n",
    "print(response.status_code)  # Check if the request was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Real-World Application: Complete Course Information Extraction\n",
    "\n",
    "**Purpose:**\n",
    "This comprehensive example demonstrates a complete workflow for extracting, processing, and structuring course information from an academic website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of error, run this:\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_course_information(url):\n",
    "    \"\"\"\n",
    "    Complete course information extraction workflow\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch and parse the webpage\n",
    "        html = urlopen(url).read()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Extract structured information\n",
    "        course_data = {\n",
    "            'page_title': soup.title.get_text(strip=True) if soup.title else 'No Title',\n",
    "            'headings': [],\n",
    "            'course_descriptions': [],\n",
    "            'links': [],\n",
    "            'last_scraped': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        # Extract headings with hierarchy\n",
    "        for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            course_data['headings'].append({\n",
    "                'level': int(heading.name[1]),\n",
    "                'text': heading.get_text(strip=True)\n",
    "            })\n",
    "        \n",
    "        # Extract course descriptions (assuming they're in specific containers)\n",
    "        for section in soup.find_all(['section', 'div'], class_=lambda x: x and 'course' in x.lower()):\n",
    "            description = section.get_text(strip=True)\n",
    "            if len(description) > 50:  # Filter out short, likely irrelevant content\n",
    "                course_data['course_descriptions'].append(description)\n",
    "        \n",
    "        # Extract relevant links\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            link_text = link.get_text(strip=True)\n",
    "            if link_text and len(link_text) > 3:  # Filter out empty or very short links\n",
    "                course_data['links'].append({\n",
    "                    'url': href,\n",
    "                    'text': link_text\n",
    "                })\n",
    "        \n",
    "        return course_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "url = \"https://muidsi.missouri.edu/academic-programs/m-s-data-science-and-analytics-program/masters/curriculum/\"\n",
    "course_info = extract_course_information(url)\n",
    "\n",
    "if course_info:\n",
    "    print(f\"Successfully scraped: {course_info['page_title']}\")\n",
    "    print(f\"Found {len(course_info['headings'])} headings\")\n",
    "    print(f\"Found {len(course_info['links'])} links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Further Learning\n",
    "\n",
    "This tutorial has covered the fundamental concepts and practical applications of web scraping with BeautifulSoup4. You've learned how to:\n",
    "\n",
    "- Set up a proper scraping environment with appropriate libraries\n",
    "- Navigate and parse HTML documents effectively\n",
    "- Extract various types of content (text, links, tables, headings)\n",
    "- Apply best practices for ethical and efficient scraping\n",
    "- Integrate scraping with data analysis workflows\n",
    "\n",
    "### Recommended Next Steps:\n",
    "1. **Practice with different websites** to encounter various HTML structures\n",
    "2. **Learn CSS selectors** for more precise element targeting\n",
    "3. **Explore Scrapy framework** for large-scale scraping projects\n",
    "4. **Study web APIs** as alternatives to scraping when available\n",
    "5. **Master regular expressions** for advanced text processing\n",
    "\n",
    "### Additional Resources:\n",
    "- BeautifulSoup Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "- Requests Documentation: https://docs.python-requests.org/\n",
    "- Web Scraping Ethics Guide: Always prioritize respectful, legal scraping practices"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WebScraping_using_BeautifulSoup.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
