{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Exploration and Analysis of Tokenization and Embeddings in Modern LLMs\n",
    "\n",
    "**Background:**  \n",
    "You’ve studied in detail how natural language is transformed so that large language models can process it. In this assignment, you’ll push further: you’ll test how multiple real tokenizers operate, dissect how their output impacts the resulting embeddings, and analyze why these choices matter for both accuracy and efficiency in language tasks.\n",
    "\n",
    "### Instructions & Deliverables\n",
    "\n",
    "#### 1. **Tokenization Deep Dive (2 points)**\n",
    "\n",
    "- Select *five diverse sentences*:  \n",
    "  - Two with formal academic language  \n",
    "  - One with slang or social media language  \n",
    "  - One with an emoji  \n",
    "  - One with code/math notation  \n",
    "- For each sentence, tokenize with **three different pretrained tokenizers** (choose from e.g. `bert-base-uncased`, `gpt2`, `microsoft/Phi-3-mini-4k-instruct`, `google/flan-t5-small`, etc.).\n",
    "- Display for each:\n",
    "  - The original text\n",
    "  - The sequence of tokens and their decoded forms (subwords)\n",
    "  - The token IDs\n",
    "\n",
    "#### 2. **Cross-Tokenizer Comparison (2 points)**\n",
    "\n",
    "- Place your results in a **comparison table**:\n",
    "  - For each sentence and tokenizer, show:  \n",
    "    - Number of tokens  \n",
    "    - How words or special features (names/emoji/code) are split\n",
    "    - Presence of [UNK] or unknown tokens\n",
    "- In a *markdown cell*, answer:  \n",
    "  - Which tokenization schemes are more robust to slang, emojis, and code?  \n",
    "  - Which produce the longest and shortest sequences? Why?\n",
    "\n",
    "#### 3. **Token Embedding Visualization (3 points)**\n",
    "\n",
    "- Pick one sentence and one tokenizer from your previous results.\n",
    "- Use the tokenizer’s pretrained embedding layer (from its associated model) to produce the embedding vector for each token in the sentence.\n",
    "- Use PCA or t-SNE to project the token embeddings to 2D and create a **scatter plot**:\n",
    "  - Each point should be labeled with the decoded token.\n",
    "  - Color points differently for subwords, whole words, and special tokens.\n",
    "- Comment on the geometry: Do related words/subwords cluster? Are special tokens outliers?\n",
    "\n",
    "#### 4. **Prompt Engineering & Model Output (2 points)**\n",
    "\n",
    "- Take two tokenized prompts that yielded notably different token splits across tokenizers (e.g. one with code/math and one with informal language).\n",
    "- For each:\n",
    "  - Use two *different* language models (“matching” the tokenizer used) to generate text completions.\n",
    "  - In a short table, report:\n",
    "    - Length (in tokens and characters) of the generated output\n",
    "    - Are any [UNK] tokens, empty outputs, or odd/non-conversational results observed?\n",
    "- Discuss how the tokenizer choice might affect downstream output quality and efficiency.\n",
    "\n",
    "#### 5. **Reflection (1 point)**\n",
    "\n",
    "- In a paragraph (markdown), summarize:\n",
    "  - How does the choice of tokenizer and embedding scheme affect which kinds of input a model can “understand”?\n",
    "  - Why must LLM practitioners consider both the *efficiency* (sequence length) and the *semantic coverage* (handling unknowns, subwords, emoji) of each tokenizer?\n",
    "\n",
    "**Submission**:  \n",
    "Produce a Jupyter notebook with clearly separated code and markdown cells for each section. All code must run under Python and Hugging Face Transformers. Include all required tables, plots, and discussion.\n",
    "\n",
    "**Grading Rubric:**\n",
    "\n",
    "| Section                           | Points |\n",
    "|:-----------------------------------|:------:|\n",
    "| Tokenization Deep Dive             | 2      |\n",
    "| Cross-Tokenizer Comparison Table & Analysis | 2 |\n",
    "| Embedding Visualization            | 3      |\n",
    "| Prompt Engineering & Model Output  | 2      |\n",
    "| Reflection                        | 1      |\n",
    "| **Total**                         | **10** |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
