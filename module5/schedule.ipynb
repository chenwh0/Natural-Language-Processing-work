{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49236e62",
   "metadata": {},
   "source": [
    "# Module: Text Classification\n",
    "\n",
    "## Module Overview\n",
    "\n",
    "This module provides a comprehensive journey through text classification methods, from traditional machine learning approaches to state-of-the-art deep learning architectures. Students will master the progression from classical techniques (Naive Bayes, SVM) through neural embeddings (Word2Vec, Doc2Vec) to modern deep learning models (CNNs, LSTMs, BERT), culminating in advanced topics like data augmentation and model robustness.\n",
    "\n",
    "### Module Objectives\n",
    "\n",
    "By the end of this module, students will be able to:\n",
    "\n",
    "1. **Master Traditional Text Classification**: Implement and compare classical ML algorithms (Naive Bayes, SVM, Logistic Regression) for text classification tasks\n",
    "2. **Apply Neural Embeddings**: Use Word2Vec and Doc2Vec to create semantic representations for sentiment analysis and document classification\n",
    "3. **Build Deep Learning Models**: Design and train CNNs and LSTMs for text classification, understanding their architectural strengths and trade-offs\n",
    "4. **Leverage Modern Transformers**: Fine-tune BERT and work with modern LLMs for state-of-the-art text classification performance\n",
    "5. **Address Real-World Challenges**: Implement data augmentation techniques and evaluate model robustness across different text conditions\n",
    "\n",
    "### Module Components\n",
    "\n",
    "#### Theoretical Foundation\n",
    "- Evolution from sparse (BoW, TF-IDF) to dense (embeddings) to contextual (transformers) representations\n",
    "- Mathematical foundations of classification algorithms and neural architectures\n",
    "- Understanding attention mechanisms and bidirectional context in transformers\n",
    "- Data augmentation strategies and robustness evaluation in NLP systems\n",
    "- Modern tokenization techniques and their impact on model performance\n",
    "\n",
    "#### Practical Skills\n",
    "- Implementation of traditional ML pipelines for text classification\n",
    "- Training and evaluation of Word2Vec and Doc2Vec models on real datasets\n",
    "- Building CNN and LSTM architectures using TensorFlow/Keras\n",
    "- Fine-tuning pre-trained BERT models for domain-specific tasks\n",
    "- Working with modern LLMs, tokenization, and embedding extraction\n",
    "- Developing robust classification systems with data augmentation techniques\n",
    "\n",
    "---\n",
    "\n",
    "## Module Content\n",
    "\n",
    "### Week 1: Foundations and Traditional Approaches\n",
    "\n",
    "#### Lecture Materials\n",
    "- **[Text Classification Foundations (PDF)]** - Traditional ML approaches, evaluation metrics, and pipeline design\n",
    "- **[Classical Algorithms Deep Dive (PDF)]** - Mathematical foundations of Naive Bayes, SVM, and Logistic Regression\n",
    "\n",
    "#### Practical Sessions\n",
    "- **[Environment Setup and Data Preparation](practices/301_environment_setup.ipynb)**\n",
    "  - Setting up deep learning environments (TensorFlow, PyTorch, Hugging Face)\n",
    "  - Data preprocessing pipelines and evaluation frameworks\n",
    "  - University of Missouri dataset preparation for all exercises\n",
    "\n",
    "- **[One Pipeline, Many Classifiers](practices/301_onepipeline_manyclassifiers.ipynb)**\n",
    "  - Implementing traditional ML classification pipeline\n",
    "  - Comparing Naive Bayes, SVM, and Logistic Regression performance\n",
    "  - Handling class imbalance and feature engineering challenges\n",
    "  - Economic news relevance classification case study\n",
    "\n",
    "### Week 2: Neural Embeddings and Semantic Representations\n",
    "\n",
    "#### Lecture Materials\n",
    "- **[Word Embeddings Deep Dive (PDF)]** - Word2Vec, Doc2Vec, and semantic representation learning\n",
    "- **[Embedding Evaluation Methods (PDF)]** - Intrinsic and extrinsic evaluation approaches\n",
    "\n",
    "#### Practical Sessions\n",
    "- **[Sentiment Analysis with Word2Vec](practices/302_word2vec_example.ipynb)**\n",
    "  - Loading and using pre-trained Google News Word2Vec embeddings\n",
    "  - Converting sentences to fixed-size vectors through averaging\n",
    "  - Training logistic regression on dense semantic features\n",
    "  - Evaluating performance on multi-domain sentiment data\n",
    "\n",
    "- **[Document Classification with Doc2Vec](practices/303_doc2vec_example.ipynb)**\n",
    "  - Training Doc2Vec models on Twitter emotion recognition dataset\n",
    "  - Comparing Distributed Memory (DM) vs. Distributed Bag of Words (DBOW) architectures\n",
    "  - Handling multi-class classification with imbalanced social media data\n",
    "  - Performance evaluation across six emotion categories\n",
    "\n",
    "### Week 3: Deep Learning Architectures\n",
    "\n",
    "#### Lecture Materials\n",
    "- **[Deep Learning for NLP (PDF)]** - CNNs, LSTMs, and neural architecture principles\n",
    "- **[Sequence Modeling (PDF)]** - RNN fundamentals, LSTM/GRU architectures, and bidirectional models\n",
    "\n",
    "#### Practical Sessions\n",
    "- **[Convolutional Neural Networks for Text](practices/304_cnn_text_classification.ipynb)**\n",
    "  - Building CNN architectures for sentiment classification on IMDB dataset\n",
    "  - Multiple filter sizes and max pooling strategies\n",
    "  - Comparing pre-trained GloVe vs. trainable embeddings\n",
    "  - Regularization techniques and hyperparameter tuning\n",
    "\n",
    "- **[LSTM and RNN Models](practices/304_lstm_text_classification.ipynb)**\n",
    "  - Implementing LSTM models with dropout and recurrent regularization\n",
    "  - Bidirectional LSTM architectures for improved context understanding\n",
    "  - Sequence padding and masking strategies\n",
    "  - Performance benchmarking and computational efficiency analysis\n",
    "\n",
    "### Week 4: Modern Transformers and BERT\n",
    "\n",
    "#### Lecture Materials\n",
    "- **[Transformer Architecture (PDF)]** - Attention mechanisms, self-attention, and positional encoding\n",
    "- **[BERT and Fine-tuning (PDF)]** - Pre-training objectives, fine-tuning strategies, and domain adaptation\n",
    "\n",
    "#### Practical Sessions\n",
    "- **[BERT for Sentiment Classification](practices/305_classify_text_with_bert.ipynb)**\n",
    "  - Fine-tuning pre-trained BERT models for sentiment analysis\n",
    "  - Understanding BERT preprocessing and tokenization pipeline\n",
    "  - Implementing end-to-end classification with TensorFlow Hub\n",
    "  - Achieving state-of-the-art performance on benchmark datasets\n",
    "\n",
    "- **[Advanced BERT Applications](practices/305_bert_advanced.ipynb)**\n",
    "  - Multi-class classification with domain-specific BERT models\n",
    "  - Handling long documents and sequence length limitations\n",
    "  - BERT embeddings for feature extraction and downstream tasks\n",
    "\n",
    "### Week 5: Large Language Models and Tokenization\n",
    "\n",
    "#### Lecture Materials\n",
    "- **[Introduction to Large Language Models (PDF)]** - Evolution from BERT to GPT, scaling laws, and emergent capabilities\n",
    "- **[Tokens and Embeddings (PDF)]** - Modern tokenization techniques, subword encoding, and contextual embeddings\n",
    "\n",
    "#### Practical Sessions\n",
    "- **[Working with Modern LLMs](practices/501_llm_introduction.ipynb)**\n",
    "  - Loading and using open-source language models (Phi-3, Llama, Gemma)\n",
    "  - Text generation and prompt engineering techniques\n",
    "  - Understanding model architectures and parameter scaling\n",
    "\n",
    "- **[Tokenization and Embeddings Analysis](practices/502_tokens_embeddings.ipynb)**\n",
    "  - Comparing different tokenization strategies (BPE, WordPiece, SentencePiece)\n",
    "  - Extracting and analyzing contextual embeddings from LLMs\n",
    "  - Visualizing token representations and semantic relationships\n",
    "\n",
    "### Week 6: Data Augmentation and Robustness\n",
    "\n",
    "#### Lecture Materials\n",
    "- **[Data Augmentation in NLP (PDF)]** - Augmentation strategies, back-translation, and synthetic data generation\n",
    "- **[Model Robustness (PDF)]** - Adversarial examples, evaluation frameworks, and defense mechanisms\n",
    "\n",
    "#### Practical Sessions\n",
    "- **[Text Data Augmentation Techniques](practices/601_data_augmentation.ipynb)**\n",
    "  - Implementing synonym replacement, back-translation, and paraphrasing\n",
    "  - Easy Data Augmentation (EDA) techniques and their effectiveness\n",
    "  - Generating synthetic training data for imbalanced datasets\n",
    "\n",
    "- **[Robustness Testing and Evaluation](practices/601_robustness_testing.ipynb)**\n",
    "  - Creating adversarial examples and noisy test sets\n",
    "  - Evaluating model performance under distribution shift\n",
    "  - Implementing robustness metrics and visualization techniques\n",
    "\n",
    "---\n",
    "\n",
    "## Assignments\n",
    "\n",
    "### Assignment 1: Model Comparison and Architecture Analysis\n",
    "**File:** [Assignment_301.ipynb](assignments/Assignment_301.ipynb)  \n",
    "**Points:** 15  \n",
    "**Focus:** Comprehensive comparison of traditional ML vs. deep learning approaches\n",
    "\n",
    "**Overview:** This assignment requires students to implement and systematically compare multiple text classification approaches on a common dataset, analyzing their strengths, weaknesses, and computational trade-offs.\n",
    "\n",
    "**Key Components:**\n",
    "- **Dataset Selection & Preprocessing (3 pts):** Choose and prepare a multi-class text classification dataset with thorough preprocessing\n",
    "- **Traditional ML Implementation (4 pts):** Implement and optimize Naive Bayes, SVM, and Logistic Regression with proper feature engineering\n",
    "- **Deep Learning Models (5 pts):** Build and train CNN, LSTM, and BERT models with proper hyperparameter tuning\n",
    "- **Comparative Analysis (2 pts):** Create comprehensive performance comparison including accuracy, training time, and interpretability analysis\n",
    "- **Technical Report (1 pt):** Write detailed analysis of results, recommendations for different use cases, and lessons learned\n",
    "\n",
    "**Learning Outcomes:** Understanding of trade-offs between different approaches, practical experience with model selection, and ability to make informed architectural decisions.\n",
    "\n",
    "### Assignment 2: Advanced LLM Applications and Tokenization\n",
    "**File:** [Assignment_502.ipynb](assignments/Assignment_502.ipynb)  \n",
    "**Points:** 15  \n",
    "**Focus:** Modern LLM techniques, tokenization analysis, and embedding applications\n",
    "\n",
    "**Overview:** This assignment explores advanced applications of large language models, focusing on tokenization strategies, embedding extraction, and practical implementation challenges.\n",
    "\n",
    "**Key Components:**\n",
    "- **Tokenization Deep Dive (4 pts):** Compare multiple tokenizers across diverse text types (academic, social media, code, multilingual)\n",
    "- **Embedding Analysis (4 pts):** Extract and visualize contextual embeddings, analyze semantic relationships\n",
    "- **LLM Applications (4 pts):** Implement text classification using LLM embeddings, prompt engineering techniques\n",
    "- **Robustness Evaluation (2 pts):** Test model performance under various input conditions and augmentation strategies\n",
    "- **Innovation Component (1 pt):** Propose and implement a novel application or improvement\n",
    "\n",
    "**Learning Outcomes:** Advanced understanding of modern NLP techniques, practical experience with state-of-the-art models, and ability to adapt cutting-edge research to practical applications.\n",
    "\n",
    "### Assignment 3: Data Augmentation and Robustness Project\n",
    "**File:** [Assignment_601.ipynb](assignments/Assignment_601.ipynb)  \n",
    "**Points:** 10  \n",
    "**Focus:** Implementation and evaluation of data augmentation and robustness techniques\n",
    "\n",
    "**Overview:** This assignment focuses on advanced techniques for improving model robustness through data augmentation and systematic evaluation across different types of input variations.\n",
    "\n",
    "**Key Components:**\n",
    "- **Augmentation Implementation (3 pts):** Implement multiple augmentation strategies including back-translation and synthetic generation\n",
    "- **Robustness Testing (3 pts):** Create comprehensive test suites for evaluating model robustness\n",
    "- **Performance Analysis (2 pts):** Systematic evaluation of augmentation effectiveness across different model architectures\n",
    "- **Research Component (2 pts):** Literature review and implementation of recent augmentation techniques\n",
    "\n",
    "**Learning Outcomes:** Practical skills in improving model robustness, understanding of real-world deployment challenges, and ability to implement research-based solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Path\n",
    "\n",
    "### Foundation Level (Weeks 1-2)\n",
    "1. **Start with traditional approaches** - Understand classical ML algorithms and their applications\n",
    "2. **Master feature engineering** - Learn text preprocessing and feature extraction techniques\n",
    "3. **Explore neural embeddings** - Implement Word2Vec and Doc2Vec for semantic representations\n",
    "4. **Evaluate and compare** - Develop skills in model evaluation and comparative analysis\n",
    "\n",
    "### Intermediate Level (Weeks 3-4)\n",
    "5. **Build deep architectures** - Implement CNN and LSTM models from scratch\n",
    "6. **Understand attention** - Learn transformer architectures and self-attention mechanisms\n",
    "7. **Fine-tune BERT** - Master transfer learning and domain adaptation techniques\n",
    "8. **Optimize performance** - Learn hyperparameter tuning and regularization strategies\n",
    "\n",
    "### Advanced Level (Weeks 5-6)\n",
    "9. **Work with modern LLMs** - Understand large language model architectures and capabilities\n",
    "10. **Master tokenization** - Analyze different tokenization strategies and their impact\n",
    "11. **Implement augmentation** - Develop data augmentation and robustness techniques\n",
    "12. **Integrate approaches** - Combine multiple techniques for optimal performance\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Technical Requirements\n",
    "- Strong Python programming skills with experience in data manipulation\n",
    "- Familiarity with machine learning concepts (classification, evaluation metrics, cross-validation)\n",
    "- Basic understanding of neural networks and backpropagation\n",
    "- Experience with NumPy, pandas, and scikit-learn\n",
    "- Completion of foundational NLP modules (text preprocessing, vectorization)\n",
    "\n",
    "### Mathematical Background\n",
    "- Linear algebra (vectors, matrices, eigenvalues)\n",
    "- Probability and statistics (Bayes' theorem, distributions, hypothesis testing)\n",
    "- Calculus (derivatives, optimization, gradient descent)\n",
    "- Understanding of information theory concepts (entropy, mutual information)\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Environment\n",
    "\n",
    "### Required Libraries and Frameworks\n",
    "\n",
    "```python\n",
    "# Core data science libraries\n",
    "pip install numpy pandas matplotlib seaborn jupyter\n",
    "\n",
    "# Traditional machine learning\n",
    "pip install scikit-learn nltk spacy\n",
    "\n",
    "# Deep learning frameworks\n",
    "pip install tensorflow torch transformers\n",
    "\n",
    "# Modern NLP libraries\n",
    "pip install datasets tokenizers sentence-transformers\n",
    "\n",
    "# Specialized tools\n",
    "pip install textattack wordcloud plotly umap-learn\n",
    "\n",
    "# Download language models\n",
    "python -m spacy download en_core_web_sm\n",
    "python -c \"import nltk; nltk.download('all')\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82433ccb",
   "metadata": {},
   "source": [
    "### Computing Requirements\n",
    "- **Minimum:** 8GB RAM, modern CPU, 10GB storage\n",
    "- **Recommended:** 16GB+ RAM, GPU with 8GB+ VRAM, 20GB storage\n",
    "- **Cloud options:** Google Colab Pro, AWS SageMaker, Azure Machine Learning\n",
    "\n",
    "### Development Environment\n",
    "- **Primary:** Jupyter Notebooks for interactive development\n",
    "- **Alternative:** VS Code with Python extension, PyCharm\n",
    "- **Version control:** Git integration for assignment submission\n",
    "- **Documentation:** Markdown for reports and documentation\n",
    "\n",
    "---\n",
    "\n",
    "## Assessment Strategy\n",
    "\n",
    "### Continuous Assessment (70%)\n",
    "- **Weekly quizzes** (20%): Short assessments on theoretical concepts\n",
    "- **Practical exercises** (30%): Hands-on implementation tasks\n",
    "- **Assignments** (20%): Three major projects with increasing complexity\n",
    "\n",
    "### Final Assessment (30%)\n",
    "- **Comprehensive project** (20%): End-to-end text classification system\n",
    "- **Technical presentation** (10%): Present project results and methodology\n",
    "\n",
    "### Grading Rubric\n",
    "- **Code quality and functionality** (40%): Working implementations with proper documentation\n",
    "- **Technical understanding** (30%): Demonstrated understanding of concepts and methods\n",
    "- **Analysis and insights** (20%): Quality of analysis, interpretation, and recommendations\n",
    "- **Innovation and creativity** (10%): Novel approaches, creative solutions, and extensions\n",
    "\n",
    "---\n",
    "\n",
    "## Resources and References\n",
    "\n",
    "### Core Textbooks\n",
    "- Vajjala, Sowmya, et al. *Practical Natural Language Processing*. O'Reilly Media, 2020\n",
    "- Alammar, Jay, and Maarten Grootendorst. *Hands-on Large Language Models*. O'Reilly Media, 2024\n",
    "- Tunstall, Lewis, et al. *Natural Language Processing with Transformers*. O'Reilly Media, 2022\n",
    "\n",
    "### Key Research Papers\n",
    "- **Word2Vec:** Mikolov, T., et al. \"Efficient Estimation of Word Representations in Vector Space.\" ICLR 2013\n",
    "- **CNNs for Text:** Kim, Y. \"Convolutional Neural Networks for Sentence Classification.\" EMNLP 2014\n",
    "- **LSTM for NLP:** Hochreiter, S., & Schmidhuber, J. \"Long Short-Term Memory.\" Neural Computation, 1997\n",
    "- **Attention Mechanism:** Vaswani, A., et al. \"Attention Is All You Need.\" NIPS 2017\n",
    "- **BERT:** Devlin, J., et al. \"BERT: Pre-training of Deep Bidirectional Transformers.\" NAACL 2019\n",
    "- **Data Augmentation:** Wei, J., & Zou, K. \"EDA: Easy Data Augmentation Techniques for Boosting Performance.\" EMNLP 2019\n",
    "\n",
    "### Online Resources\n",
    "- **Hugging Face Hub:** Pre-trained models and datasets\n",
    "- **Papers With Code:** Latest research implementations\n",
    "- **Google Colab:** Free GPU access for experiments\n",
    "- **Kaggle:** Datasets and competitions for practice\n",
    "- **GitHub:** Open-source implementations and examples\n",
    "\n",
    "### University of Missouri Resources\n",
    "- **IDSI Computing Cluster:** High-performance computing access\n",
    "- **Library Databases:** Academic paper access\n",
    "- **Industry Partnerships:** Real-world datasets and problems\n",
    "- **Research Groups:** Collaboration opportunities with faculty\n",
    "\n",
    "---\n",
    "\n",
    "## Module Innovation Features\n",
    "\n",
    "### Practical Applications\n",
    "- **Real-world datasets:** University communications, social media, news articles\n",
    "- **Industry partnerships:** Guest lectures from data science professionals\n",
    "- **Case studies:** Missouri-specific applications (agriculture, healthcare, education)\n",
    "\n",
    "### Technology Integration\n",
    "- **Latest models:** Integration of newest LLMs and techniques\n",
    "- **Cloud platforms:** Experience with modern deployment strategies\n",
    "- **MLOps practices:** Model versioning, monitoring, and deployment\n",
    "\n",
    "### Research Opportunities\n",
    "- **Faculty collaboration:** Opportunities to work on ongoing research projects\n",
    "- **Conference submissions:** Support for presenting work at academic conferences\n",
    "- **Open source contributions:** Contributing to popular NLP libraries\n",
    "\n",
    "### Career Preparation\n",
    "- **Portfolio development:** Building a comprehensive project portfolio\n",
    "- **Technical interviews:** Practice with industry-standard questions\n",
    "- **Networking events:** Connections with alumni and industry professionals\n",
    "- **Certification paths:** Preparation for industry certifications (AWS ML, Google Cloud AI)\n",
    "\n",
    "---\n",
    "\n",
    "## Success Metrics and Outcomes\n",
    "\n",
    "### Technical Competencies\n",
    "Students will demonstrate ability to:\n",
    "- **Implement complete ML pipelines** from data preprocessing to model deployment\n",
    "- **Select appropriate architectures** based on problem requirements and constraints\n",
    "- **Optimize model performance** through hyperparameter tuning and regularization\n",
    "- **Evaluate models rigorously** using appropriate metrics and validation strategies\n",
    "- **Handle real-world challenges** including imbalanced data, noisy inputs, and distribution shift\n",
    "\n",
    "### Professional Skills\n",
    "Students will develop:\n",
    "- **Technical communication** through documentation and presentations\n",
    "- **Critical thinking** in model selection and evaluation\n",
    "- **Problem-solving abilities** for novel NLP challenges\n",
    "- **Collaboration skills** through group projects and peer review\n",
    "- **Research skills** in staying current with rapidly evolving field\n",
    "\n",
    "### Career Readiness\n",
    "Graduates will be prepared for:\n",
    "- **Data Scientist positions** with strong NLP specialization\n",
    "- **ML Engineer roles** focusing on text processing systems\n",
    "- **Research positions** in academic or industrial settings\n",
    "- **Consulting opportunities** in NLP applications\n",
    "- **Entrepreneurial ventures** leveraging text analysis technologies\n",
    "\n",
    "This comprehensive module design ensures students gain both theoretical understanding and practical experience with the full spectrum of text classification techniques, preparing them for success in modern data science and NLP careers."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
